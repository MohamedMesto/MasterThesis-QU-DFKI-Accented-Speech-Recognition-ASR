{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roshdl_R_SVy"
      },
      "source": [
        " \n",
        " ## <font color=\"1497d4\">  **Statistical Analysis for Accented Speech recognition**</font>\n",
        "\n",
        " \n",
        " \n",
        "<font color=white> **Mozilla Common Voice 10.0. dataset**</font> \n",
        "\n",
        "## <font color=\"1497d4\">  **Analyze results from Carlos’ model**</font>\n",
        " \n",
        "<font color=white> **Setup dev environment:**</font> \n",
        "\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faqkMqilUfUf"
      },
      "source": [
        "###<font color=\"1497d4\"> Import the necessary python libraries:</font>\n",
        "\n",
        "\n",
        "The next step is to import the required python libraries:\n",
        "*   NumPy:   for scientific computing.\n",
        "*   Matplotlib: a plotting library for Python.\n",
        "*   Matplotlib.pyplot:  a group of functions that allow matplotlib to work like MATLAB.\n",
        "*   Pandas: used for data science/data analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "McOS7Eo8Uf7K"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Import math Library   -- # Check whether some values are NaN\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RrW7nVo0jCx",
        "outputId": "4d31a170-d48d-4477-b02b-f8af77f3c3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     total_bill   tip     sex smoker   day    time  size\n",
            "0         16.99  1.01  Female     No   Sun  Dinner     2\n",
            "1         10.34  1.66    Male     No   Sun  Dinner     3\n",
            "2         21.01  3.50    Male     No   Sun  Dinner     3\n",
            "3         23.68  3.31    Male     No   Sun  Dinner     2\n",
            "4         24.59  3.61  Female     No   Sun  Dinner     4\n",
            "..          ...   ...     ...    ...   ...     ...   ...\n",
            "239       29.03  5.92    Male     No   Sat  Dinner     3\n",
            "240       27.18  2.00  Female    Yes   Sat  Dinner     2\n",
            "241       22.67  2.00    Male    Yes   Sat  Dinner     2\n",
            "242       17.82  1.75    Male     No   Sat  Dinner     2\n",
            "243       18.78  3.00  Female     No  Thur  Dinner     2\n",
            "\n",
            "[244 rows x 7 columns]\n",
            "**************************************************\n",
            "19.78594262295082\n"
          ]
        }
      ],
      "source": [
        "## exp\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tips=sns.load_dataset('tips')\n",
        "print(tips)\n",
        "print('*'*50)\n",
        "print(np.mean(tips['total_bill']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtBUzY5cUyBI"
      },
      "source": [
        "###<font color=\"1497d4\"> Importing the Dataset:</font>\n",
        "\n",
        "\n",
        "The next step is to import the dataset file by either uploading the file on Google Colab or mounting the google drive and empowering Colab to access all files(dataset)on it. Then to copy the dataset to our colab runtime files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fBPFt0FVjrRp"
      },
      "outputs": [],
      "source": [
        "#Mount drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PLv5IywBU061"
      },
      "outputs": [],
      "source": [
        "# Import the dataset file by method1 \n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/results.json   /content/results.json\n",
        "\n",
        "# copy the expermintations files to deal with them\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_at.txt /content/test_at.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_ca.txt /content/test_ca.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_ch.txt /content/test_ch.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_de_al.txt /content/test_de_al.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_de_ni.txt /content/test_de_ni.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_de.txt /content/test_de.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_fr.txt /content/test_fr.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_gb.txt /content/test_gb.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_it.txt /content/test_it.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_ru.txt /content/test_ru.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_us.txt /content/test_us.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl36l5cFVD30"
      },
      "source": [
        "###<font color=\"1497d4\"> Reading the Dataset:</font>\n",
        "**The following Method takes the dataset filename and loads it into data frame called dataset.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XPfKGsjKjfTD"
      },
      "outputs": [],
      "source": [
        "dataset_results_json = pd.read_json('results.json')\n",
        "dataset_results_json['index_column'] = dataset_results_json.index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nefFAUwoj8xM",
        "outputId": "1d311005-558d-414e-8e07-830462307533"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                          test_at.txt  \\\n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  {'reference': 'er ist nach eustache de saint p...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  {'reference': 'eine erste kleinformatige vorst...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  {'reference': 'er blieb ohne erfolg und lag in...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  {'reference': 'mein garten ist teil eines klei...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  {'reference': 'was habe ich damals für einen u...   \n",
              "...                                                                                               ...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...                                                NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...                                                NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...                                                NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...                                                NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...                                                NaN   \n",
              "\n",
              "                                                   test_gb.txt test_it.txt  \\\n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "...                                                        ...         ...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "\n",
              "                                                   test_de_al.txt test_fr.txt  \\\n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "...                                                           ...         ...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "\n",
              "                                                   test_de_ni.txt test_ch.txt  \\\n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "...                                                           ...         ...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...            NaN         NaN   \n",
              "\n",
              "                                                   test_de.txt test_us.txt  \\\n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "...                                                        ...         ...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN         NaN   \n",
              "\n",
              "                                                   test_ca.txt  \\\n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN   \n",
              "...                                                        ...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...         NaN   \n",
              "\n",
              "                                                                                          test_ru.txt  \\\n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...                                                NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...                                                NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...                                                NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...                                                NaN   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...                                                NaN   \n",
              "...                                                                                               ...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  {'reference': 'sie kramte in ihrer tasche', 'h...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  {'reference': 'libreville ist die hauptstadt v...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  {'reference': 'das ramponierte image haben sie...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  {'reference': 'er macht einen unterforderten e...   \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  {'reference': 'mit der kippe sind sie für ihr ...   \n",
              "\n",
              "                                                                                         index_column  \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  /ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...  \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  /ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...  \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  /ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...  \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  /ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...  \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  /ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...  \n",
              "...                                                                                               ...  \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  /ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...  \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  /ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...  \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  /ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...  \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  /ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...  \n",
              "/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-...  /ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...  \n",
              "\n",
              "[49230 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd18532a-cca9-4121-b7cd-f02c47d5d251\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_at.txt</th>\n",
              "      <th>test_gb.txt</th>\n",
              "      <th>test_it.txt</th>\n",
              "      <th>test_de_al.txt</th>\n",
              "      <th>test_fr.txt</th>\n",
              "      <th>test_de_ni.txt</th>\n",
              "      <th>test_ch.txt</th>\n",
              "      <th>test_de.txt</th>\n",
              "      <th>test_us.txt</th>\n",
              "      <th>test_ca.txt</th>\n",
              "      <th>test_ru.txt</th>\n",
              "      <th>index_column</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-2022-07-04/de/clips/common_voice_de_31449916.mp3</th>\n",
              "      <td>{'reference': 'er ist nach eustache de saint p...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-2022-07-04/de/clips/common_voice_de_19730674.mp3</th>\n",
              "      <td>{'reference': 'eine erste kleinformatige vorst...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-2022-07-04/de/clips/common_voice_de_19703888.mp3</th>\n",
              "      <td>{'reference': 'er blieb ohne erfolg und lag in...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-2022-07-04/de/clips/common_voice_de_18507972.mp3</th>\n",
              "      <td>{'reference': 'mein garten ist teil eines klei...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-2022-07-04/de/clips/common_voice_de_24131267.mp3</th>\n",
              "      <td>{'reference': 'was habe ich damals für einen u...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-2022-07-04/de/clips/common_voice_de_17645865.mp3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'reference': 'sie kramte in ihrer tasche', 'h...</td>\n",
              "      <td>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-2022-07-04/de/clips/common_voice_de_17645866.mp3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'reference': 'libreville ist die hauptstadt v...</td>\n",
              "      <td>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-2022-07-04/de/clips/common_voice_de_18862913.mp3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'reference': 'das ramponierte image haben sie...</td>\n",
              "      <td>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-2022-07-04/de/clips/common_voice_de_18862914.mp3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'reference': 'er macht einen unterforderten e...</td>\n",
              "      <td>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0-2022-07-04/de/clips/common_voice_de_18862915.mp3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'reference': 'mit der kippe sind sie für ihr ...</td>\n",
              "      <td>/ds/audio/CommonVoiceCorpus10.0/cv-corpus-10.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49230 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd18532a-cca9-4121-b7cd-f02c47d5d251')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cd18532a-cca9-4121-b7cd-f02c47d5d251 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cd18532a-cca9-4121-b7cd-f02c47d5d251');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dataset_results_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "5mMm3i0ulDt5",
        "outputId": "b08a2be9-bb0c-4033-eee4-6715c1d2c720"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-59deece55d52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset_results_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmember\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_results_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'members'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmember\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'John Doe#0001'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmember\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'posessions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msomething\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ],
      "source": [
        "import json\n",
        "dataset_results_json = json.load(results.json)\n",
        "for member in dataset_results_json['members']:\n",
        "    if member['username'] == 'John Doe#0001':\n",
        "        member['posessions'].append(something)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi5ZwXWulcrM"
      },
      "outputs": [],
      "source": [
        "date_index = pd.date_range('1/1/2010', periods=6, freq='D')\n",
        "df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]},\n",
        "                   index=date_index)\n",
        "df2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxV3Kiz1F-zv"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame({'accent_short':['at','gb','it','de_al','fr','de_ni','ch','de','us','ca','ru'],\n",
        "                                          'accent_long':['Österreichisches Deutsch','Britisches Deutsch','Italienisch Deutsch','Alemannische Färbung,Schweizer Standart Deutsch',\n",
        "'Französisch Deutsch','Niederländisch Deutsch','Schweizerdeutsch','Deutschland Deutsch','Amerikanisches Deutsch','Kanadisches Deutsch','Russisch Deutsch'],\n",
        "                                        'test_accent_txt':['test_at.txt','test_gb.txt','test_it.txt','test_de_al.txt','test_fr.txt','test_de_ni.txt','test_ch.txt',\n",
        "               'test_de.txt','test_us.txt','test_ca.txt','test_ru.txt']})\n",
        "df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YOngn6EGI9n"
      },
      "outputs": [],
      "source": [
        "new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n",
        "             'Chrome','111','Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n",
        "             'Chrome']\n",
        "df1=df.reindex(new_index)\n",
        "df1['accent_short']=np.where(df['accent_short']!=np.NaN,df['accent_short'],'000')\n",
        "df1['accent_long']=np.where(df['accent_long']!=np.NaN,df['accent_long'],'000')\n",
        "df1['test_accent_txt']=np.where(df['test_accent_txt']!=np.NaN,df['test_accent_txt'],'000')\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUVN4YE71D5I"
      },
      "outputs": [],
      "source": [
        "#Count non-NA cells for each column or row.\n",
        "\n",
        "import pandas as pd\n",
        "datasetcount= pd.read_json ('results.json')   \n",
        "print(\"*************************************\" )\n",
        "print(datasetcount.count())\n",
        "print(\"*************************************\" )\n",
        "s=pd.Series(datasetcount.any)\n",
        "print(s.count())\n",
        "datasetcount.drop(datasetcount[(datasetcount['test_at.txt'].isna())].index, inplace=True)\n",
        "print(\"*************************************\" )\n",
        "print(datasetcount.count()) #Count non-NA cells for each column or row.\n",
        "print(\"*************************************\" )\n",
        "s=pd.Series(datasetcount.any)\n",
        "print(\"*************************************\" )\n",
        "print(s.count())\n",
        "#***************************************\n",
        "\n",
        "'''\n",
        "delayed_flights = data[data['delayed'] == True] #filter to only rows where delayer == True\n",
        "delayed_flights['unique_carrier'].value_counts() #count the number of rows for each carrier\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8-ON0yYHI5z"
      },
      "outputs": [],
      "source": [
        "# How do I replace NaN with 0 in Python?\n",
        "# Replace NaN Values with Zeros in Pandas DataFrame\n",
        "\n",
        "# (1) For a single column using Pandas: df['DataFrame Column'] = df['DataFrame Column'].fillna(0)\n",
        "# (2) For a single column using NumPy: df['DataFrame Column'] = df['DataFrame Column'].replace(np.nan, 0)\n",
        "# (3) For an entire DataFrame using Pandas: df.fillna(0)\n",
        "\n",
        "#By default, dropna() will drop all rows in which any null value is present:\n",
        "# df.dropna()\n",
        "#***************************************************\n",
        "# Operating on Null Values\n",
        "# As we have seen, Pandas treats None and NaN as essentially interchangeable for indicating missing or null values. To facilitate this convention, there are several useful methods for detecting, removing, and replacing null values in Pandas data structures. They are:\n",
        "\n",
        "# isnull(): generate a boolean mask indicating missing values\n",
        "# notnull(): opposite of isnull()\n",
        "# dropna(): return a filtered version of the data\n",
        "# fillna(): return a copy of the data with missing values filled or imputed\n",
        "# We will finish this section with a brief discussion and demonstration of these routines:\n",
        "#*********************************************************\n",
        "\n",
        "\n",
        "#https://www.oreilly.com/content/handling-missing-data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwq0_VcfA7y2"
      },
      "outputs": [],
      "source": [
        "# read a TSV file/Dataset\n",
        "# Duration distribution of audiofiles per speaker's accent\n",
        "\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/cv-corpus04072022/de/validated.tsv /content/validated.tsv\n",
        "\n",
        "\n",
        "#,y=\"speaker's accent\",x=\"Duration distribution\"\n",
        "import pandas as pd\n",
        "dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t')\n",
        "dataset_validated_tsv.drop(dataset_validated_tsv[(dataset_validated_tsv['accents'].isna())].index, inplace=True)\n",
        "dataset_validated_tsv\n",
        "print('*******************print Labels************************')\n",
        "\n",
        "for label, content in dataset_validated_tsv.items():\n",
        "    print(f'label: {label}')\n",
        "    #print(f'content: {content}', sep='\\n')\n",
        "\n",
        "print('*********************print Contents**********************')\n",
        "\n",
        "for label, content in dataset_validated_tsv.items():\n",
        "    #print(f'label: {label}')\n",
        "    print(f'content: {content}', sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW9oM4FSz4H9"
      },
      "outputs": [],
      "source": [
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/cv-corpus04072022/de/validated.tsv /content/validated.tsv\n",
        "#read a TSV file/Dataset\n",
        "\n",
        "import pandas as pd\n",
        "tsv_data = pd.read_csv('validated.tsv', sep='\\t')\n",
        "tsv_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "139835UQXY7O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ3U-F5X2DzN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2SzHZ004l75"
      },
      "outputs": [],
      "source": [
        "#plot the dataset\n",
        "import pandas as pd\n",
        "datasetsum= pd.read_json ('results.json')   \n",
        "print(\"*************************************\" )\n",
        "######print(datasetsum.fillna(0)) # Filling missing values: fillna NaN --> 0\n",
        "\n",
        "#By default, dropna() will drop all rows in which any null value is present:\n",
        "# df.dropna()\n",
        "\n",
        "#datasetsum.col =pd.to_numeric(datasetsum.col, errors ='coerce').fillna(0).astype('int')\n",
        "# datasetsum.columns=pd.to_numeric(datasetsum.column,errors='Halllloooooooo').fillna(1)\n",
        "print(\"*************************************\" )\n",
        "datasetsum_1=datasetsum.fillna(0)\n",
        "datasetsum_1.replace(np.nan, 1)\n",
        "#print(datasetsum_2)\n",
        "print(\"*************************************\" )\n",
        "# print(datasetsum['test_at.txt'].fillna(1) )\n",
        "#datasetsum.plot()\n",
        "#sns.displot(dataset_at,y=\"test_at.txt\",x=\"test_gp.txt\")\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "# print(\"*************************************\" )\n",
        "# s=pd.Series(datasetcount.any)\n",
        "# print(s.count())\n",
        "# datasetcount.drop(datasetcount[(datasetcount['test_at.txt'].isna())].index, inplace=True)\n",
        "# print(\"*************************************\" )\n",
        "# print(datasetcount.count()) #Count non-NA cells for each column or row.\n",
        "# print(\"*************************************\" )\n",
        "# s=pd.Series(datasetcount.any)\n",
        "# print(\"*************************************\" )\n",
        "# print(s.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAxCctMi4z29"
      },
      "source": [
        "###<font color=\"1497d4\">The descriptive statistics and computational methods discussed in the data structure overview (and listed here and here) are all written to account for missing data. </font>\n",
        "**For example:**\n",
        "*   When countming data, NA (missing) values will be treated as zero.\n",
        "*   If the data are all NA, the result will be 0.\n",
        "*   Cumulative methods like cumcount() and cumprod() ignore NA values by default, but preserve them in the resulting arrays. To override this behaviour and include NA values, use skipna=False.\n",
        "\n",
        "http://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#experimental-na-scalar-to-denote-missing-values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPONjltmUQ1Y"
      },
      "outputs": [],
      "source": [
        "#plot the dataset\n",
        "import pandas as pd\n",
        "dataset= pd.read_json ('results.json')   \n",
        "#dataset_test_at = pd.read_csv('test_at.txt', header=None, names=['audio_filepath','text','duration'], sep='\\t')\n",
        "dataset = dataset.fillna('') # replace missing values with '' \n",
        "pd.options.mode.use_inf_as_na = True\n",
        "#dataset.plot()\n",
        "#sns.displot(dataset_at,y=\"test_at.txt\",x=\"test_gp.txt\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0h3nDKAZaCp"
      },
      "source": [
        "###<font color=\"1497d4\"> Cleaning our dataset</font>\n",
        "\n",
        "We have 49230  rows in our dataset. Consequently, we will delete the rows which do not meet the conditions (Values range). otherwise it changed normaly with the mean of the field in the small dataset.\n",
        "After deleting the rows which have missing data. We relize that the new number 4495 rows. !!!!!!!!??????"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C7REjaHUjTy"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "# Check the initial shape of the DataFrame\n",
        "# Delete rows where case numbers are less than 1\n",
        "# This deletion is completed by \"selecting\" rows where case numbers are non zero\n",
        "dataset.drop(dataset[(dataset['VQ'] < 1) | (dataset['VF'] < 1)|(dataset['VU'] < 1)|(dataset['VD'] < 1)].index, inplace=True)\n",
        "dataset\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j1VY_ZeUjO_"
      },
      "outputs": [],
      "source": [
        " \n",
        "# Check the initial shape of the DataFrame\n",
        "# Delete rows where case numbers are less than 1\n",
        "# This deletion is completed by \"selecting\" rows where case numbers are non zero\n",
        "# (dataset['test_at.txt'].isna())|\n",
        "\n",
        "#dataset.drop(dataset[(dataset['test_gb.txt'].isna())|(dataset['test_it.txt'].isna())|(dataset['test_de_al.txt'] .isna())|(dataset['test_fr.txt'].isna())|(dataset['test_de_ni.txt'] .isna())|(dataset['test_ch.txt'].isna())|(dataset['test_de.txt'].isna())|(dataset['test_us.txt'].isna())|(dataset['test_ca.txt'].isna())|(dataset['test_ru.txt'].isna())].index, inplace=True)\n",
        "\n",
        "#dataset.drop(dataset[(dataset['test_gb.txt'].isna())|(dataset['test_it.txt'].isna())|(dataset['test_de_al.txt'] .isna())|(dataset['test_fr.txt'].isna())|(dataset['test_de_ni.txt'] .isna())|(dataset['test_ch.txt'].isna())|(dataset['test_de.txt'].isna())|(dataset['test_us.txt'].isna())|(dataset['test_ca.txt'].isna())|(dataset['test_ru.txt'].isna())].index, inplace=True)\n",
        "dataset_results_json= pd.read_json ('results.json')   \n",
        "dataset_results_json.drop(dataset_results_json[(dataset_results_json['test_at.txt'].isna())].index, inplace=True)\n",
        "# print(\" **************************************************\")\n",
        "# print(\"The dataset of test_at.txt Values\",\" **************************************************\",dataset_results_json.head())\n",
        "# print(\" **************************************************\")\n",
        "# print(\"The dataset's Shape of test_at.txt\",dataset_results_json.shape)\n",
        "dataset_results_json.head()\n",
        "dataset_results_json['test_at.txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZCBqckcb4Kr"
      },
      "outputs": [],
      "source": [
        "dataset_results_json.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryrKgRisCrV0"
      },
      "outputs": [],
      "source": [
        "dataset_results_json= pd.read_json ('results.json')   \n",
        "dataset_results_json.drop(dataset_results_json[(dataset_results_json['test_at.txt'].isna())].index, inplace=True)\n",
        "print(dataset_results_json[\"test_at.txt\"])\n",
        "print(\" **************************************************\")\n",
        "print(dataset_results_json[\"test_at.txt\"])\n",
        "\n",
        "# print(dataset_at)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Ct9AQYxa46"
      },
      "source": [
        "###<font color=\"1497d4\"> **You can find the test data, split by accent, in the data zip. The results are also within that zip file, named results.json. Here are some numbers and plots that we would like to have.**</font>\n",
        "\n",
        "1.   **Duration distribution of audiofiles per speaker's accent, gender and age group, both as a plot and in terms of mean and variance.**\n",
        "2.   **WER distribution per speaker's accent, gender, age group \n",
        "and duration, both as a plot and in terms of mean and variance.**\n",
        " \n",
        "I am looking forward to your analysis!  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJhzv96txp1o"
      },
      "source": [
        "#### **Duration distribution of audiofiles per speaker's accent ,both as a plot and in terms of mean and variance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDCirTNZZBOB"
      },
      "outputs": [],
      "source": [
        "# read a TSV file/Dataset\n",
        "# Duration distribution of audiofiles per speaker's accent\n",
        "\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/cv-corpus04072022/de/validated.tsv /content/validated.tsv\n",
        "\n",
        "\n",
        "#,y=\"speaker's accent\",x=\"Duration distribution\"\n",
        "import pandas as pd\n",
        "tsv_data = pd.read_csv('validated.tsv', sep='\\t')\n",
        "tsv_data.drop(tsv_data[(tsv_data['accents'].isna())].index, inplace=True)\n",
        "tsv_data\n",
        "print('*******************print Labels************************')\n",
        "\n",
        "for label, content in tsv_data.items():\n",
        "    print(f'label: {label}')\n",
        "    #print(f'content: {content}', sep='\\n')\n",
        "\n",
        "print('*********************print Contents**********************')\n",
        "\n",
        "for label, content in tsv_data.items():\n",
        "    #print(f'label: {label}')\n",
        "    print(f'content: {content}', sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb-HkYVF4EBD"
      },
      "outputs": [],
      "source": [
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_at.txt /content/test_at.txt\n",
        "\n",
        "# read the TSV file/Dataset *********test_at.txt**********\n",
        "# Duration distribution of audiofiles per speaker's AT accent \n",
        "\n",
        "dataset_test_at = pd.read_csv('test_at.txt')\n",
        "#dataset_test_at = pd.read_csv('test_at.txt', sep='\\t')\n",
        "#print('*******************************************')\n",
        "dataset_test_at.info()\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFtBEfQLo_cJ"
      },
      "outputs": [],
      "source": [
        "# ! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_ca.txt /content/test_ca.txt\n",
        "\n",
        "# dataset_test_ca = pd.read_csv('test_ca.txt',index_col=' \"duration\": 3.1}')\n",
        "# dataset_test_ca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l88yvChw0gW9"
      },
      "outputs": [],
      "source": [
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_at.txt /content/test_at.txt\n",
        "\n",
        "# read the TSV file/Dataset test_at.txt\n",
        "# Duration distribution of audiofiles per speaker's accent\n",
        "\n",
        "dataset_test_at = pd.read_csv('test_at.txt', sep='\\t')\n",
        "print('*******************************************')\n",
        "print(dataset_test_at)\n",
        "print('*******************************************')\n",
        "dataset_test_at.info()\n",
        "print('*******************************************')\n",
        "print('*********************indexes**********************')\n",
        "#dataset_test_at[' \"duration\": 4.9}']\n",
        "\n",
        "print('*******************************************')\n",
        "dataset_test_at = pd.read_csv('test_at.txt')\n",
        "print('*******************************************')\n",
        "print(dataset_test_at)\n",
        "print('*******************************************')\n",
        "dataset_test_at.info()\n",
        "print('*******************************************')\n",
        "print('*********************indexes**********************')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToP0pUXbpoE-"
      },
      "outputs": [],
      "source": [
        "#How to Print the Dataset Columns (Labels) and Rows (contents)\n",
        "for label, content in dataset_test_at.items():\n",
        "    print(f'label: {label}')\n",
        "    #print(f'content: {content}', sep='\\n')\n",
        "\n",
        "#dataset_test_at[' \"duration\": 4.9}'].index\n",
        "\n",
        "\n",
        "#dataset_test_at.drop(dataset_test_at[(dataset_test_at['accents'].isna())].index, inplace=True)\n",
        "\n",
        "\n",
        "# tsv_data.drop(tsv_data[(tsv_data['accents'].isna())].index, inplace=True)\n",
        "# tsv_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb__qNnoppuj"
      },
      "outputs": [],
      "source": [
        "dataset_test_at.drop(dataset_test_at[(dataset_test_at[' \"duration\": 4.9}'].isna())].index, inplace=True)\n",
        "#[*dataset_test_at]\n",
        "dataset_test_at.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUAT9OeOsiUg"
      },
      "outputs": [],
      "source": [
        "#,y=\"speaker's accent\",x=\"Duration distribution\"\n",
        "dataset_test_at.drop(dataset_test_at[(dataset_test_at[' \"duration\": 4.9}'].isna())].index, inplace=True)\n",
        "dataset_test_at\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFsMLzex_MdS"
      },
      "outputs": [],
      "source": [
        "#,y=\"speaker's accent\",x=\"Duration distribution\"\n",
        "import re # import the library of research about special value in String \n",
        "# add Columns Names to the Dataset: header=None, names=['audio_filepath','text','duration']\n",
        "dataset_test_at = pd.read_csv('test_at.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "'''\n",
        "df['Numbers Only'] = df['Numbers and Text'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "'''\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called\n",
        "dataset_test_at['duration_numeric'] = dataset_test_at['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_at['duration_numeric'] =dataset_test_at['duration_numeric'].div(10)\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called \n",
        "#dataset_test_at['audio_filepath_numeric'] = dataset_test_at['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_at['audio_filepath_numeric'] = dataset_test_at['audio_filepath'].str[103:111]\n",
        "\n",
        "###dataset_test_at['audio_filepath_numeric']=re.search('AAA(.+?)ZZZ', text)\n",
        "dataset_test_at\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjKpVo9UM01S"
      },
      "source": [
        "#### **Duration distribution of audiofiles per speaker's accent AT,CA, CH, DE_AL, DE_ni, DE, FR, GB, IT, RU, US. both as a plot and in terms of mean and variance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G2aro7FQjAc"
      },
      "outputs": [],
      "source": [
        "dataset_test_at.mean(0)\n",
        "################################## Accent AT ###########################################\n",
        "print('################################## Accent AT ###########################################')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_e0hA6GmZeR"
      },
      "outputs": [],
      "source": [
        "dataset_test_at"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1vouEJ3NY4-"
      },
      "outputs": [],
      "source": [
        "# Duration distribution of audiofiles per speaker's accent AT,CA, CH, DE_AL, DE_ni, DE, FR, GB, IT, RU, US. both as a plot and in terms of mean and variance.\n",
        "import re # import the library of research about special value in String \n",
        "################################## Accent AT ###########################################\n",
        "print('################################## Accent AT ###########################################')\n",
        "# add Columns Names to the Dataset: header=None, names=['audio_filepath','text','duration']\n",
        "dataset_test_at = pd.read_csv('test_at.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "\n",
        "'''\n",
        "df['Numbers Only'] = df['Numbers and Text'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "'''\n",
        "\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called duration_numeric\n",
        "dataset_test_at['duration_numeric'] = dataset_test_at['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_at['duration_numeric'] =dataset_test_at['duration_numeric'].div(10)\n",
        "# extract the audio files ID values from the first Column and store it in a new column called audio_filepath_numeric\n",
        "#dataset_test_at['audio_filepath_numeric'] = dataset_test_at['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_at['audio_filepath_numeric'] = dataset_test_at['audio_filepath'].str[-13:-5]\n",
        "\n",
        "##dataset_test_at['audio_filepath_numeric']=re.search('AAA(.+?)ZZZ', text)\n",
        "print(dataset_test_at)\n",
        "################################### Accent CA ##########################################\n",
        "print('################################## Accent CA ###########################################')\n",
        "dataset_test_ca = pd.read_csv('test_ca.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called duration_numeric\n",
        "dataset_test_ca['duration_numeric'] = dataset_test_ca['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_ca['duration_numeric'] =dataset_test_ca['duration_numeric'].div(10)\n",
        "# extract the audio files ID values from the first Column and store it in a new column called audio_filepath_numeric\n",
        "#dataset_test_ca['audio_filepath_numeric'] = dataset_test_ca['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_ca['audio_filepath_numeric'] = dataset_test_ca['audio_filepath'].str[-13:-5]\n",
        "\n",
        "print(dataset_test_ca)\n",
        "##################################### Accent CH ########################################\n",
        "print('################################## Accent CH ###########################################')\n",
        "dataset_test_ch = pd.read_csv('test_ch.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called duration_numeric\n",
        "dataset_test_ch['duration_numeric'] = dataset_test_ch['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_ch['duration_numeric'] =dataset_test_ch['duration_numeric'].div(10)\n",
        "# extract the audio files ID values from the first Column and store it in a new column called audio_filepath_numeric\n",
        "#dataset_test_ch['audio_filepath_numeric'] = dataset_test_ch['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_ch['audio_filepath_numeric'] = dataset_test_ch['audio_filepath'].str[-13:-5]\n",
        "\n",
        "print(dataset_test_ch)\n",
        "##################################### Accent DE_AL ########################################\n",
        "print('################################## Accent DE_AL ###########################################')\n",
        "dataset_test_de_al = pd.read_csv('test_de_al.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called duration_numeric\n",
        "dataset_test_de_al['duration_numeric'] = dataset_test_de_al['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_de_al['duration_numeric'] =dataset_test_de_al['duration_numeric'].div(10)\n",
        "# extract the audio files ID values from the first Column and store it in a new column called audio_filepath_numeric\n",
        "#dataset_test_de_al['audio_filepath_numeric'] = dataset_test_de_al['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_de_al['audio_filepath_numeric'] = dataset_test_de_al['audio_filepath'].str[-13:-5]\n",
        "\n",
        "print(dataset_test_de_al)\n",
        "##################################### Accent DE_NI ########################################\n",
        "print('################################## Accent DE_NI ###########################################')\n",
        "dataset_test_de_ni = pd.read_csv('test_de_ni.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called duration_numeric\n",
        "dataset_test_de_ni['duration_numeric'] = dataset_test_de_ni['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_de_ni['duration_numeric'] =dataset_test_de_ni['duration_numeric'].div(10)\n",
        "# extract the audio files ID values from the first Column and store it in a new column called audio_filepath_numeric\n",
        "#dataset_test_de_ni['audio_filepath_numeric'] = dataset_test_de_ni['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_de_ni['audio_filepath_numeric'] = dataset_test_de_ni['audio_filepath'].str[-13:-5]\n",
        "\n",
        "###dataset_test_de_ni['audio_filepath_numeric']=re.search('AAA(.+?)ZZZ', text)\n",
        "print(dataset_test_de_ni)\n",
        "##################################### Accent DE ########################################\n",
        "print('################################## Accent DE ###########################################')\n",
        "dataset_test_de = pd.read_csv('test_de.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called duration_numeric\n",
        "dataset_test_de['duration_numeric'] = dataset_test_de['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_de['duration_numeric'] =dataset_test_de['duration_numeric'].div(10)\n",
        "# extract the audio files ID values from the first Column and store it in a new column called audio_filepath_numeric\n",
        "#dataset_test_de['audio_filepath_numeric'] = dataset_test_de['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_de['audio_filepath_numeric'] = dataset_test_de['audio_filepath'].str[-13:-5]\n",
        "\n",
        "###dataset_test_de['audio_filepath_numeric']=re.search('AAA(.+?)ZZZ', text)\n",
        "print(dataset_test_de)\n",
        "##################################### Accent FR ########################################\n",
        "print('################################## Accent FR ###########################################')\n",
        "dataset_test_fr = pd.read_csv('test_fr.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called duration_numeric\n",
        "dataset_test_fr['duration_numeric'] = dataset_test_fr['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_fr['duration_numeric'] =dataset_test_fr['duration_numeric'].div(10)\n",
        "# extract the audio files ID values from the first Column and store it in a new column called audio_filepath_numeric\n",
        "#dataset_test_fr['audio_filepath_numeric'] = dataset_test_fr['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_fr['audio_filepath_numeric'] = dataset_test_fr['audio_filepath'].str[-13:-5]\n",
        "\n",
        "###dataset_test_fr['audio_filepath_numeric']=re.search('AAA(.+?)ZZZ', text)\n",
        "print(dataset_test_fr)\n",
        "##################################### Accent GB ########################################\n",
        "print('################################## Accent GB ###########################################')\n",
        "dataset_test_gb = pd.read_csv('test_gb.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called duration_numeric\n",
        "dataset_test_gb['duration_numeric'] = dataset_test_gb['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_gb['duration_numeric'] =dataset_test_gb['duration_numeric'].div(10)\n",
        "# extract the audio files ID values from the first Column and store it in a new column called audio_filepath_numeric\n",
        "#dataset_test_gb['audio_filepath_numeric'] = dataset_test_gb['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_gb['audio_filepath_numeric'] = dataset_test_gb['audio_filepath'].str[-13:-5]\n",
        "\n",
        "###dataset_test_gb['audio_filepath_numeric']=re.search('AAA(.+?)ZZZ', text)\n",
        "print(dataset_test_gb)\n",
        "##################################### Accent IT ########################################\n",
        "print('################################## Accent IT ###########################################')\n",
        "dataset_test_it = pd.read_csv('test_it.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called duration_numeric\n",
        "dataset_test_it['duration_numeric'] = dataset_test_it['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_it['duration_numeric'] =dataset_test_it['duration_numeric'].div(10)\n",
        "# extract the audio files ID values from the first Column and store it in a new column called audio_filepath_numeric\n",
        "#dataset_test_it['audio_filepath_numeric'] = dataset_test_it['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_it['audio_filepath_numeric'] = dataset_test_it['audio_filepath'].str[-13:-5]\n",
        "\n",
        "###dataset_test_it['audio_filepath_numeric']=re.search('AAA(.+?)ZZZ', text)\n",
        "print(dataset_test_it)\n",
        "##################################### Accent RU ########################################\n",
        "print('################################## Accent RU ###########################################')\n",
        "dataset_test_ru = pd.read_csv('test_ru.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called duration_numeric\n",
        "dataset_test_ru['duration_numeric'] = dataset_test_ru['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_ru['duration_numeric'] =dataset_test_ru['duration_numeric'].div(10)\n",
        "# extract the audio files ID values from the first Column and store it in a new column called audio_filepath_numeric\n",
        "#dataset_test_ru['audio_filepath_numeric'] = dataset_test_ru['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_ru['audio_filepath_numeric'] = dataset_test_ru['audio_filepath'].str[-13:-5]\n",
        "\n",
        "###dataset_test_ru['audio_filepath_numeric']=re.search('AAA(.+?)ZZZ', text)\n",
        "print(dataset_test_ru)\n",
        "##################################### Accent US ########################################\n",
        "print('################################## Accent US ###########################################')\n",
        "dataset_test_us = pd.read_csv('test_us.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called duration_numeric\n",
        "dataset_test_us['duration_numeric'] = dataset_test_us['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_us['duration_numeric'] =dataset_test_us['duration_numeric'].div(10)\n",
        "# extract the audio files ID values from the first Column and store it in a new column called audio_filepath_numeric\n",
        "#dataset_test_us['audio_filepath_numeric'] = dataset_test_us['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_us['audio_filepath_numeric'] = dataset_test_us['audio_filepath'].str[-13:-5]\n",
        "\n",
        "###dataset_test_us['audio_filepath_numeric']=re.search('AAA(.+?)ZZZ', text)\n",
        "print(dataset_test_us)\n",
        "\n",
        "##################################### Accent Mean ########################################\n",
        "\n",
        "print('################################## Accent Mean ###########################################')\n",
        "dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us']\n",
        "dataset_name=\"dataset_test_\"\n",
        "\n",
        "for i in range(len(dataset_test_)):\n",
        "  Accent_Values=dataset_test_[i]\n",
        "  #print('dataset_test_'+Accent_Values+' mean =',Accent_Values)\n",
        "  \n",
        "  Accent_type=dataset_name+Accent_Values\n",
        "  print(f'{Accent_type}_mean =',Accent_type)\n",
        "print('################################## Accent Mean ###########################################')\n",
        "\n",
        "print(np.mean(dataset_test_at['duration_numeric']))\n",
        "print(np.mean(dataset_test_ca['duration_numeric']))\n",
        "print(np.mean(dataset_test_ch['duration_numeric']))\n",
        "print(np.mean(dataset_test_de_al['duration_numeric']))\n",
        "print(np.mean(dataset_test_de_ni['duration_numeric']))\n",
        "print(np.mean(dataset_test_de['duration_numeric']))\n",
        "print(np.mean(dataset_test_fr['duration_numeric']))\n",
        "print(np.mean(dataset_test_gb['duration_numeric']))\n",
        "print(np.mean(dataset_test_it['duration_numeric']))\n",
        "print(np.mean(dataset_test_ru['duration_numeric']))\n",
        "print(np.mean(dataset_test_us['duration_numeric']))\n",
        "\n",
        "# for i in range(1,12):\n",
        "#   dataset_accent_all=[i]\n",
        "#   print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHM1R9feef_G"
      },
      "outputs": [],
      "source": [
        "print('################################## Accent Mean ###########################################')\n",
        "dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us']\n",
        "dataset_test_[0]\n",
        "for i in range(len(dataset_test_)):\n",
        "  Accent_Values=dataset_test_[i]\n",
        "  print('dataset_test_'+Accent_Values+' mean =',Accent_Values)\n",
        "print('################################## Accent Mean ###########################################')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpwokaTrs7VQ"
      },
      "outputs": [],
      "source": [
        "#   return np.mean(DatasetName)\n",
        " \n",
        "\n",
        "print('################################## Accent Mean ###########################################')\n",
        "list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us']\n",
        "dataset_dataset_test_=pd.DataFrame(list_dataset_test_, columns=['accent']) \n",
        "\n",
        "dataset_name=\"dataset_test_\"\n",
        "dataset_column=\"['duration_numeric']\"\n",
        "\n",
        "dataset_all_accent=[] # create a dataset called \"dataset_all_accent\" required column of mean calculations of all Accents\n",
        "list_all_accent_cal=[] # list of the results of mean calculations of all accents\n",
        "list_all_accent=[]\n",
        "list_all_accent_mean=[] # create a dataset called \"list_all_accent_mean\" contains the mean calculations of all Accents\n",
        "for i in range(len(list_dataset_test_)):\n",
        "  \n",
        "  Accent_Values=dataset_test_[i]\n",
        "  Accent_type=dataset_name+Accent_Values+dataset_column\n",
        "  ###list_all_accent_cal.insert(i,5+i)\n",
        "  #list_all_accent_cal[i]=np.mean(dataset_test_at['duration_numeric'])\n",
        "  ###print(f' \\'np.mean({Accent_type}\\'),')\n",
        "  temp_list=f' {Accent_type}'\n",
        "  temp_mean_list=f' np.mean({Accent_type})'\n",
        "\n",
        "  \n",
        "\n",
        "  #list_all_accent_cal.insert(i,np.mean(temp_list))\n",
        "\n",
        "  print(f'np.mean({Accent_type})')\n",
        "  # create a dataset called \"list_all_accent_mean\" contains the mean calculations for all Accents\n",
        "  ###dataset_all_accent.insert(i,Accent_type)\n",
        "  list_all_accent.insert(i,temp_list)\n",
        "  list_all_accent_mean.insert(i,temp_mean_list)\n",
        "  \n",
        "# print('#'*60,'dataset_all_accent')\n",
        "# print(dataset_all_accent,'\\n')\n",
        "\n",
        "print('#'*30,'list_all_accent=','#'*30)\n",
        "#print(f'{list_all_accent}',sep=os.linesep)\n",
        "print(f' \\n {list_all_accent}')\n",
        "\n",
        "\n",
        "print('#'*30,'list_all_accent_mean=','#'*30)\n",
        "#print(f'{list_all_accent_mean}',sep=os.linesep)\n",
        "print(f'\\n  {list_all_accent_mean}')\n",
        "\n",
        "print('#'*30,'list_all_accent_cal=','#'*30)\n",
        "print(f'\\n  {list_all_accent_cal}')\n",
        "\n",
        "\n",
        " \n",
        "\n",
        " \n",
        "# np.mean(dataset_test_at['duration_numeric'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqGdfnLZjoPH"
      },
      "outputs": [],
      "source": [
        "list_all_accent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeiJ_ERN2BJv"
      },
      "outputs": [],
      "source": [
        "list_all_accent_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72Ze9aLV2DlF"
      },
      "outputs": [],
      "source": [
        "dataset_all_accent= pd.DataFrame (list_all_accent, columns = ['accent'])\n",
        "dataset_all_accent_mean = pd.DataFrame (list_all_accent_mean, columns = ['mean_of_accent'])\n",
        "# dataset_all_accent_mean['Column'] = df['Column'].apply(ast.literal_eval)\n",
        "dataset_all_accent_mean\n",
        "# dataset_all_accent_mean.mean_of_all_accent[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChlwCBSf2GQu"
      },
      "outputs": [],
      "source": [
        "dataset_all_accent_mean.mean_of_accent[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8uAtMOP2IK1"
      },
      "outputs": [],
      "source": [
        "####################################Insert Accent Loop##################################################\n",
        "#######this output will written in an external py file called dataset_accent.py ########################\n",
        "############## To implement it here in colab and get the mean of all accent ############################\n",
        "\n",
        "list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us','stop']\n",
        "list1=['']\n",
        "Accent_=0\n",
        "print('#'*60,'Insert Accent Loop','#'*60)\n",
        "for j in list_dataset_test_:\n",
        "  for i in range(0,11):\n",
        "    #print(i)\n",
        "    # print(onetime)\n",
        "    if j!='stop':\n",
        "      #print(f'Accent_{j}={dataset_all_accent_mean.mean_of_all_accent[i]}')\n",
        "      print(f\"\\n print(' Accent_{j}_Duration_mean= ')\")\n",
        "      print(f\"\\n print({dataset_all_accent_mean.mean_of_accent[i]})\")\n",
        "      stopcal=i+1\n",
        "      j=list_dataset_test_[stopcal]\n",
        "  if j=='stop':\n",
        "    break\n",
        "  print('#'*60,{j},{i},'#'*60)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbhj9I7j3M0t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# remove the script if exists \n",
        "# os.remove(\"dataset_accent.py\")\n",
        "# open script to write in the calculation of Mean of all accent\n",
        "dataset_accent_write_file = open(\"dataset_accent.py\",'w')\n",
        "dataset_accent_write_file.write(\"np.mean(dataset_test_at['duration_numeric'])\")\n",
        "dataset_accent_write_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4kCgGZp4b4e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "! cp /content/test_us.txt\n",
        "# read the script\n",
        "Read_dataset_file=open(\"dataset_accent.py\",'r')\n",
        "print(Read_dataset_file.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbz06x8r4lw0"
      },
      "outputs": [],
      "source": [
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/dataset_accent.py   /content/dataset_accent.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFJulmM3-RN6"
      },
      "outputs": [],
      "source": [
        "# implement the script\n",
        "exec(open(\"dataset_accent.py\").read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWM7TFMSKpZm"
      },
      "outputs": [],
      "source": [
        "# # add to the script\n",
        "# datei = open(\"test3.py\",'a')\n",
        "# datei.write(\"\\n np.mean(dataset_test_ca['duration_numeric'])\")\n",
        "# datei.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVejCY9XCAmA"
      },
      "outputs": [],
      "source": [
        "##############  store the mean results of all Accent in a dataframep##################################################\n",
        "########################this output will written to a list and dataframe #############################################\n",
        "import pandas as pd \n",
        "dataset_accent_all_Duration_mean_result=pd.DataFrame({'accent_type':['Accent_at_Duration_mean=', 'Accnt_ca_Duration_mean=' ,'Accnt_ch_Duration_mean=' , 'Accnt_de_al_Duration_mean=' , 'Accnt_de_ni_Duration_mean=' ,\n",
        "                                                     'Accnt_de_Duration_mean=' ,'Accnt_fr_Duration_mean=' , 'Accnt_gb_Duration_mean=' , 'Accnt_it_Duration_mean=' , 'Accnt_ru_Duration_mean=' , 'Accnt_us_Duration_mean=' ] ,\n",
        "                                      'accent_Duration_mean':[5.1273192436040045,5.942276422764228,5.504779917469051,5.370441988950277,5.6436661698956785,4.842688472496165,\n",
        "                                            5.236254416961131,5.5195945945945954,5.891308793456033,4.192086330935251,5.838144329896908]    })\n",
        "\n",
        "\n",
        "  \n",
        " \n",
        "print('#'*60,'dataset_accent_all_Duration_mean_result','#'*60)\n",
        "print(dataset_accent_all_Duration_mean_result)\n",
        "print('#'*120)\n",
        "# list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us','stop']\n",
        "# list1=['']\n",
        "# Accent_=0\n",
        "\n",
        "# list_accent_all_mean_result=[]\n",
        "# dataset_all_accent_mean=[]\n",
        "# print('#'*60,'Insert Accent Loop','#'*60)\n",
        "# for j in list_dataset_test_:\n",
        "#   for i in range(0,11):\n",
        "#     #print(i)\n",
        "#     # print(onetime)\n",
        "#     if j!='stop':\n",
        "#       #print(f'Accent_{j}={dataset_all_accent_mean.mean_of_all_accent[i]}')\n",
        "#       print(f\"\\n print(' Accent_{j}= ')\")\n",
        "#       print(f\"\\n print({dataset_all_accent_mean.mean_of_all_accent[i]})\")\n",
        "#       stopcal=i+1\n",
        "#       j=list_dataset_test_[stopcal]\n",
        "#       ####list_accent_all_mean_result[i]=dataset_all_accent_mean.mean_of_all_accent[i]\n",
        "#   if j=='stop':\n",
        "#     break\n",
        "#   print('#'*60,{j},{i},'#'*60)\n",
        "\n",
        "# dataset_accent_all_mean_result=pd.DataFrame(list_accent_all_mean_result,columns=[''])\n",
        "# print('#'*60,'dataset_accent_all_mean_result','#'*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JAYsgZRYW5I"
      },
      "outputs": [],
      "source": [
        "# add the accent_Duration_mean column to the dataset_test_at\n",
        "dataset_test_at['accent_Duration_mean'] = np.where(dataset_test_at['audio_filepath']!=np.NaN,5.127319, '000')\n",
        "dataset_test_at"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NtCl5X6Z-kc"
      },
      "outputs": [],
      "source": [
        "# change dataset_test_at['audio_filepath_numeric'] from object to numeric\n",
        "dataset_test_at['audio_filepath_numeric'] = pd.to_numeric(dataset_test_at['audio_filepath_numeric'])\n",
        "dataset_test_at.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9NL6216LyQg"
      },
      "outputs": [],
      "source": [
        "dataset_test_at"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2-_2cz_jzKb"
      },
      "outputs": [],
      "source": [
        "dataset_accent_all_Duration_mean_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIsS8XGUKhV3"
      },
      "outputs": [],
      "source": [
        "dataset_accent_all_Duration_mean_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2hKOMo94aBd"
      },
      "source": [
        "#### Bar Diagram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvlLb8aPZ_xj"
      },
      "outputs": [],
      "source": [
        "#plt.subplot(2,3,1)\n",
        "#fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
        "#plt.legend()\n",
        "\n",
        "# list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us','stop']\n",
        "# list1=['']\n",
        "# Accent_=0\n",
        "# print('#'*60,'Insert Accent Loop','#'*60)\n",
        "# for j in list_dataset_test_:\n",
        "#   for i in range(0,11):\n",
        "#     #print(i)\n",
        "#     # print(onetime)\n",
        "#     if j!='stop':\n",
        "#       #print(f'Accent_{j}={dataset_all_accent_mean.mean_of_all_accent[i]}')\n",
        "#       print(f\"\\n print(' Accent_{j}_Duration_mean= ')\")\n",
        "#       print(f\"\\n print({dataset_all_accent_mean.mean_of_all_accent[i]})\")\n",
        "#       stopcal=i+1\n",
        "#       j=list_dataset_test_[stopcal]\n",
        "#   if j=='stop':\n",
        "#     break\n",
        "#   print('#'*60,{j},{i},'#'*60)\n",
        "# To show only the Accent letters ,we add neu column called accent in dataset_accent_all_Duration_mean_result\n",
        "list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us','stop']\n",
        "list_dataset_test_accent=['Österreichisches Deutsch','Kanadisches Deutsch','Schweizerdeutsch','Alemannische Färbung,Schweizer Standart Deutsch','Niederländisch Deutsch',' Deutschland Deutsch','Französisch Deutsch\t','Britisches Deutsch',' Italienisch Deutsch'\n",
        ",'Russisch Deutsch','Amerikanisches Deutsch']\n",
        "\n",
        "\n",
        "dataset_accent_all_Duration_mean_result['accent']=dataset_dataset_test_['accent']\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "################################## Accent AT ###########################################\n",
        "# position of the figure and figure;s size \n",
        "figure_duriation_distribution_accent=plt.figure(12,figsize=(10,10))  \n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "####>>>>>ax_duriation_distribution_accent_at=figure_duriation_distribution_accent.add_subplot(3,4,1)\n",
        "\n",
        "# for i in range(len(dataset_accent_all_Duration_mean_result)):\n",
        "dataset_test_at['accent_Duration_mean'] = np.where(dataset_test_at['audio_filepath']!=np.NaN,5.127319, '000')\n",
        "\n",
        "# plt.xlabel(\"Speaker's accent - Austria\")\n",
        "# plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "# plt.plot(dataset_test_at['accent_Duration_mean'],dataset_test_at['audio_filepath_numeric'], lw=3, ms=20)\n",
        "font1 = {'family':'serif','color':'blue','size':20}\n",
        "font2 = {'family':'serif','color':'darkred','size':15}\n",
        "\n",
        "plt.title('Duration mean for all Accents',fontdict  = font1)\n",
        "\n",
        "\n",
        "ax = sns.barplot(x = 'accent', y = 'accent_Duration_mean', data = dataset_accent_all_Duration_mean_result)\n",
        "plt.savefig('plotBarDiagrams_Accent_Duration.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po6z_5uz4xTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkDNGjM-xODd"
      },
      "source": [
        "#### **Duration distribution of audiofiles per speaker's gender. both as a plot and in terms of mean and variance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3--bp7142Wh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn9RMG1xqLpa"
      },
      "outputs": [],
      "source": [
        "dataset_test_at"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ABcHc1xBi9H"
      },
      "outputs": [],
      "source": [
        "# How to Compare Values between two Pandas DataFrames using compare_df Function\n",
        "#########################################################################################################\n",
        "######### We will copy a column audio_filepath_numeric from df2  ==> a column audio_filepath_numeric in df2 #######\n",
        "######### To have the same column name  in df1 and df2. then we apply compare_df Function in order#######\n",
        "######### To make compairsion to get match the audio_filepath between df1 and df2. then we need to#######\n",
        "######### copy the gender column from df2 to df1 according to the matched audio_filepath result   ####### \n",
        "################ Then plot the diagram within x=gender y=Duriation from df1  ############################\n",
        "#########################################################################################################\n",
        "# Plot Values Of 2 Columns From Different Datasets \n",
        "##########################################################################\n",
        " \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#from compare_df import * # import compare-df Function\n",
        "\n",
        "\n",
        "dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t')\n",
        "\n",
        "# dataset_test_at\n",
        "# dataset_validated_tsv\n",
        "#dataset_test_at_validated_tsv=[]\n",
        "# get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "##>>>>>>dataset_validated_tsv['audio_filepath_numeric']=pd.to_numeric(dataset_validated_tsv['audio_filepath_numeric'])\n",
        "\n",
        "dataset_validated_tsv\n",
        "\n",
        "\n",
        "# delete or hide the male and NaN values from displaying at dataset_validated_tsv \n",
        "#>>>>>>>dataset_validated_tsv.drop(dataset_validated_tsv[(dataset_validated_tsv['gender'] == 'male') | (dataset_validated_tsv['gender'].isna())].index, inplace=True)\n",
        "### or \n",
        "# dataset_validated_tsv['path_match']=dataset_validated_tsv.drop(dataset_validated_tsv[(dataset_validated_tsv['path_match'].isna())].index, inplace=True)\n",
        "\n",
        "\n",
        "# dataset_test_at_validated_tsv\n",
        "# dataset_test_at\n",
        "# dataset_validated_tsv\n",
        "\n",
        "#df1['gender_from_df2'] = np.where(df1['path_match_ID']== df2['audio_filepath_numeric'], 'NOOOOO',df2['gender'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgRmEWuGmpPj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "dataset_validated_tsv.drop(dataset_validated_tsv[(dataset_validated_tsv['gender'] != 'female') | (dataset_validated_tsv['gender'].isna())| (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!='Österreichisches Deutsch')].index, inplace=True)\n",
        "dataset_validated_tsv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beAGPEGlVrH1"
      },
      "outputs": [],
      "source": [
        "#############################************ Merge using audio_filepath_numeric ***********################################################\n",
        "####################calculate the Mean of duration when the gender is female###########################\n",
        "dataset_test_at_validated_tsv_female=pd.merge(dataset_test_at,dataset_validated_tsv)\n",
        "dataset_test_at_validated_tsv_female['gender']=dataset_test_at_validated_tsv_female['gender'].astype(\"string\")\n",
        "dataset_test_at_validated_tsv_female['accent_Duration_mean_gender']=np.mean(dataset_test_at_validated_tsv_female['duration_numeric'])\n",
        "\n",
        "dataset_test_at_validated_tsv_female"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMf6FveLWMO1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yO9YgqD4gROZ"
      },
      "outputs": [],
      "source": [
        "################**********Male***************#################3\n",
        "dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t')\n",
        "\n",
        "# dataset_test_at\n",
        "# dataset_validated_tsv\n",
        "#dataset_test_at_validated_tsv=[]\n",
        "# get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "##>>>>>>dataset_validated_tsv['audio_filepath_numeric']=pd.to_numeric(dataset_validated_tsv['audio_filepath_numeric'])\n",
        "\n",
        "\n",
        "dataset_validated_tsv.drop(dataset_validated_tsv[(dataset_validated_tsv['gender'] != 'male') | (dataset_validated_tsv['gender'].isna())| (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!='Österreichisches Deutsch')].index, inplace=True)\n",
        "dataset_validated_tsv\n",
        "#############################************ Merge using audio_filepath_numeric ***********################################################\n",
        "####################calculate the Mean of duration when the gender is female###########################\n",
        "dataset_test_at_validated_tsv_male=pd.merge(dataset_test_at,dataset_validated_tsv)\n",
        "dataset_test_at_validated_tsv_male['gender']=dataset_test_at_validated_tsv_male['gender'].astype(\"string\")\n",
        "dataset_test_at_validated_tsv_male['accent_Duration_mean_gender']=np.mean(dataset_test_at_validated_tsv_male['duration_numeric'])\n",
        "\n",
        "dataset_test_at_validated_tsv_male\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WzJdhXXrvzm"
      },
      "outputs": [],
      "source": [
        "dataset_test_at_validated_tsv_family=dataset_test_at_validated_tsv_female.append(dataset_test_at_validated_tsv_male)\n",
        "dataset_test_at_validated_tsv_family"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwO9qo0sWUgW"
      },
      "outputs": [],
      "source": [
        "############################## for loop#######################################\n",
        "list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us']\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#from compare_df import * # import compare-df Function\n",
        "\n",
        "############################# 1- reading the dataset ##########################################################\n",
        "dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t')\n",
        "\n",
        "#################################Extract and calculate the duration_numeric########################################\n",
        "# add Columns Names to the Dataset: header=None, names=['audio_filepath','text','duration']\n",
        "# dataset_test_ca = pd.read_csv('test_ca.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "'''\n",
        "df['Numbers Only'] = df['Numbers and Text'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "'''\n",
        "\n",
        "for i in list_dataset_test_:\n",
        "  # accent=i\n",
        "  # dataset_accent=f'dataset_test_{i}'\n",
        "  # textfile_name=f'test_{i}.txt'\n",
        "  dataset_test_ca = pd.read_csv(f'test_{i}.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "  \n",
        "  # dataset_test_ca=f'dataset_test_{i}'\n",
        "  # extract the numeric values of the durations from the third Column and store it in a new column called\n",
        "  dataset_test_ca['duration_numeric'] = dataset_test_ca['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "  dataset_test_ca['duration_numeric'] =dataset_test_ca['duration_numeric'].div(10)\n",
        "  # extract the numeric values of the durations from the third Column and store it in a new column called \n",
        "  #dataset_test_ca['audio_filepath_numeric'] = dataset_test_ca['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "  dataset_test_ca['audio_filepath_numeric'] = dataset_test_ca['audio_filepath'].str[-13:-5]\n",
        "  print(dataset_test_ca)\n",
        "  print('################################## Gender ###########################################')\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGh6EghB-BHF"
      },
      "source": [
        "####**<font color=\"1497d4\">Bar chart: </font>** **x='accent', y = 'accent_Duration_mean'** **Accent ='Österreichisches Deutsch'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwT96tSL96-M"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#from compare_df import * # import compare-df Function\n",
        "\n",
        "############################# 1- reading the dataset ##########################################################\n",
        "dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t',low_memory=False)\n",
        " \n",
        "#################################Extract and calculate the duration_numeric########################################\n",
        "# add Columns Names to the Dataset: header=None, names=['audio_filepath','text','duration']\n",
        "dataset_test_at = pd.read_csv('test_at.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "'''\n",
        "df['Numbers Only'] = df['Numbers and Text'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "'''\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called\n",
        "dataset_test_at['duration_numeric'] = dataset_test_at['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_at['duration_numeric'] =dataset_test_at['duration_numeric'].div(10)\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called \n",
        "#dataset_test_at['audio_filepath_numeric'] = dataset_test_at['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_at['audio_filepath_numeric'] = dataset_test_at['audio_filepath'].str[-13:-5]\n",
        "\n",
        "\n",
        "dataset_test_at\n",
        "############################# 2- extract the audio_filepath number and change them to integer################## \n",
        "\n",
        "# get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "dataset_test_at['audio_filepath_numeric']=dataset_test_at['audio_filepath_numeric'].astype('int')\n",
        "\n",
        "\n",
        "############################# 3- Drop all NaN gender and NaN Accent values, and show only the female gender and Österreichisches Deutsch rows #############################\n",
        "dataset_validated_tsv.drop(dataset_validated_tsv[(dataset_validated_tsv['gender'] != 'female') | (dataset_validated_tsv['gender'].isna())| (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!='Österreichisches Deutsch')].index, inplace=True)\n",
        "#dataset_validated_tsv\n",
        "\n",
        "\n",
        "\n",
        "############################# 4- Merge using audio_filepath_numeric column ##################################\n",
        "####################5-calculate the Mean of duration when the gender is female###########################\n",
        "dataset_test_at_validated_tsv_female=pd.merge(dataset_test_at,dataset_validated_tsv)\n",
        "dataset_test_at_validated_tsv_female['gender']=dataset_test_at_validated_tsv_female['gender'].astype(\"string\")\n",
        "dataset_test_at_validated_tsv_female['accent_Duration_mean_gender']=np.mean(dataset_test_at_validated_tsv_female['duration_numeric'])\n",
        "#dataset_test_at_validated_tsv_female\n",
        "\n",
        "############################# 6- reading the dataset ##########################################################\n",
        "############################# 7- extract the audio_filepath number and change them to integer################## \n",
        "# get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "\n",
        "dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t',low_memory=False)\n",
        "dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "\n",
        "############################# 8- Drop all NaN gender and NaN Accent values, and show only the male gender and Österreichisches Deutsch rows #############################\n",
        "dataset_validated_tsv.drop(dataset_validated_tsv[(dataset_validated_tsv['gender'] != 'male') | (dataset_validated_tsv['gender'].isna())| (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!='Österreichisches Deutsch')].index, inplace=True)\n",
        "\n",
        "############################# 9- Merge using audio_filepath_numeric column ##################################\n",
        "dataset_test_at_validated_tsv_male=pd.merge(dataset_test_at,dataset_validated_tsv)\n",
        "dataset_test_at_validated_tsv_male['gender']=dataset_test_at_validated_tsv_male['gender'].astype(\"string\")\n",
        "\n",
        "#################### 10- calculate the Mean of duration when the gender is male###########################\n",
        "dataset_test_at_validated_tsv_male['accent_Duration_mean_gender']=np.mean(dataset_test_at_validated_tsv_male['duration_numeric'])\n",
        "#dataset_test_at_validated_tsv_male\n",
        "###### add/append the upper dataset dataset_test_at_validated_tsv_female to the lower dataset dataset_test_at_validated_tsv_male###########################\n",
        "###### and call the result dataset_test_at_validated_tsv_family\n",
        "dataset_test_at_validated_tsv_family=dataset_test_at_validated_tsv_female.append(dataset_test_at_validated_tsv_male)\n",
        "dataset_test_at_validated_tsv_family\n",
        "#################### 11- plot the dataset_test_at_validated_tsv_family in Bar Chart #######################\n",
        "\n",
        "figure_duriation_distribution_accent=plt.figure(12,figsize=(20,20))  \n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "ax_duriation_distribution_accent_at=figure_duriation_distribution_accent.add_subplot(3,4,1)\n",
        "plt.title(\"Österreichisches Deutsch\")\n",
        "ax = sns.barplot(x = 'gender', y = 'accent_Duration_mean_gender', data = dataset_test_at_validated_tsv_family)\n",
        "plt.savefig('plotBarDiagrams_Accent_Duration_Österreichisches_Deutsch.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmK_lrXYUHud"
      },
      "source": [
        "####**<font color=\"1497d4\">Bar chart: </font>** **x='accent', y = 'accent_Duration_mean'** **Accent ='Kanadisches Deutsch'**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWoxVFzL-Wf9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#from compare_df import * # import compare-df Function\n",
        "\n",
        "############################# 1- reading the dataset ##########################################################\n",
        "dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t',low_memory=False)\n",
        "\n",
        "#################################Extract and calculate the duration_numeric########################################\n",
        "# add Columns Names to the Dataset: header=None, names=['audio_filepath','text','duration']\n",
        "dataset_test_ca = pd.read_csv('test_ca.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "'''\n",
        "df['Numbers Only'] = df['Numbers and Text'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "'''\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called\n",
        "dataset_test_ca['duration_numeric'] = dataset_test_ca['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "dataset_test_ca['duration_numeric'] =dataset_test_ca['duration_numeric'].div(10)\n",
        "# extract the numeric values of the durations from the third Column and store it in a new column called \n",
        "#dataset_test_ca['audio_filepath_numeric'] = dataset_test_ca['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "dataset_test_ca['audio_filepath_numeric'] = dataset_test_ca['audio_filepath'].str[-13:-5]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ############################# 2- extract the audio_filepath number and change them to integer################## \n",
        "\n",
        "# get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "dataset_test_ca['audio_filepath_numeric']=dataset_test_ca['audio_filepath_numeric'].astype('int')\n",
        "\n",
        " \n",
        "############################# 3- Drop all NaN gender and NaN Accent values, and show only the female gender and Kanadisches Deutsch rows #############################\n",
        "dataset_validated_tsv.drop(dataset_validated_tsv[(dataset_validated_tsv['gender'] != 'female') | (dataset_validated_tsv['gender'].isna())| \n",
        "(dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!='Kanadisches Deutsch')].index, inplace=True)\n",
        "#dataset_validated_tsv\n",
        "\n",
        "\n",
        "\n",
        "############################# 4- Merge using audio_filepath_numeric column ##################################\n",
        "####################5-calculate the Mean of duration when the gender is female###########################\n",
        "dataset_test_ca_validated_tsv_female=pd.merge(dataset_test_ca,dataset_validated_tsv)\n",
        "dataset_test_ca_validated_tsv_female['gender']=dataset_test_ca_validated_tsv_female['gender'].astype(\"string\")\n",
        "dataset_test_ca_validated_tsv_female['accent_Duration_mean_gender']=np.mean(dataset_test_ca_validated_tsv_female['duration_numeric'])\n",
        "#dataset_test_ca_validated_tsv_female\n",
        "\n",
        "############################# 6- reading the dataset ##########################################################\n",
        "############################# 7- extract the audio_filepath number and change them to integer################## \n",
        "# get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "\n",
        "dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t',low_memory=False)\n",
        "dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "\n",
        "############################# 8- Drop all NaN gender and NaN Accent values, and show only the male gender and Kanadisches Deutsch rows #############################\n",
        "dataset_validated_tsv.drop(dataset_validated_tsv[(dataset_validated_tsv['gender'] != 'male') | (dataset_validated_tsv['gender'].isna())| (dataset_validated_tsv['accents'].isna())\n",
        "| (dataset_validated_tsv['accents']!='Kanadisches Deutsch')].index, inplace=True)\n",
        "\n",
        "############################# 9- Merge using audio_filepath_numeric column ##################################\n",
        "dataset_test_ca_validated_tsv_male=pd.merge(dataset_test_ca,dataset_validated_tsv)\n",
        "dataset_test_ca_validated_tsv_male['gender']=dataset_test_ca_validated_tsv_male['gender'].astype(\"string\")\n",
        "\n",
        "#################### 10- calculate the Mean of duration when the gender is male###########################\n",
        "dataset_test_ca_validated_tsv_male['accent_Duration_mean_gender']=np.mean(dataset_test_ca_validated_tsv_male['duration_numeric'])\n",
        "#dataset_test_ca_validated_tsv_male\n",
        "###### add/append the upper dataset dataset_test_ca_validated_tsv_female to the lower dataset dataset_test_ca_validated_tsv_male###########################\n",
        "###### and call the result dataset_test_ca_validated_tsv_family\n",
        "dataset_test_ca_validated_tsv_family=dataset_test_ca_validated_tsv_female.append(dataset_test_ca_validated_tsv_male)\n",
        "dataset_test_ca_validated_tsv_family\n",
        "#################### 11- plot the dataset_test_ca_validated_tsv_family in Bar Chart #######################\n",
        "\n",
        "figure_duriation_distribution_accent=plt.figure(12,figsize=(20,20))  \n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "\n",
        "ax_duriation_distribution_accent_at=figure_duriation_distribution_accent.add_subplot(3,4,1)\n",
        "font1 = {'family':'serif','color':'blue','size':20}\n",
        "font2 = {'family':'serif','color':'darkred','size':15}\n",
        "\n",
        "plt.title(\"Österreichisches Deutsch\",fontdict  = font1)\n",
        "#ax.legend()\n",
        "ax = sns.barplot(x = 'gender', y = 'accent_Duration_mean_gender', data = dataset_test_at_validated_tsv_family)\n",
        "\n",
        "ax_duriation_distribution_accent_ca=figure_duriation_distribution_accent.add_subplot(3,4,2)\n",
        "plt.title(\"Kanadisches Deutsch\",fontdict  = font1)\n",
        "#ax.legend()\n",
        "ax = sns.barplot(x = 'gender', y = 'accent_Duration_mean_gender', data = dataset_test_ca_validated_tsv_family)\n",
        "plt.savefig('plotBarDiagrams_Accent_Duration_at_ca.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjCvyMiDvxdt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-OEys7tQMAa"
      },
      "outputs": [],
      "source": [
        "############################## Bar chart: x='accent', y = 'accent_Duration_mean' according to the gender - Accent ='all Accent'#######################################\n",
        "print(\"################################## Bar chart: x='accent', y = 'accent_Duration_mean' according to the gender - Accent ='all Accent' ###########################################\")\n",
        "##### def plotBarChartsDurrationGenderAllAccent():\n",
        "list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us']\n",
        "list_dataset_test_accent=['Österreichisches Deutsch','Kanadisches Deutsch','Schweizerdeutsch','Alemannische Färbung,Schweizer Standart Deutsch','Niederländisch Deutsch','Deutschland Deutsch',\n",
        "                          'Französisch Deutsch','Britisches Deutsch','Italienisch Deutsch'\n",
        ",'Russisch Deutsch','Amerikanisches Deutsch']\n",
        "\n",
        "\n",
        "# list_dataset_test_=['de_al']\n",
        "# list_dataset_test_accent=['Alemannische Färbung,Schweizer Standart Deutsch']\n",
        "\n",
        "\n",
        "\n",
        "# list_dataset_test_=['at']\n",
        "# list_dataset_test_accent=['Österreichisches Deutsch']\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "data_2 = {'Accent_short':list_dataset_test_,'Accent_long': list_dataset_test_accent}\n",
        "dataset_accent = pd.DataFrame(data_2)\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "############################# 1- reading the dataset ##########################################################\n",
        "\n",
        "\n",
        "\n",
        "for i in range(0,len(dataset_accent)):\n",
        "\n",
        "  dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t',low_memory=False)\n",
        "  accent_short_var=dataset_accent['Accent_short'][i]\n",
        "  accent_long_var=dataset_accent['Accent_long'][i]\n",
        "\n",
        "  ##\n",
        "\n",
        "  # dataset_accent=f'dataset_test_{i}'\n",
        "  # textfile_name=f'test_{i}.txt'\n",
        "  dataset_test_ca = pd.read_csv(f'test_{accent_short_var}.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "  \n",
        "\n",
        "  # dataset_test_ca=f'dataset_test_{i}'\n",
        "  # extract the numeric values of the durations from the third Column and store it in a new column called\n",
        "  dataset_test_ca['duration_numeric'] = dataset_test_ca['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "  dataset_test_ca['duration_numeric'] =dataset_test_ca['duration_numeric'].div(10)\n",
        "  # extract the numeric values of the durations from the third Column and store it in a new column called \n",
        "  #dataset_test_ca['audio_filepath_numeric'] = dataset_test_ca['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "  dataset_test_ca['audio_filepath_numeric'] = dataset_test_ca['audio_filepath'].str[-13:-5]\n",
        "  \n",
        "\n",
        "\n",
        "  # ############################# 2- extract the audio_filepath number and change them to integer################## \n",
        "\n",
        "  # get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "  dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "  dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "  dataset_test_ca['audio_filepath_numeric']=dataset_test_ca['audio_filepath_numeric'].astype('int')\n",
        "  \n",
        "  \n",
        "  ############################# 3- Drop all NaN gender and NaN Accent values, and show only the female gender and accent_long_var rows #############################\n",
        "  dataset_validated_tsv.drop(dataset_validated_tsv[(dataset_validated_tsv['gender'] != 'female') | (dataset_validated_tsv['gender'].isna())| \n",
        "  (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!=accent_long_var)].index, inplace=True)\n",
        "  #dataset_validated_tsv\n",
        " \n",
        "  ############################# 4- Merge using audio_filepath_numeric column ##################################\n",
        "  ####################5-calculate the Mean of duration when the gender is female###########################\n",
        "  dataset_test_ca_validated_tsv_female=pd.merge(dataset_test_ca,dataset_validated_tsv)\n",
        "  dataset_test_ca_validated_tsv_female['gender']=dataset_test_ca_validated_tsv_female['gender'].astype(\"string\")\n",
        "  dataset_test_ca_validated_tsv_female\n",
        "  dataset_test_ca_validated_tsv_female['accent_Duration_mean_gender']=np.mean(dataset_test_ca_validated_tsv_female['duration_numeric'])\n",
        "  \n",
        "\n",
        "  \n",
        "  ############################# 6- reading the dataset ##########################################################\n",
        "  ############################# 7- extract the audio_filepath number and change them to integer################## \n",
        "  # get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "  #dataset_test_ca = pd.read_csv(f'test_{accent_short_var}.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "  dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t',low_memory=False)\n",
        "  dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "  dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  ############################# 8- Drop all NaN gender and NaN Accent values, and show only the male gender and accent_long_var rows #############################\n",
        "  dataset_validated_tsv.drop(dataset_validated_tsv[(dataset_validated_tsv['gender'] != 'male') | (dataset_validated_tsv['gender'].isna())| (dataset_validated_tsv['accents'].isna())\n",
        "  | (dataset_validated_tsv['accents']!=accent_long_var)].index, inplace=True)\n",
        "\n",
        "  ############################# 9- Merge using audio_filepath_numeric column ##################################\n",
        "  dataset_test_ca_validated_tsv_male=pd.merge(dataset_test_ca,dataset_validated_tsv)\n",
        "  dataset_test_ca_validated_tsv_male['gender']=dataset_test_ca_validated_tsv_male['gender'].astype(\"string\")\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "  #################### 10- calculate the Mean of duration when the gender is male###########################\n",
        "  dataset_test_ca_validated_tsv_male['accent_Duration_mean_gender']=np.mean(dataset_test_ca_validated_tsv_male['duration_numeric'])\n",
        "  #dataset_test_ca_validated_tsv_male\n",
        "  ###### add/append the upper dataset dataset_test_ca_validated_tsv_female to the lower dataset dataset_test_ca_validated_tsv_male###########################\n",
        "  ###### and call the result dataset_test_ca_validated_tsv_family\n",
        "\n",
        "  ###### test if the dataframes are empty!\n",
        "  if not dataset_test_ca_validated_tsv_female.empty and not dataset_test_ca_validated_tsv_male.empty:\n",
        "    dataset_test_ca_validated_tsv_family=dataset_test_ca_validated_tsv_female.append(dataset_test_ca_validated_tsv_male)\n",
        "  elif dataset_test_ca_validated_tsv_female.empty and not dataset_test_ca_validated_tsv_male.empty:\n",
        "    dataset_test_ca_validated_tsv_family==dataset_test_ca_validated_tsv_male\n",
        "  elif dataset_test_ca_validated_tsv_male.empty and not dataset_test_ca_validated_tsv_female.empty:\n",
        "    dataset_test_ca_validated_tsv_family==dataset_test_ca_validated_tsv_female\n",
        "  else:\n",
        "    data_3={'gender':['female','male'],'accent_Duration_mean_gender':[0,0]}\n",
        "    dataset_test_ca_validated_tsv_family=pd.DataFrame(data_3)\n",
        "\n",
        "    ################### 11- plot the dataset_test_ca_validated_tsv_family in Bar Chart #######################\n",
        "\n",
        "  figure_duriation_distribution_accent=plt.figure(12,figsize=(20,20))  \n",
        "  #plt.subplots(figure's number per column,figure's number per row)\n",
        "\n",
        "  # Sup_plot_position=f'dataset_test_{i}'\n",
        "\n",
        "  # Sup_plot_position=Sup_plot_position.astype(int)\n",
        "  ax_duriation_distribution_accent_at=figure_duriation_distribution_accent.add_subplot(3,4,i+1)\n",
        "  font1 = {'family':'serif','color':'blue','size':20}\n",
        "  font2 = {'family':'serif','color':'darkred','size':15}\n",
        "  if accent_long_var=='Alemannische Färbung,Schweizer Standart Deutsch':\n",
        "    font1 = {'family':'serif','color':'blue','size':13}\n",
        "    accent_long_var='Alemannische Färbung,\\n Schweizer Standart Deutsch'\n",
        "  \n",
        "  plt.title(accent_long_var,fontdict  = font1)\n",
        "  #ax.legend()\n",
        "  #min([], default=\"EMPTY\")\n",
        "  ax = sns.barplot(x = 'gender', y = 'accent_Duration_mean_gender', data = dataset_test_ca_validated_tsv_family)\n",
        "\n",
        "plt.savefig('plotBarDiagramsAccent_gender_.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbTLFjmGyk2m"
      },
      "outputs": [],
      "source": [
        "list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us']\n",
        "list_dataset_test_accent=['Österreichisches Deutsch','Kanadisches Deutsch','Schweizerdeutsch','Alemannische Färbung,Schweizer Standart Deutsch','Niederländisch Deutsch',' Deutschland Deutsch','Französisch Deutsch\t','Britisches Deutsch',' Italienisch Deutsch'\n",
        ",'Russisch Deutsch','Amerikanisches Deutsch']\n",
        "\n",
        "   \n",
        "data_2 = {'Accent_short': ['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us'],\n",
        "                  'Accent_long': ['Österreichisches Deutsch','Kanadisches Deutsch','Schweizerdeutsch','Alemannische Färbung,Schweizer Standart Deutsch','Niederländisch Deutsch',' Deutschland Deutsch','Französisch Deutsch\t','Britisches Deutsch',' Italienisch Deutsch'\n",
        ",'Russisch Deutsch','Amerikanisches Deutsch']\n",
        "                  }\n",
        "dataset_accent = pd.DataFrame(data_2)\n",
        "  \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#from compare_df import * # import compare-df Function\n",
        "\n",
        "############################# 1- reading the dataset ##########################################################\n",
        "dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t')\n",
        "\n",
        "\n",
        "for i in range(0,len(dataset_accent)):\n",
        "\n",
        "  accent_short_var=dataset_accent['Accent_short'][i]\n",
        "  accent_long_var=dataset_accent['Accent_long'][i]\n",
        "  print(accent_short_var,accent_long_var)\n",
        "\n",
        "  #   if j =='Amerikanisches Deutsch':\n",
        "  #    break\n",
        "  # if j =='us':\n",
        "  #   break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVqaULkCxEtv"
      },
      "source": [
        "####**<font color=\"1497d4\">Bar chart: All Accent - Age</font>** **x='accent', y = 'accent_Duration_mean' according to age where Accent ='all Accent'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aBhaDvF9JN2"
      },
      "outputs": [],
      "source": [
        "\n",
        "type(dataset_accent_age)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-z5AyO682zF"
      },
      "outputs": [],
      "source": [
        "dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t',low_memory=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0ockYl1lSLx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# write a column from the validate.tsv to dataset_validated_tsv_accent.py\n",
        "import os\n",
        "# remove the script if exists \n",
        "# os.remove(\"dataset_accent.py\")\n",
        "# open script to write in the calculation of Mean of all accent\n",
        "dataset_accent_write_file = open(\"dataset_validated_tsv_accent_Alemannische Färbung,Schweizer Standart Deutsch.py\",'w')\n",
        "dataset_validated_tsv.drop(dataset_validated_tsv[ (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!='Alemannische Färbung,Schweizer Standart Deutsch')].index, inplace=True)\n",
        "\n",
        "for i in (dataset_validated_tsv['accents']):\n",
        "  dataset_accent_write_file.write(f\"\\n'{i}'\")\n",
        "dataset_accent_write_file.close() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9iAngB6Ov1U"
      },
      "source": [
        "####**<font color=\"1497d4\">Bar chart: All Accent - Age</font>** **x='accent', y = 'accent_Duration_mean' according to age where Accent ='all Accent'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjQf6kqjc_1t"
      },
      "outputs": [],
      "source": [
        "############################## Bar chart: x='accent', y = 'accent_Duration_mean' according to the age - Accent ='all Accent'#######################################\n",
        "print(\"################################## Bar chart: x='accent', y = 'accent_Duration_mean' according to the age - Accent ='all Accent' ###########################################\")\n",
        "# def plotBarChartsDurrationageAllAccent():\n",
        "list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us']\n",
        "list_dataset_test_accent=['Österreichisches Deutsch','Kanadisches Deutsch','Schweizerdeutsch','Alemannische Färbung,Schweizer Standart Deutsch','Niederländisch Deutsch','Deutschland Deutsch',\n",
        "'Französisch Deutsch','Britisches Deutsch','Italienisch Deutsch','Russisch Deutsch','Amerikanisches Deutsch']\n",
        "\n",
        "# list_dataset_test_=['at']\n",
        "# list_dataset_test_accent=['Österreichisches Deutsch']\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# four lists of iterations of twenties and thirties elements\n",
        "list_twenties=list(map(lambda x:'twenties',list_dataset_test_))\n",
        "list_thirties=list(map(lambda x:'thirties',list_dataset_test_))\n",
        "list_fourties=list(map(lambda x:'fourties',list_dataset_test_))\n",
        "list_fifties=list(map(lambda x:'fifties',list_dataset_test_))\n",
        "\n",
        "\n",
        "\n",
        "data_1 = {'Accent_short': list_dataset_test_,\n",
        "'Accent_long': list_dataset_test_accent,'twenties':list_twenties,'thirties':list_thirties,'fourties':list_fourties,'fifties':list_fifties  }\n",
        "dataset_accent_age = pd.DataFrame(data_1)\n",
        "############################# 1- reading the dataset ##########################################################\n",
        "\n",
        "\n",
        "\n",
        "for i in range(0,len(dataset_accent_age)):\n",
        "  \n",
        "\n",
        "  accent_short_var=dataset_accent_age['Accent_short'][i]\n",
        "  accent_long_var=dataset_accent_age['Accent_long'][i]\n",
        "  age_var_twenties=dataset_accent_age['twenties'][i]\n",
        "  age_var_thirties=dataset_accent_age['thirties'][i]\n",
        "  age_var_fourties=dataset_accent_age['fourties'][i]\n",
        "  age_var_fifties=dataset_accent_age['fifties'][i]\n",
        "\n",
        "  ##\n",
        "\n",
        "#########################################################################################################################################################################\n",
        "########################################################### ages 20 - 30 ################################################################################################\n",
        "#########################################################################################################################################################################\n",
        "  # dataset_accent_age=f'dataset_test_{i}'\n",
        "  # textfile_name=f'test_{i}.txt'\n",
        "  dataset_test_ca = pd.read_csv(f'test_{accent_short_var}.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "  dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t',low_memory=False)\n",
        "  \n",
        "  # dataset_test_ca=f'dataset_test_{i}'\n",
        "  # extract the numeric values of the durations from the third Column and store it in a new column called\n",
        "  dataset_test_ca['duration_numeric'] = dataset_test_ca['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "  dataset_test_ca['duration_numeric'] =dataset_test_ca['duration_numeric'].div(10)\n",
        "  # extract the numeric values of the durations from the third Column and store it in a new column called \n",
        "  #dataset_test_ca['audio_filepath_numeric'] = dataset_test_ca['audio_filepath'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "  dataset_test_ca['audio_filepath_numeric'] = dataset_test_ca['audio_filepath'].str[-13:-5]\n",
        "\n",
        "\n",
        "\n",
        "  # ############################# 2- extract the audio_filepath number and change them to integer################## \n",
        "\n",
        "  # get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "  dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "  dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "  dataset_test_ca['audio_filepath_numeric']=dataset_test_ca['audio_filepath_numeric'].astype('int')\n",
        "\n",
        "#########***********************************\n",
        "\n",
        "  ############################ 3-A Drop all NaN age and NaN Accent values, and show only the twenties-->>age_var_twenties age and accent_long_var rows #############################\n",
        "  dataset_validated_tsv.drop(dataset_validated_tsv[ (dataset_validated_tsv['age'].isna())| (dataset_validated_tsv['age'] != age_var_twenties)].index, inplace=True)\n",
        "\n",
        "\n",
        "  ############################ 3-B Drop all NaN age and NaN Accent values, and show only the twenties-->>age_var_twenties age and accent_long_var rows #############################\n",
        "  dataset_validated_tsv.drop(dataset_validated_tsv[ (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!=accent_long_var)].index, inplace=True)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "  ############################# 3- Drop all NaN age and NaN Accent values, and show only the twenties-->>age_var_twenties age and accent_long_var rows #############################\n",
        "  # dataset_validated_tsv.drop(dataset_validated_tsv[ (dataset_validated_tsv['age'].isna())| (dataset_validated_tsv['age'] != age_var_twenties) |\n",
        "  # (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!=accent_long_var)].index, inplace=True)\n",
        "  #dataset_validated_tsv\n",
        "\n",
        "\n",
        "\n",
        "  ############################# 4- Merge using audio_filepath_numeric column ##################################\n",
        "  ####################5-calculate the Mean of duration when the age is twenties###########################\n",
        "  dataset_test_ca_validated_tsv_twenties=pd.merge(dataset_test_ca,dataset_validated_tsv)\n",
        "  dataset_test_ca_validated_tsv_twenties['age']=dataset_test_ca_validated_tsv_twenties['age'].astype(\"string\")\n",
        "  dataset_test_ca_validated_tsv_twenties['accent_Duration_mean_age']=np.mean(dataset_test_ca_validated_tsv_twenties['duration_numeric'])\n",
        "\n",
        "\n",
        "  # ############################ 6- reading the dataset to make the calculations of the age thirties #################\n",
        "  # ############################ 7- extract the audio_filepath number and change them to integer  #################### \n",
        "  # get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "   \n",
        "  dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t',low_memory=False)\n",
        "  dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "  dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "\n",
        "# print('##'*20,accent_long_var,'##'*60)\n",
        "# print(dataset_validated_tsv)\n",
        "\n",
        "  # ############################# 8-A Drop all NaN age and NaN Accent values, and show only the thirties -->>age_var_thirties age and accent_long_var rows #############################\n",
        "  dataset_validated_tsv.drop(dataset_validated_tsv[ (dataset_validated_tsv['age'].isna())| (dataset_validated_tsv['age'] != age_var_thirties)].index, inplace=True)\n",
        "\n",
        "############################ 8-B Drop all NaN age and NaN Accent values, and show only the twenties-->>age_var_thirties age and accent_long_var rows #############################\n",
        "  dataset_validated_tsv.drop(dataset_validated_tsv[ (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!=accent_long_var)].index, inplace=True)\n",
        "\n",
        "# print('##'*20,accent_long_var,'##'*60)\n",
        "# dataset_validated_tsv\n",
        "\n",
        " \n",
        "  ############################# 9- Merge using audio_filepath_numeric column ##################################\n",
        "  dataset_test_ca_validated_tsv_thirties=pd.merge(dataset_test_ca,dataset_validated_tsv)\n",
        "  dataset_test_ca_validated_tsv_thirties['age']=dataset_test_ca_validated_tsv_thirties['age'].astype(\"string\")\n",
        "\n",
        "\n",
        "  #################### 10- calculate the Mean of duration when the age is thirties###########################\n",
        "  dataset_test_ca_validated_tsv_thirties['accent_Duration_mean_age']=np.mean(dataset_test_ca_validated_tsv_thirties['duration_numeric'])\n",
        "  #dataset_test_ca_validated_tsv_thirties\n",
        "  ###### add/append the upper dataset dataset_test_ca_validated_tsv_twenties to the lower dataset dataset_test_ca_validated_tsv_thirties###########################\n",
        "  ###### and call the result dataset_test_ca_validated_tsv_20_30\n",
        "  ####>>>>>>>dataset_test_ca_validated_tsv_20_30=dataset_test_ca_validated_tsv_twenties.append(dataset_test_ca_validated_tsv_thirties)\n",
        "\n",
        "#########################################################################################################################################################################\n",
        "########################################################### test if the dataframes are empty!##############################################################################\n",
        "#########################################################################################################################################################################\n",
        "\n",
        "  if not dataset_test_ca_validated_tsv_twenties.empty and not dataset_test_ca_validated_tsv_thirties.empty:\n",
        "    dataset_test_ca_validated_tsv_20_30=dataset_test_ca_validated_tsv_twenties.append(dataset_test_ca_validated_tsv_thirties)\n",
        "  elif dataset_test_ca_validated_tsv_twenties.empty and not dataset_test_ca_validated_tsv_thirties.empty:\n",
        "    dataset_test_ca_validated_tsv_20_30=dataset_test_ca_validated_tsv_thirties\n",
        "  elif dataset_test_ca_validated_tsv_thirties.empty and not dataset_test_ca_validated_tsv_twenties.empty:\n",
        "    dataset_test_ca_validated_tsv_20_30=dataset_test_ca_validated_tsv_twenties\n",
        "  else:\n",
        "    data_3={'age':['twenties','thirties'],'accent_Duration_mean_age':[0,0]}\n",
        "    dataset_test_ca_validated_tsv_20_30=pd.DataFrame(data_3)\n",
        "\n",
        "\n",
        "#########################################################################################################################################################################\n",
        "########################################################### ages 40 - 50 ################################################################################################\n",
        "#########################################################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "  dataset_test_ca = pd.read_csv(f'test_{accent_short_var}.txt', header=None, names=['audio_filepath','text','duration'])\n",
        "  dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t',low_memory=False)\n",
        "  \n",
        "\n",
        "  # extract the numeric values of the durations from the third Column and store it in a new column called\n",
        "  dataset_test_ca['duration_numeric'] = dataset_test_ca['duration'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "  dataset_test_ca['duration_numeric'] =dataset_test_ca['duration_numeric'].div(10)\n",
        "  # extract the numeric values of the durations from the third Column and store it in a new column called audio_filepath_numeric\n",
        "  dataset_test_ca['audio_filepath_numeric'] = dataset_test_ca['audio_filepath'].str[-13:-5]\n",
        "\n",
        "\n",
        "\n",
        "  # ############################# 2- extract the audio_filepath number and change them to integer################## \n",
        "\n",
        "  # get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "  dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "  dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "  dataset_test_ca['audio_filepath_numeric']=dataset_test_ca['audio_filepath_numeric'].astype('int')\n",
        "\n",
        "\n",
        "  ############################ 3-A Drop all NaN age and NaN Accent values, and show only the fourties-->>age_var_fourties age and accent_long_var rows #############################\n",
        "  dataset_validated_tsv.drop(dataset_validated_tsv[ (dataset_validated_tsv['age'].isna())| (dataset_validated_tsv['age'] != age_var_fourties)].index, inplace=True)\n",
        "\n",
        "\n",
        "  ############################ 3-B Drop all NaN age and NaN Accent values, and show only the fourties-->>age_var_fourties age and accent_long_var rows #############################\n",
        "  dataset_validated_tsv.drop(dataset_validated_tsv[ (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!=accent_long_var)].index, inplace=True)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "  ############################# 3- Drop all NaN age and NaN Accent values, and show only the fourties-->>age_var_fourties age and accent_long_var rows #############################\n",
        "  # dataset_validated_tsv.drop(dataset_validated_tsv[ (dataset_validated_tsv['age'].isna())| (dataset_validated_tsv['age'] != age_var_fourties) |\n",
        "  # (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!=accent_long_var)].index, inplace=True)\n",
        "  #dataset_validated_tsv\n",
        "\n",
        "\n",
        "\n",
        "  ############################# 4- Merge using audio_filepath_numeric column ##################################\n",
        "  ####################5-calculate the Mean of duration when the age is fourties###########################\n",
        "  dataset_test_ca_validated_tsv_fourties=pd.merge(dataset_test_ca,dataset_validated_tsv)\n",
        "  dataset_test_ca_validated_tsv_fourties['age']=dataset_test_ca_validated_tsv_fourties['age'].astype(\"string\")\n",
        "  dataset_test_ca_validated_tsv_fourties['accent_Duration_mean_age']=np.mean(dataset_test_ca_validated_tsv_fourties['duration_numeric'])\n",
        "\n",
        "\n",
        "  # ############################ 6- reading the dataset to make the calculations of the age fifties #################\n",
        "  # ############################ 7- extract the audio_filepath number and change them to integer  #################### \n",
        "  # get the audio file number and display it at dataset_validated_tsv['audio_filepath_numeric']\n",
        "  dataset_validated_tsv = pd.read_csv('validated.tsv', sep='\\t',low_memory=False)\n",
        "  dataset_validated_tsv['audio_filepath_numeric'] = dataset_validated_tsv['path'].str[-12:-4]\n",
        "  dataset_validated_tsv['audio_filepath_numeric']=dataset_validated_tsv['audio_filepath_numeric'].astype('int')\n",
        "\n",
        "# print('##'*20,accent_long_var,'##'*60)\n",
        "# print(dataset_validated_tsv)\n",
        "\n",
        "  # ############################# 8-A Drop all NaN age and NaN Accent values, and show only the fifties -->>age_var_fifties age and accent_long_var rows #############################\n",
        "  dataset_validated_tsv.drop(dataset_validated_tsv[ (dataset_validated_tsv['age'].isna())| (dataset_validated_tsv['age'] != age_var_fifties)].index, inplace=True)\n",
        "\n",
        "############################ 8-B Drop all NaN age and NaN Accent values, and show only the fourties-->>age_var_fifties age and accent_long_var rows #############################\n",
        "  dataset_validated_tsv.drop(dataset_validated_tsv[ (dataset_validated_tsv['accents'].isna())| (dataset_validated_tsv['accents']!=accent_long_var)].index, inplace=True)\n",
        "\n",
        "# print('##'*20,accent_long_var,'##'*60)\n",
        "# dataset_validated_tsv\n",
        "\n",
        " \n",
        "  ############################# 9- Merge using audio_filepath_numeric column ##################################\n",
        "  dataset_test_ca_validated_tsv_fifties=pd.merge(dataset_test_ca,dataset_validated_tsv)\n",
        "  dataset_test_ca_validated_tsv_fifties['age']=dataset_test_ca_validated_tsv_fifties['age'].astype(\"string\")\n",
        "\n",
        "\n",
        "  #################### 10- calculate the Mean of duration when the age is fifties###########################\n",
        "  dataset_test_ca_validated_tsv_fifties['accent_Duration_mean_age']=np.mean(dataset_test_ca_validated_tsv_fifties['duration_numeric'])\n",
        "  #dataset_test_ca_validated_tsv_fifties\n",
        "  ###### add/append the upper dataset dataset_test_ca_validated_tsv_fourties to the lower dataset dataset_test_ca_validated_tsv_fifties###########################\n",
        "  ###### and call the result dataset_test_ca_validated_tsv_40_50\n",
        "  ####>>>>>>>dataset_test_ca_validated_tsv_40_50=dataset_test_ca_validated_tsv_fourties.append(dataset_test_ca_validated_tsv_fifties)\n",
        "\n",
        "#########################################################################################################################################################################\n",
        "########################################################### test if the dataframes are empty!##############################################################################\n",
        "#########################################################################################################################################################################\n",
        "\n",
        "  if not dataset_test_ca_validated_tsv_fourties.empty and not dataset_test_ca_validated_tsv_fifties.empty:\n",
        "    dataset_test_ca_validated_tsv_40_50=dataset_test_ca_validated_tsv_fourties.append(dataset_test_ca_validated_tsv_fifties)\n",
        "  elif dataset_test_ca_validated_tsv_fourties.empty and not dataset_test_ca_validated_tsv_fifties.empty:\n",
        "    dataset_test_ca_validated_tsv_40_50=dataset_test_ca_validated_tsv_fifties\n",
        "  elif dataset_test_ca_validated_tsv_fifties.empty and not dataset_test_ca_validated_tsv_fourties.empty:\n",
        "    dataset_test_ca_validated_tsv_40_50=dataset_test_ca_validated_tsv_fourties\n",
        "  else:\n",
        "    data_3={'age':['fourties','fifties'],'accent_Duration_mean_age':[0,0]}\n",
        "    dataset_test_ca_validated_tsv_40_50=pd.DataFrame(data_3)\n",
        "\n",
        "#########################################################################################################################################################################\n",
        "########################################################### merge all ages: 20-30 with 40 - 50 ########################################################################## \n",
        "#########################################################################################################################################################################\n",
        "  ###>>>>>>>>dataset_test_ca_validated_tsv_all_ages=dataset_test_ca_validated_tsv_20_30.append(dataset_test_ca_validated_tsv_40_50)\n",
        "\n",
        "\n",
        "#########################################################################################################################################################################\n",
        "########################################################### test if the dataframes are empty!##############################################################################\n",
        "#########################################################################################################################################################################\n",
        "\n",
        "  if not dataset_test_ca_validated_tsv_20_30.empty and not dataset_test_ca_validated_tsv_40_50.empty:\n",
        "    dataset_test_ca_validated_tsv_all_ages=dataset_test_ca_validated_tsv_20_30.append(dataset_test_ca_validated_tsv_40_50)\n",
        "  elif dataset_test_ca_validated_tsv_20_30.empty and not dataset_test_ca_validated_tsv_40_50.empty:\n",
        "    dataset_test_ca_validated_tsv_all_ages=dataset_test_ca_validated_tsv_40_50\n",
        "  elif dataset_test_ca_validated_tsv_40_50.empty and not dataset_test_ca_validated_tsv_20_30.empty:\n",
        "    dataset_test_ca_validated_tsv_all_ages=dataset_test_ca_validated_tsv_20_30\n",
        "  else:\n",
        "    data_3={'age':['fourties','fifties'],'accent_Duration_mean_age':[0,0]}\n",
        "    dataset_test_ca_validated_tsv_all_ages=pd.DataFrame(data_3)\n",
        "\n",
        "\n",
        "\n",
        "  ################### 11- plot the dataset_test_ca_validated_tsv_family in Bar Chart #######################\n",
        "\n",
        "  figure_duriation_distribution_accent=plt.figure(12,figsize=(25,25))  \n",
        "  #plt.subplots(figure's number per column,figure's number per row)\n",
        "\n",
        "  # Sup_plot_position=f'dataset_test_{i}'\n",
        "\n",
        "  # Sup_plot_position=Sup_plot_position.astype(int)\n",
        "  ax_duriation_distribution_accent_at=figure_duriation_distribution_accent.add_subplot(3,4,i+1)\n",
        "  font1 = {'family':'serif','color':'blue','size':20}\n",
        "  font2 = {'family':'serif','color':'darkred','size':15}\n",
        "  if accent_long_var=='Alemannische Färbung,Schweizer Standart Deutsch':\n",
        "    font1 = {'family':'serif','color':'blue','size':13}\n",
        "    accent_long_var='Alemannische Färbung,\\n Schweizer Standart Deutsch'\n",
        "  plt.title(accent_long_var,fontdict  = font1)\n",
        "  #ax.legend()\n",
        "  ax = sns.barplot(x = 'age', y = 'accent_Duration_mean_age', data = dataset_test_ca_validated_tsv_all_ages)\n",
        "plt.savefig('plotBarDiagramsAccent_age.png')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezlIy10vIY6c"
      },
      "outputs": [],
      "source": [
        "# write a column from the validate.tsv to dataset_validated_tsv_accent.py\n",
        "import os\n",
        "# remove the script if exists \n",
        "# os.remove(\"dataset_accent.py\")\n",
        "# open script to write in the calculation of Mean of all accent\n",
        "dataset_accent_write_file = open(\"dataset_validated_tsv_age.py\",'w')\n",
        "\n",
        "for i in (dataset_validated_tsv['age']):\n",
        "  dataset_accent_write_file.write(f\"\\n'{i}'\")\n",
        "dataset_accent_write_file.close() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Niw6tjcJHk_s"
      },
      "outputs": [],
      "source": [
        "dataset_results_json = pd.read_json('results.json')\n",
        "dataset_results_json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh7Hsl5OWbI_"
      },
      "source": [
        "###**<font color=\"1497d4\">Bar charts Word Error Rate's Accent  </font>**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VTOsfWeeVYm"
      },
      "source": [
        "####**<font color=\"1497d4\">Bar chart: </font>** **x='accent', y = 'accent_Duration_mean' according to Word Error Rate Accent ='all Accent'**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwsCA8s7VzIJ"
      },
      "source": [
        "####**<font color=\"1497d4\">Bar chart: </font>** **x='accent', y = 'accent_Duration_mean'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjYJfTZY17lA"
      },
      "outputs": [],
      "source": [
        "####################################################################################################\n",
        "####################Bar chart: x='accent', y = 'accent_Duration_mean' according to Word Error Rate Accent ='all Accent'##########################\n",
        "####################################################################################################\n",
        "\n",
        "accent_long_var='Alemannische Färbung,Schweizer Standart Deutsch'\n",
        "\n",
        " \n",
        "###########################################################################\n",
        "# dataset_results_json_accent=pd.DataFrame({'accent_short':['at','gb'],\n",
        "#                                           'accent_long':['Österreichisches Deutsch','Britisches Deutsch'],\n",
        "#                                         'test_accent_txt':['test_at.txt','test_gb.txt']})\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "dataset_results_json_accent=pd.DataFrame({'accent_short':['at','gb','it','de_al','fr','de_ni','ch','de','us','ca','ru'],\n",
        "                                          'accent_long':['Österreichisches Deutsch','Britisches Deutsch','Italienisch Deutsch','Alemannische Färbung,Schweizer Standart Deutsch',\n",
        "'Französisch Deutsch','Niederländisch Deutsch','Schweizerdeutsch','Deutschland Deutsch','Amerikanisches Deutsch','Kanadisches Deutsch','Russisch Deutsch'],\n",
        "                                        'test_accent_txt':['test_at.txt','test_gb.txt','test_it.txt','test_de_al.txt','test_fr.txt','test_de_ni.txt','test_ch.txt',\n",
        "               'test_de.txt','test_us.txt','test_ca.txt','test_ru.txt']})\n",
        "\n",
        "##############################################################################################################\n",
        "\n",
        "list_accent_all_wer_mean_result=[]\n",
        "dataset_accent_all_wer_mean_result=pd.DataFrame(list_accent_all_wer_mean_result,columns=['accent_short','accent_long','wer_accent_mean'])\n",
        "##############################################################################################################\n",
        "\n",
        "\n",
        "for i in range(len(dataset_results_json_accent)):\n",
        "  dataset_results_json = pd.read_json('results.json')\n",
        "\n",
        "  ###################################################################################################\n",
        "  ###################Convert Json Index to column contents in dataframe #############################\n",
        "  ###################################################################################################\n",
        "\n",
        "  dataset_results_json['audio_filepath'] = dataset_results_json.index\n",
        "\n",
        "  dataset_results_json['audio_filepath_numeric'] = dataset_results_json['audio_filepath'].str[-12:-4]\n",
        "  \n",
        "  ###>>> dataset_results_json_audio_filepath is the dataset_results_json DataFrame with the audio_filepath as index\n",
        "  dataset_results_json_audio_filepath=dataset_results_json.reindex(dataset_results_json['audio_filepath_numeric'])\n",
        "\n",
        "  # filling up the ['audio_filepath_numeric'] Values of dataset_results_json_audio_filepath form dataset_results_json\n",
        "  dataset_results_json_audio_filepath['audio_filepath_numeric']=np.where(dataset_results_json['audio_filepath_numeric']!=np.NaN,dataset_results_json['audio_filepath_numeric'],'0000')\n",
        "\n",
        "\n",
        "  # filling up the Values of dataset_results_json_audio_filepath form dataset_results_json\n",
        "  dataset_results_json_audio_filepath[dataset_results_json_accent.test_accent_txt[i]]=np.where(dataset_results_json[dataset_results_json_accent.test_accent_txt[i]]!=np.NaN,\n",
        "                                                                                               dataset_results_json[dataset_results_json_accent.test_accent_txt[i]],'000')\n",
        "  \n",
        "  # dataset_results_json_audio_filepath['accent_long']=np.where(dataset_results_json['accent_long']!=np.NaN,dataset_results_json['accent_long'],'000')\n",
        "  # dataset_results_json_audio_filepath['test_accent_txt']=np.where(dataset_results_json['test_accent_txt']!=np.NaN,dataset_results_json['test_accent_txt'],'000')\n",
        " \n",
        "\n",
        " \n",
        "  #################################Extract and calculate the wer_accent_numeric########################################\n",
        "  # drop the NaN values of the accent \"test_accent_txt\"  by each one \n",
        "  dataset_results_json_audio_filepath.drop(dataset_results_json_audio_filepath[ (dataset_results_json_audio_filepath[dataset_results_json_accent.test_accent_txt[i]].isna())].index, inplace=True)\n",
        "\n",
        "  \n",
        "  # extract the numeric values of the WER durations from the accent Column and store it in a new column called wer_accent_numeric\n",
        " \n",
        "  dataset_results_json_audio_filepath['wer_accent_numeric'] = dataset_results_json_audio_filepath[dataset_results_json_accent.test_accent_txt[i]].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "\n",
        "  #### calculate the mean of the WER for each accent  \n",
        "  # dataset_results_json_audio_filepath['wer_accent_mean'] = np.where(dataset_results_json_audio_filepath['wer_accent_numeric']!=np.NaN,np.mean(dataset_results_json_audio_filepath['wer_accent_numeric']), '000')\n",
        "  # dataset_results_json_audio_filepath['wer_accent_mean']=dataset_results_json_audio_filepath['wer_accent_mean'].astype(float)\n",
        "\n",
        "  # dataset_results_json_audio_filepath['wer_accent_mean']=dataset_results_json_audio_filepath['wer_accent_mean'].div(10e+14/1.93)\n",
        "\n",
        "  # dataset_results_json_audio_filepath['wer_accent_mean'] = np.where(dataset_results_json_audio_filepath['wer_accent_numeric']!=np.NaN,np.round(dataset_results_json_audio_filepath['wer_accent_mean'],9), '000')\n",
        "\n",
        "\n",
        "  # to store the mean of each WER Accents in var from the DataFrame dataset_results_json_audio_filepath\n",
        "  # wer_accent_mean_var=dataset_results_json_audio_filepath['wer_accent_mean']\n",
        "  wer_accent_mean_var=dataset_results_json_audio_filepath.mean(axis=0, skipna=None, level=None, numeric_only=True)\n",
        "  \n",
        "\n",
        "  wer_accent_mean_var=wer_accent_mean_var/(10e+14/1.93)\n",
        "  wer_accent_mean_var =  wer_accent_mean_var.to_string()\n",
        "  wer_accent_mean_var=wer_accent_mean_var[22:25]\n",
        " \n",
        "  # print('*'*60,type(wer_accent_mean_var))\n",
        "  # print('*'*60,wer_accent_mean_var)\n",
        "  ## prepare the last DataFrame which contains of 3 coulmns accent and wer_accent_mean\n",
        "\n",
        "\n",
        "  dataset_accent_all_wer_mean_result = dataset_accent_all_wer_mean_result.append({'accent_short': f'{dataset_results_json_accent.accent_short[i]}','accent_long': f'{dataset_results_json_accent.accent_long[i]}',\n",
        "                                                                                  'wer_accent_mean': f'{wer_accent_mean_var}'}, ignore_index=True)\n",
        "  dataset_accent_all_wer_mean_result['wer_accent_mean']=dataset_accent_all_wer_mean_result['wer_accent_mean'].astype(float)\n",
        "\n",
        "  # # dataset_accent_all_wer_mean_result = dataset_accent_all_wer_mean_result.append({'accent_short': f'{dataset_results_json_accent.accent_short[i]}','accent_long': f'{dataset_results_json_accent.accent_long[i]}',\n",
        "  # #                                                                                   'wer_accent_mean': f'{wer_accent_mean_var[22:32]}'}, ignore_index=True)\n",
        "  # dataset_accent_all_wer_mean_result['accent_short'] = np.where(dataset_results_json_accent['accent_short']!=np.NaN,dataset_results_json_accent['accent_short'], '000')\n",
        "  # dataset_accent_all_wer_mean_result['accent_long'] = np.where(dataset_results_json_accent['accent_long']!=np.NaN,dataset_results_json_accent['accent_long'], '000')\n",
        "  # dataset_accent_all_wer_mean_result['wer_accent_mean'] = np.where(dataset_accent_all_wer_mean_result['accent_short']!=np.NaN,(f'{wer_accent_mean_var[35:38]}'), '000')\n",
        "\n",
        "#   # # dataset_accent_all_wer_mean_result = dataset_accent_all_wer_mean_result.append('wer_accent_mean': f'{wer_accent_mean_var}'}, ignore_index=True)\n",
        "\n",
        "# dataset_accent_all_wer_mean_result\n",
        "  \n",
        "  # dataset_accent_all_wer_mean_result = dataset_accent_all_wer_mean_result.append({'accent_short': f'{dataset_results_json_accent.accent_short[i]}','accent_long': f'{dataset_results_json_accent.accent_long[i]}',\n",
        "  #                                                                                 'wer_accent_mean': f'{wer_accent_mean_var}'}, ignore_index=True)\n",
        "  \n",
        "# dataset_accent_all_wer_mean_result['wer_accent_mean']=dataset_accent_all_wer_mean_result['wer_accent_mean'].astype(float)\n",
        "\n",
        "\n",
        "      # df = df.append({'a': 1, 'b': 2}, ignore_index=True)\n",
        "# dataset_accent_all_wer_mean_result = dataset_accent_all_wer_mean_result.append({'accent': f'{dataset_results_json_accent.accent_short[i]}', 'wer_accent_mean': f'{wer_accent_mean_var[22:32]}'}, ignore_index=True)\n",
        " \n",
        "\n",
        "\n",
        "# dataset_accent_all_wer_mean_result['accent_long'] = np.where(dataset_accent_all_wer_mean_result['accent']==dataset_results_json_accent['accent_short'],dataset_results_json_accent['accent_long'], '000')\n",
        "\n",
        "# print(dataset_accent_all_wer_mean_result)\n",
        " \n",
        "#   # dataset_accent_all_wer_mean_result['wer_accent_mean'] = np.where(dataset_test_['accent']!=np.NaN,wer_accent_mean_var[22:32], '000')\n",
        "\n",
        "  # print('*'*60,AA)\n",
        "  # print(dataset_accent_all_wer_mean_result)\n",
        "\n",
        "\n",
        "################################## Accent AT ###########################################\n",
        "# position of the figure and figure's size \n",
        "figure_duriation_distribution_accent=plt.figure(12,figsize=(10,10))  \n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "# ax_duriation_distribution_accent_at=figure_duriation_distribution_accent.add_subplot(3,4,i+1)\n",
        "\n",
        "\n",
        "# plt.xlabel(\"Speaker's accent - Austria\")\n",
        "# plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "# plt.plot(dataset_results_json['accent_Duration_mean'],dataset_results_json['audio_filepath_numeric'], lw=3, ms=20)\n",
        "font1 = {'family':'serif','color':'blue','size':20}\n",
        "font2 = {'family':'serif','color':'darkred','size':15}\n",
        "if accent_long_var=='Alemannische Färbung,Schweizer Standart Deutsch':\n",
        "  font1 = {'family':'serif','color':'blue','size':13}\n",
        "  accent_long_var='Alemannische Färbung,\\n Schweizer Standart Deutsch'\n",
        "  \n",
        "\n",
        "plt.title('Word Error Rate mean for all Accents',fontdict  = font1)\n",
        "\n",
        "\n",
        "ax = sns.barplot(x = 'accent_short', y = 'wer_accent_mean', data = dataset_accent_all_wer_mean_result)\n",
        "plt.savefig('plotBarDiagrams_Accent_WER.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPdgXydFpR_t"
      },
      "outputs": [],
      "source": [
        "####################################################################################################\n",
        "####################Bar chart: x='accent', y = 'accent_Duration_mean' according to Word Error Rate Accent ='all Accent'##########################\n",
        "####################################################################################################\n",
        "\n",
        "accent_long_var='Alemannische Färbung,Schweizer Standart Deutsch'\n",
        "\n",
        " \n",
        "###########################################################################\n",
        "# dataset_results_json_accent=pd.DataFrame({'accent_short':['at','gb'],\n",
        "#                                           'accent_long':['Österreichisches Deutsch','Britisches Deutsch'],\n",
        "#                                         'test_accent_txt':['test_at.txt','test_gb.txt']})\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "dataset_results_json_accent=pd.DataFrame({'accent_short':['at','gb','it','de_al','fr','de_ni','ch','de','us','ca','ru'],\n",
        "                                          'accent_long':['Österreichisches Deutsch','Britisches Deutsch','Italienisch Deutsch','Alemannische Färbung,Schweizer Standart Deutsch',\n",
        "'Französisch Deutsch','Niederländisch Deutsch','Schweizerdeutsch','Deutschland Deutsch','Amerikanisches Deutsch','Kanadisches Deutsch','Russisch Deutsch'],\n",
        "                                        'test_accent_txt':['test_at.txt','test_gb.txt','test_it.txt','test_de_al.txt','test_fr.txt','test_de_ni.txt','test_ch.txt',\n",
        "               'test_de.txt','test_us.txt','test_ca.txt','test_ru.txt']})\n",
        "\n",
        "##############################################################################################################\n",
        "\n",
        "list_accent_all_wer_mean_result=[]\n",
        "dataset_accent_all_wer_mean_result=pd.DataFrame(list_accent_all_wer_mean_result,columns=['accent_short','accent_long','wer_accent_mean'])\n",
        "##############################################################################################################\n",
        "\n",
        "\n",
        "for i in range(len(dataset_results_json_accent)):\n",
        "  dataset_results_json = pd.read_json('results.json')\n",
        "\n",
        "  ###################################################################################################\n",
        "  ###################Convert Json Index to column contents in dataframe #############################\n",
        "  ###################################################################################################\n",
        "\n",
        "  dataset_results_json['audio_filepath'] = dataset_results_json.index\n",
        "\n",
        "  dataset_results_json['audio_filepath_numeric'] = dataset_results_json['audio_filepath'].str[-12:-4]\n",
        "  \n",
        "  ###>>> dataset_results_json_audio_filepath is the dataset_results_json DataFrame with the audio_filepath as index\n",
        "  dataset_results_json_audio_filepath=dataset_results_json.reindex(dataset_results_json['audio_filepath_numeric'])\n",
        "\n",
        "  # filling up the ['audio_filepath_numeric'] Values of dataset_results_json_audio_filepath form dataset_results_json\n",
        "  dataset_results_json_audio_filepath['audio_filepath_numeric']=np.where(dataset_results_json['audio_filepath_numeric']!=np.NaN,dataset_results_json['audio_filepath_numeric'],'0000')\n",
        "\n",
        "\n",
        "  # filling up the Values of dataset_results_json_audio_filepath form dataset_results_json\n",
        "  dataset_results_json_audio_filepath[dataset_results_json_accent.test_accent_txt[i]]=np.where(dataset_results_json[dataset_results_json_accent.test_accent_txt[i]]!=np.NaN,\n",
        "                                                                                               dataset_results_json[dataset_results_json_accent.test_accent_txt[i]],'000')\n",
        "  \n",
        "  # dataset_results_json_audio_filepath['accent_long']=np.where(dataset_results_json['accent_long']!=np.NaN,dataset_results_json['accent_long'],'000')\n",
        "  # dataset_results_json_audio_filepath['test_accent_txt']=np.where(dataset_results_json['test_accent_txt']!=np.NaN,dataset_results_json['test_accent_txt'],'000')\n",
        " \n",
        "\n",
        " \n",
        "  #################################Extract and calculate the wer_accent_numeric########################################\n",
        "  # drop the NaN values of the accent \"test_accent_txt\"  by each one \n",
        "  dataset_results_json_audio_filepath.drop(dataset_results_json_audio_filepath[ (dataset_results_json_audio_filepath[dataset_results_json_accent.test_accent_txt[i]].isna())].index, inplace=True)\n",
        "\n",
        "  \n",
        "  # extract the numeric values of the WER durations from the accent Column and store it in a new column called wer_accent_numeric\n",
        " \n",
        "  dataset_results_json_audio_filepath['wer_accent_numeric'] = dataset_results_json_audio_filepath[dataset_results_json_accent.test_accent_txt[i]].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
        "\n",
        "\n",
        "  #### calculate the mean of the WER for each accent  \n",
        "  # dataset_results_json_audio_filepath['wer_accent_mean'] = np.where(dataset_results_json_audio_filepath['wer_accent_numeric']!=np.NaN,np.mean(dataset_results_json_audio_filepath['wer_accent_numeric']), '000')\n",
        "  # dataset_results_json_audio_filepath['wer_accent_mean']=dataset_results_json_audio_filepath['wer_accent_mean'].astype(float)\n",
        "\n",
        "  # dataset_results_json_audio_filepath['wer_accent_mean']=dataset_results_json_audio_filepath['wer_accent_mean'].div(10e+14/1.93)\n",
        "\n",
        "  # dataset_results_json_audio_filepath['wer_accent_mean'] = np.where(dataset_results_json_audio_filepath['wer_accent_numeric']!=np.NaN,np.round(dataset_results_json_audio_filepath['wer_accent_mean'],9), '000')\n",
        "\n",
        "\n",
        "  # to store the mean of each WER Accents in var from the DataFrame dataset_results_json_audio_filepath\n",
        "  # wer_accent_mean_var=dataset_results_json_audio_filepath['wer_accent_mean']\n",
        "  wer_accent_mean_var=dataset_results_json_audio_filepath.mean(axis=0, skipna=None, level=None, numeric_only=True)\n",
        "  \n",
        "\n",
        "  wer_accent_mean_var=wer_accent_mean_var/(10e+14/1.93)\n",
        "  wer_accent_mean_var =  wer_accent_mean_var.to_string()\n",
        "  wer_accent_mean_var=wer_accent_mean_var[22:25]\n",
        " \n",
        "  # print('*'*60,type(wer_accent_mean_var))\n",
        "  # print('*'*60,wer_accent_mean_var)\n",
        "  ## prepare the last DataFrame which contains of 3 coulmns accent and wer_accent_mean\n",
        "\n",
        "\n",
        "  dataset_accent_all_wer_mean_result = dataset_accent_all_wer_mean_result.append({'accent_short': f'{dataset_results_json_accent.accent_short[i]}','accent_long': f'{dataset_results_json_accent.accent_long[i]}',\n",
        "                                                                                  'wer_accent_mean': f'{wer_accent_mean_var}'}, ignore_index=True)\n",
        "  dataset_accent_all_wer_mean_result['wer_accent_mean']=dataset_accent_all_wer_mean_result['wer_accent_mean'].astype(float)\n",
        "\n",
        "\n",
        "\n",
        "################################## Accent AT ###########################################\n",
        "# position of the figure and figure's size \n",
        "figure_duriation_distribution_accent=plt.figure(12,figsize=(12,12))  \n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "# ax_duriation_distribution_accent_at=figure_duriation_distribution_accent.add_subplot(3,4,i+1)\n",
        "\n",
        "\n",
        "# plt.xlabel(\"Speaker's accent - Austria\")\n",
        "# plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "# plt.plot(dataset_results_json['accent_Duration_mean'],dataset_results_json['audio_filepath_numeric'], lw=3, ms=20)\n",
        "font1 = {'family':'serif','color':'blue','size':20}\n",
        "font2 = {'family':'serif','color':'darkred','size':15}\n",
        "# if accent_long_var=='Alemannische Färbung,Schweizer Standart Deutsch':\n",
        "  #font1 = {'family':'serif','color':'blue','size':13}\n",
        "  # font2 = {'family':'serif','color':'darkred','size':10}\n",
        "#   accent_long_var='Alemannische Färbung,\\n Schweizer Standart Deutsch'\n",
        "\n",
        "# to short hte long accent which called Alemannische Färbung,Schweizer Standart Deutsch\n",
        "dataset_accent_all_wer_mean_result['accent_long']=np.where(dataset_accent_all_wer_mean_result['accent_long']=='Alemannische Färbung,Schweizer Standart Deutsch', 'Alemannische**Standart Deutsch',dataset_accent_all_wer_mean_result['accent_long'])\n",
        "plt.title('Word Error Rate mean for all Accents',fontdict  = font1)\n",
        "\n",
        "# if accent_long_var=='Alemannische Färbung,Schweizer Standart Deutsch':\n",
        "#   accent_long_var='Alemannische ** Standart Deutsch'\n",
        "ax = sns.barplot(x = 'accent_long' , y = 'wer_accent_mean', data = dataset_accent_all_wer_mean_result)\n",
        "\n",
        "plt.xticks(rotation=70)\n",
        "  \n",
        "  \n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('plotBarDiagrams_Accent_WER.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V_yjbly0RnY"
      },
      "outputs": [],
      "source": [
        "#plt.subplot(2,3,1)\n",
        "#fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
        "#plt.legend()\n",
        "\n",
        "\n",
        "# position of the figure and figure;s size \n",
        "plt.figure(1,(10,15))  \n",
        "\n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "# plt.subplots(3,4)\n",
        "\n",
        "plt.xlabel(\"Speaker's accent - Austria\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_at['audio_filepath_numeric'],dataset_test_at['duration_numeric'], lw=1, ms=20)\n",
        "\n",
        "#kurvenname = ['blau', 'orange', 'grün', 'rot', 'lila']\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IngVteQv4h0W"
      },
      "outputs": [],
      "source": [
        "#plt.subplot(2,3,1)\n",
        "#fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
        "#plt.legend()\n",
        "\n",
        "# list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us','stop']\n",
        "# list1=['']\n",
        "# Accent_=0\n",
        "# print('#'*60,'Insert Accent Loop','#'*60)\n",
        "# for j in list_dataset_test_:\n",
        "#   for i in range(0,11):\n",
        "#     #print(i)\n",
        "#     # print(onetime)\n",
        "#     if j!='stop':\n",
        "#       #print(f'Accent_{j}={dataset_all_accent_mean.mean_of_all_accent[i]}')\n",
        "#       print(f\"\\n print(' Accent_{j}_Duration_mean= ')\")\n",
        "#       print(f\"\\n print({dataset_all_accent_mean.mean_of_all_accent[i]})\")\n",
        "#       stopcal=i+1\n",
        "#       j=list_dataset_test_[stopcal]\n",
        "#   if j=='stop':\n",
        "#     break\n",
        "#   print('#'*60,{j},{i},'#'*60)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "################################## Accent AT ###########################################\n",
        "# position of the figure and figure;s size \n",
        "figure_duriation_distribution_accent=plt.figure(12,figsize=(20,20))  \n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "ax_duriation_distribution_accent_at=figure_duriation_distribution_accent.add_subplot(3,4,1)\n",
        "\n",
        "# for i in range(len(dataset_accent_all_Duration_mean_result)):\n",
        "dataset_test_at['accent_Duration_mean'] = np.where(dataset_test_at['audio_filepath']!=np.NaN,5.127319, '000')\n",
        "\n",
        "plt.xlabel(\"Speaker's accent - Austria\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_at['audio_filepath_numeric'],dataset_test_at['duration_numeric'], lw=1, ms=20)\n",
        " \n",
        "################################## Accent CA ###########################################  \n",
        "ax_duriation_distribution_accent_ca=figure_duriation_distribution_accent.add_subplot(3,4,2)\n",
        "plt.xlabel(\"Speaker's accent - Canada\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_ca['audio_filepath_numeric'],dataset_test_ca['duration_numeric'], lw=1, ms=20)\n",
        "\n",
        "################################## Accent CH ###########################################\n",
        "ax_duriation_distribution_accent_ch=figure_duriation_distribution_accent.add_subplot(343)\n",
        "plt.xlabel(\"Speaker's accent - China\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_ch['audio_filepath_numeric'],dataset_test_ch['duration_numeric'], lw=1, ms=20)\n",
        "\n",
        "################################## Accent DE_AL ###########################################\n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "ax_duriation_distribution_accent_de_al=figure_duriation_distribution_accent.add_subplot(344)\n",
        "plt.xlabel(\"Speaker's accent - Deutsch AL\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_de_al['audio_filepath_numeric'],dataset_test_de_al['duration_numeric'], lw=1, ms=20)\n",
        "\n",
        "################################## Accent DE_NI ###########################################\n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "ax_duriation_distribution_accent_de_ni=figure_duriation_distribution_accent.add_subplot(345)\n",
        "plt.xlabel(\"Speaker's accent - Deutsch NI\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_de_ni['audio_filepath_numeric'],dataset_test_de_ni['duration_numeric'], lw=1, ms=20)\n",
        "\n",
        "################################## Accent DE ###########################################\n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "ax_duriation_distribution_accent_de=figure_duriation_distribution_accent.add_subplot(346)\n",
        "plt.xlabel(\"Speaker's accent - Deutsch\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_de['audio_filepath_numeric'],dataset_test_de['duration_numeric'], lw=1, ms=20)\n",
        "\n",
        "################################## Accent FR ###########################################\n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "ax_duriation_distribution_accent_fr=figure_duriation_distribution_accent.add_subplot(347)\n",
        "plt.xlabel(\"Speaker's accent - France\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_fr['audio_filepath_numeric'],dataset_test_fr['duration_numeric'], lw=1, ms=20)\n",
        "\n",
        "################################## Accent GB ###########################################\n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "ax_duriation_distribution_accent_gb=figure_duriation_distribution_accent.add_subplot(348)\n",
        "plt.xlabel(\"Speaker's accent - Great Britain\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_gb['audio_filepath_numeric'],dataset_test_gb['duration_numeric'], lw=1, ms=20)\n",
        "\n",
        "################################## Accent IT ###########################################\n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "ax_duriation_distribution_accent_it=figure_duriation_distribution_accent.add_subplot(349)\n",
        "plt.xlabel(\"Speaker's accent - Italy\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_it['audio_filepath_numeric'],dataset_test_it['duration_numeric'], lw=1, ms=20)\n",
        "\n",
        "################################## Accent RU ###########################################\n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "ax_duriation_distribution_accent_ru=figure_duriation_distribution_accent.add_subplot(3,4,10)\n",
        "plt.xlabel(\"Speaker's accent - Russian\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_ru['audio_filepath_numeric'],dataset_test_ru['duration_numeric'], lw=1, ms=20)\n",
        "\n",
        "################################## Accent  United States of America ###########################################\n",
        "#plt.subplots(figure's number per column,figure's number per row)\n",
        "ax_duriation_distribution_accent_us=figure_duriation_distribution_accent.add_subplot(3,4,11)\n",
        "plt.xlabel(\"Speaker's accent -  United States of America\")\n",
        "plt.ylabel(\"Duriation Distribution of Audiofiles\")\n",
        "plt.plot(dataset_test_us['audio_filepath_numeric'],dataset_test_us['duration_numeric'], lw=1, ms=20)\n",
        "\n",
        "# #kurvenname = ['blau', 'orange', 'grün', 'rot', 'lila']\n",
        "# #plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "############## note add the name of the plot ##Duriation Distribution of Audiofiles## to the plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-ZDR9lhs_xP"
      },
      "outputs": [],
      "source": [
        "list=range(1,10)\n",
        "for i in list:\n",
        "  \n",
        "  print(f\"\\n'{i}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh_A7pmxDCY0"
      },
      "outputs": [],
      "source": [
        "### insert in Dataframe\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "  \"name\": [\"Sally\", \"Mary\", \"John\"],\n",
        "  \"qualified\": [True, False, False]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df.insert(2, \"age\", [50, 40, 30])\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nimxuIbJDaYU"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=['a', 'b'])\n",
        "for i in range(20):\n",
        "    df = df.append({'a': 1, 'b': 2}, ignore_index=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp1KiPKNNoch"
      },
      "outputs": [],
      "source": [
        "list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us']\n",
        "list_female=list(map(lambda x:'female',list_dataset_test_))\n",
        "list_male=list(map(lambda x:'male',list_dataset_test_))\n",
        "print(list_female)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omaSppk8zmFU"
      },
      "outputs": [],
      "source": [
        "####################################################################################################\n",
        "####################Convert Json Index to column cone=tents in dataframe ##########################\n",
        "####################################################################################################\n",
        "# data_1 = {'Accent_short': list_dataset_test_,\n",
        "# 'Accent_long': list_dataset_test_accent,'twenties':list_twenties,'thirties':list_thirties,'fourties':list_fourties,'fifties':list_fifties  }\n",
        "# dataset_accent_age = pd.DataFrame(data_1)\n",
        "\n",
        "list_wer_filepath=[]\n",
        "for i in range(len(dataset_results_json)):\n",
        "  #print(f'dataset_results_json[\\'test_at.txt\\'].index[{i}]= \\n ',dataset_results_json['test_at.txt'].index[i])\n",
        "  # print()\n",
        "  # list_wer_filepath=dataset_results_json['test_at.txt'].index[i]\n",
        "  list_wer_filepath.insert(i,dataset_results_json['test_at.txt'].index[i])\n",
        "  wer_filepath={'audio_filepath_numeric':list_wer_filepath}\n",
        "  dataset_wer_filepath=pd.DataFrame(wer_filepath)\n",
        "  # a.insert(i,dataset_results_json['test_at.txt'].index[i])\n",
        "  # print(f'a= \\n ',a)\n",
        "# a.str[-12:-4].astype('str')\n",
        "dataset_wer_filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs6BxJ_vjQ-v"
      },
      "outputs": [],
      "source": [
        "### accent dataframe\n",
        "dataset_results_json_accent=pd.DataFrame({'accent_short':['at','gb','it','de_al','fr','de_ni','ch','de','us','ca','ru'],\n",
        "                                          'accent_long':['Österreichisches Deutsch','Britisches Deutsch','Italienisch Deutsch','Alemannische Färbung,Schweizer Standart Deutsch',\n",
        "'Französisch Deutsch','Niederländisch Deutsch','Schweizerdeutsch','Deutschland Deutsch','Amerikanisches Deutsch','Kanadisches Deutsch','Russisch Deutsch'],\n",
        "                                        'test_accent_txt':['test_at.txt','test_gb.txt','test_it.txt','test_de_al.txt','test_fr.txt','test_de_ni.txt','test_ch.txt',\n",
        "               'test_de.txt','test_us.txt','test_ca.txt','test_ru.txt']})\n",
        "\n",
        "\n",
        "dataset_results_json_accent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW68sOHs8zsK"
      },
      "source": [
        "## <font color=\"1497d4\">  **Running a Basic Emformer Training**</font>\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "## <font color=\"1497d4\">  **Accented Speech Recognition - Speech Repo**</font>\n",
        "\n",
        "<font color=white> **speech recognition @ voize**</font> \n",
        "\n",
        "## <font color=\"1497d4\">  **Development Workflow**</font>\n",
        " \n",
        "<font color=white> **Setup dev environment:**</font> \n",
        "\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGh7bia2_Mnx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94L5Tx3XRW7M"
      },
      "source": [
        "HelloWorld\n",
        "\n",
        "pytorch/android-demo-app\n",
        "\n",
        "#### 1. Model Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySQ9f6TD8rii"
      },
      "outputs": [],
      "source": [
        " !pip install torch torchvision\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9dvpjAZT6nu"
      },
      "source": [
        "To serialize and optimize the model for Android, you can use the Python script in the root folder of HelloWorld app:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1bzDUtHW6UG"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/drive/MyDrive/QU-DFKI-Thesis-ASR/Emformer-training/app', ignore_errors=True)\n",
        "shutil.rmtree('/content/drive/MyDrive/QU-DFKI-Thesis-ASR/Pytroch', ignore_errors=True)\n",
        "\n",
        "  # import os\n",
        " # os.rmdir('/content/drive/MyDrive/QU-DFKI-Thesis-ASR/Emformer-training/app/src/main/assets')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_loof79JGnu"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "import os\n",
        "\n",
        "model = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
        "model.eval()\n",
        "example = torch.rand(1, 3, 224, 224)\n",
        "traced_script_module = torch.jit.trace(model, example)\n",
        "optimized_traced_model = optimize_for_mobile(traced_script_module)\n",
        "\n",
        "os.makedirs('/content/drive/MyDrive/QU-DFKI-Thesis-ASR/Emformer-training/app/src/main/assets')\n",
        "optimized_traced_model._save_for_lite_interpreter(\"/content/drive/MyDrive/QU-DFKI-Thesis-ASR/Emformer-training/app/src/main/assets/model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIX90PMcY2SI"
      },
      "source": [
        "If everything works well, we should have our scripted and optimized model - model.pt generated in the assets folder of android application. That will be packaged inside android application as asset and can be used on the device.\n",
        "\n",
        "By using the new MobileNet v3 model instead of the old Resnet18 model, and by calling the optimize_for_mobile method on the traced model, the model inference time on a Pixel 3 gets decreased from over 230ms to about 40ms.\n",
        "\n",
        "More details about TorchScript you can find in tutorials on pytorch.org"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrvvy5FZY5T_"
      },
      "source": [
        " #### 2. Cloning from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxzZ_3gNRp3v"
      },
      "outputs": [],
      "source": [
        "# os.makedirs('/content/drive/MyDrive/QU-DFKI-Thesis-ASR/Pytroch') \n",
        "!git clone https://github.com/pytorch/android-demo-app.git\n",
        "! https://gitlab.com/voize-gmbh/machine-learning/speech.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSHktqZFY-hm"
      },
      "outputs": [],
      "source": [
        "!cd /content/android-demo-app/HelloWorldApp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqjFYn85a0p2"
      },
      "outputs": [],
      "source": [
        "!ls\n",
        "!./gradlew installDebug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGkxtdNbak2v"
      },
      "outputs": [],
      "source": [
        " os.makedirs('/content/drive/MyDrive/QU-DFKI-Thesis-ASR/Pytroch') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFovwcIVagcB"
      },
      "outputs": [],
      "source": [
        "shutil.rmtree('/content/drive/MyDrive/QU-DFKI-Thesis-ASR/Pytroch', ignore_errors=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqsstrKOtoee"
      },
      "source": [
        "## <font color=\"1497d4\">  **Running a Basic Emformer Training**</font>\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "## <font color=\"1497d4\">  **Accented Speech Recognition - Speech Repo**</font>\n",
        "\n",
        "<font color=white> **speech recognition @ voize**</font> \n",
        "\n",
        "## <font color=\"1497d4\">  **Development Workflow**</font>\n",
        " \n",
        "<font color=white> **Setup dev environment:**</font> \n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KeOKUUAziU7"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('/c\\ontent/venv', ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWK6bUUA2y6p"
      },
      "outputs": [],
      "source": [
        "!git clone git@gitlab.com:voize-gmbh/machine-learning/speech.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWU0Eo8bSc9v"
      },
      "source": [
        " ## Statistical Analysis for Accented Speech recognition\n",
        " ## Mozilla Common Voice 10.0. dataset download \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCWseaSr3NI1"
      },
      "source": [
        "#### **print two factors with connections**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxWMnLWN3hhy"
      },
      "outputs": [],
      "source": [
        "list_dataset_test_=['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us']\n",
        "list_dataset_test_accent=['Österreichisches Deutsch','Kanadisches Deutsch','Schweizerdeutsch','Alemannische Färbung,Schweizer Standart Deutsch','Niederländisch Deutsch',' Deutschland Deutsch','Französisch Deutsch\t','Britisches Deutsch',' Italienisch Deutsch'\n",
        ",'Russisch Deutsch','Amerikanisches Deutsch']\n",
        "\n",
        "  \n",
        "data_2 = {'Accent_short': ['at','ca','ch','de_al','de_ni','de','fr','gb','it','ru','us'],\n",
        "                  'Accent_long': ['Österreichisches Deutsch','Kanadisches Deutsch','Schweizerdeutsch','Alemannische Färbung,Schweizer Standart Deutsch','Niederländisch Deutsch',' Deutschland Deutsch','Französisch Deutsch\t','Britisches Deutsch',' Italienisch Deutsch'\n",
        ",'Russisch Deutsch','Amerikanisches Deutsch']\n",
        "                  }\n",
        "dataset_accent = pd.DataFrame(data_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV0er1qb6v6V"
      },
      "outputs": [],
      "source": [
        "# print accent short and long one time for eahc accent\n",
        "for i in range(0,len(dataset_accent)):\n",
        "\n",
        "    accent_short_var=dataset_accent['Accent_short'][i]\n",
        "    accent_long_var=dataset_accent['Accent_long'][i]\n",
        "    print(accent_short_var,accent_long_var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gdEKBRio6vz8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee208c10-1a32-4e83-c88a-9c6e879b19ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before import\n",
            "before function_a\n",
            "before function_b\n",
            "before __name__ guard\n",
            "Function A\n",
            "Function B 10.0\n",
            "after __name__ guard\n"
          ]
        }
      ],
      "source": [
        "# Suppose this is foo.py.\n",
        "\n",
        "print(\"before import\")\n",
        "import math\n",
        "\n",
        "print(\"before function_a\")\n",
        "def function_a():\n",
        "    print(\"Function A\")\n",
        "\n",
        "print(\"before function_b\")\n",
        "def function_b():\n",
        "    print(\"Function B {}\".format(math.sqrt(100)))\n",
        "\n",
        "print(\"before __name__ guard\")\n",
        "if __name__ == '__main__':\n",
        "    function_a()\n",
        "    function_b()\n",
        "print(\"after __name__ guard\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"welcome to the Universe\"\n",
        "\n",
        "x = txt.split()\n",
        "\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B22yhlhRMAr",
        "outputId": "e1d286b8-029c-4273-f82f-7416f32d3cbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['welcome', 'to', 'the', 'Universe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Selam, The last Prophet after Juses son of Maryim PBUT is Prophet Muhammed PBUH, He is an amazing Perfect Person\"\n",
        "\n",
        "x = txt.split(\", \")\n",
        "\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QBhrJCXRZLu",
        "outputId": "b9afb32b-52a5-473f-9d8f-b5325b9e9cf4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Selam', 'The last Prophet after Juses son of Maryim PBUT is Prophet Muhammed PBUH', 'He is an amazing Perfect Person']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt=\"Selam,#The#last#Prophet#after#Juses#son#of#Maryim#PBUT#is#Prophet#Muhammed#PBUH,#He#is#an#amazing#Perfect#Person\"\n",
        "\n",
        "# setting the maxsplit parameter to 1, will return a list with 2 elements!\n",
        "x = txt.split(\"#\",5)\n",
        "\n",
        "print(x)\n",
        "print('**'*60)\n",
        "print(len(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPjSwZ6PRiep",
        "outputId": "a2fe180e-85dc-4665-aa64-ffc00be9a7c3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Selam,', 'The', 'last', 'Prophet', 'after', 'Juses#son#of#Maryim#PBUT#is#Prophet#Muhammed#PBUH,#He#is#an#amazing#Perfect#Person']\n",
            "************************************************************************************************************************\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Wichtig###\n",
        "# Iterate through two sequences simultaneously: zip\n",
        "# كرر من خلال سلسلتين في وقت واحد: zip\n",
        "\n",
        "\n",
        "for x,y in zip('Hallo',[12, -7.3, 1, 0, False]):\n",
        "    print(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8ITk5eASMvg",
        "outputId": "71fec360-5d36-4c55-cc80-d506e9c75dd1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H 12\n",
            "a -7.3\n",
            "l 1\n",
            "l 0\n",
            "o False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y, in enumerate([12, -7.3, 1, 0, False]):\n",
        "    print(x,y)\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCI3mDnVZ6VZ",
        "outputId": "1b214c6e-22a9-4fb8-de25-447cf7e71263"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 12\n",
            "1 -7.3\n",
            "2 1\n",
            "3 0\n",
            "4 False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "13. Enumerate\n",
        "\n",
        "Enumerate is a built-in function of Python. Its usefulness can not be summarized in a single line.\n",
        "Yet most of the newcomers and even some advanced programmers are unaware of it.\n",
        "\n",
        "***It allows us to loop over something and have an automatic counter.***\n",
        "Here is an example:\n",
        "    '''\n",
        "\n",
        "my_list = ['apple', 'banana', 'grapes', 'pear']\n",
        "for counter, value in enumerate(my_list):\n",
        "  print(counter, value)\n",
        "\n",
        "# Output:\n",
        "# 0 apple\n",
        "# 1 banana\n",
        "# 2 grapes\n",
        "# 3 pear\n",
        "'''\n",
        "And there is more! enumerate also accepts an optional argument that allows us to specify the starting index of the counter.\n",
        "'''\n",
        "my_list = ['apple', 'banana', 'grapes', 'pear']\n",
        "for c, value in enumerate(my_list, 3):\n",
        "    print(c, value)\n",
        "\n",
        "# Output:\n",
        "# 3 apple\n",
        "# 4 banana\n",
        "# 5 grapes\n",
        "# 6 pear\n",
        "'''\n",
        "An example of where the optional argument of enumerate comes in handy is creating tuples\n",
        "containing the index and list item using a list. \n",
        "Here is an example:\n",
        "'''\n",
        "my_list = ['apple', 'banana', 'grapes', 'pear']\n",
        "counter_list = list(enumerate(my_list, 1))\n",
        "print(counter_list)\n",
        "# Output: [(1, 'apple'), (2, 'banana'), (3, 'grapes'), (4, 'pear')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zap_iCnNaH5K",
        "outputId": "c3630070-1f5f-4a0e-e699-28e0afd86ffb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 apple\n",
            "1 banana\n",
            "2 grapes\n",
            "3 pear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = ['apple', 'banana', 'grapes', 'pear']\n",
        "for c, value in enumerate(my_list, 3):\n",
        "    print(c, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE1gNuYLbrLl",
        "outputId": "29f9d430-e56f-4b7f-8d80-437d29a8fa1f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 apple\n",
            "4 banana\n",
            "5 grapes\n",
            "6 pear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uic7qSDpcKW8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "94L5Tx3XRW7M",
        "wrvvy5FZY5T_"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}