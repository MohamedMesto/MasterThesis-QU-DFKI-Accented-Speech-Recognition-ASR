{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-xBkG5K3e_4h"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roshdl_R_SVy"
      },
      "source": [
        "# <font color=\"00ff00\">  **Statistical Analysis for Accented Speech recognition**</font>\n",
        "<font color=white> **evaluates the performance of an ASR model regarding accented speech**</font> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"00ff00\">  **Analyze results from Carlos’ model**</font>\n",
        "<font color=white> **Using Gradient-based techniques:**</font> "
      ],
      "metadata": {
        "id": "6rRyLoti4u0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"00ff00\">  **3- Gradient-based Analysis**</font>\n",
        "<font color=white> **simple gradient-based explanation method considers the gradient of the output $f_j$ from a neural network (where j denotes a target class) with respect to an input $x_i$ (where i refers to the ith input time-step used to index the input sequence $x$):**</font> \n",
        "\n",
        "<font color=white>\n",
        "$grad(j,i,x)={\\displaystyle \\frac{\\partial f_j}{\\partial x_i}}$</font>\n",
        "\n",
        "\n",
        " <font color=\"00ff00\">  **3.1 Attribution Analysis**</font>\n"
      ],
      "metadata": {
        "id": "irLwIKfudK8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hyphenate\n",
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVBGZ63ihR9d",
        "outputId": "d5f42394-1b35-40ce-941e-64b523b99f52"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hyphenate in /usr/local/lib/python3.8/dist-packages (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.8/dist-packages (0.20.9)\n",
            "Requirement already satisfied: Levenshtein==0.20.9 in /usr/local/lib/python3.8/dist-packages (from python-Levenshtein) (0.20.9)\n",
            "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from Levenshtein==0.20.9->python-Levenshtein) (2.13.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "A0a9FxLJe_4O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import json\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "from IPython.display import display\n",
        "from scipy.io import wavfile\n",
        "from itertools import chain \n",
        "import string\n",
        "import Levenshtein as Lev\n",
        "from itertools import groupby\n",
        "import scipy.stats as st\n",
        "from scipy import signal\n",
        "import nltk\n",
        "from scipy.stats import wasserstein_distance as wd\n",
        "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
        "from pyemd import emd\n",
        "from hyphenate import hyphenate_word\n",
        "from itertools import islice \n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5tb_f6Ke_4U"
      },
      "source": [
        "###<font color=\"00ff00\">  **Set up dataframe for transcripts and files:**</font>\n",
        "<font color=white> **Set up dataframe for transcripts and files- Test_accent.txt file case:**</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "PLv5IywBU061"
      },
      "outputs": [],
      "source": [
        "# Import the dataset file by method1 \n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/results.json   /content/results.json\n",
        "\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/results-AOE.json   /content/results-AOE.json\n",
        "\n",
        "# copy the expermintations files to deal with them\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_at.txt /content/test_at.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_ca.txt /content/test_ca.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_ch.txt /content/test_ch.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_de_al.txt /content/test_de_al.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_de_ni.txt /content/test_de_ni.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_de.txt /content/test_de.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_fr.txt /content/test_fr.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_gb.txt /content/test_gb.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_it.txt /content/test_it.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_ru.txt /content/test_ru.txt\n",
        "! cp /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_us.txt /content/test_us.txt\n",
        "\n",
        "# copy the expermintations files from Mozilla Commen Voice v 10 to deal with them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "cSPfkR6_e_4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79bae46b-c4fe-464b-b8f3-968fbc7d2b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'> \n",
            "                        file  \\\n",
            "0  common_voice_de_17784450   \n",
            "1  common_voice_de_20428000   \n",
            "2  common_voice_de_32231734   \n",
            "3  common_voice_de_17783886   \n",
            "4  common_voice_de_32230545   \n",
            "5  common_voice_de_17873902   \n",
            "6  common_voice_de_32239515   \n",
            "7  common_voice_de_32224281   \n",
            "8  common_voice_de_28908199   \n",
            "9  common_voice_de_18002381   \n",
            "\n",
            "                                          transcript          duration  \n",
            "0  \"text\": \"auch posen und krakau waren mal haupt...  \"duration\": 6.2}  \n",
            "1  \"text\": \"in dieser funktion produzierte er die...  \"duration\": 8.3}  \n",
            "2  \"text\": \"er besuchte zunächst die stadtschule ...  \"duration\": 7.3}  \n",
            "3            \"text\": \"wir haben euch schon erwartet\"  \"duration\": 2.8}  \n",
            "4  \"text\": \"sie ist weiterhin aufsichtsratsmitgli...  \"duration\": 7.6}  \n",
            "5  \"text\": \"die firewall ist löchrig wie ein schw...  \"duration\": 4.9}  \n",
            "6  \"text\": \"ray geht still mit maggie weiter zum ...  \"duration\": 4.9}  \n",
            "7  \"text\": \"das findet sich auch im bereich des r...  \"duration\": 6.2}  \n",
            "8  \"text\": \"holger astrup ist verheiratet und hat...  \"duration\": 4.0}  \n",
            "9  \"text\": \"ja das habe ich auch schon versucht a...  \"duration\": 3.6}  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2908"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "# my_data/all_overlap.txt ==>> test_at.txt\n",
        "# transcript ==>> text\n",
        "# file==>> audio_filepath\n",
        "###############################################\n",
        "df_trans_ch = pd.read_csv('test_ch.txt', delimiter = \", \", header = None ,encoding=\"utf-8\", names=['file','transcript','duration'])\n",
        "df_trans_ch['file'] = df_trans_ch['file'].map(lambda x: x.split('.',6)[2])\n",
        "df_trans_ch['file'] = df_trans_ch['file'].map(lambda x: x.split('/',6)[3])\n",
        "# df_trans_ch['text'] = df_trans_ch['text'].map(lambda x: x.split(' ')[0])\n",
        "print(type(df_trans_ch),'\\n',df_trans_ch.head(10))\n",
        "\n",
        "len(df_trans_ch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "j2D7I0sJe_4Z"
      },
      "outputs": [],
      "source": [
        "# my_data/common-final-file-info.json==>> results-AOE.json\n",
        "with open('results-AOE.json', 'r', encoding=\"utf-8\") as j:\n",
        "\tfile_meta = json.load(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "IXYiceV2e_4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd2064e-57b0-457e-94eb-8cba59d16cfb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2858"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "transcripts = list(set(df_trans_ch['transcript'].tolist()))\n",
        "len(transcripts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Qmea_YEBe_4a"
      },
      "outputs": [],
      "source": [
        "trans_dict = {x:[] for x in transcripts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "mz57wYNFe_4b"
      },
      "outputs": [],
      "source": [
        "for index, row in df_trans_ch.iterrows():\n",
        "    trans_dict[row['transcript']].append(row['file'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################## show the first 10 rows of the dictionary trans_dict##################################\n",
        "from itertools import islice\n",
        "\n",
        "def take(n, iterable):\n",
        "    \"\"\"Return the first n items of the iterable as a list.\"\"\"\n",
        "    return list(islice(iterable, n))\n",
        "\n",
        "n_items = take(10, trans_dict.items())\n",
        "n_items \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tUt29iOaC_C",
        "outputId": "8b387400-b35e-40c1-b348-c1ff18505688"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('\"text\": \"die bahnstation befindet sich an der strecke von kasan nach jekaterinburg\"',\n",
              "  ['common_voice_de_32229891']),\n",
              " ('\"text\": \"wir wollen faire preise für die landwirte in allen entwicklungsländern\"',\n",
              "  ['common_voice_de_21333785']),\n",
              " ('\"text\": \"du bist spät dran\"', ['common_voice_de_18198200']),\n",
              " ('\"text\": \"grundsätzlich können makler und gutachter den verkehrswert einer immobilie ermitteln\"',\n",
              "  ['common_voice_de_20586602']),\n",
              " ('\"text\": \"breil eingelagert\"', ['common_voice_de_28892600']),\n",
              " ('\"text\": \"das genre ist bis heute in verschiedenen variationen beliebt\"',\n",
              "  ['common_voice_de_32226371']),\n",
              " ('\"text\": \"es wird dem thüringer klimabereich „zentrale mittelgebirge und harz“ zugeordnet\"',\n",
              "  ['common_voice_de_32236092']),\n",
              " ('\"text\": \"ich brauche was richtiges gegen den hunger\"',\n",
              "  ['common_voice_de_17845491']),\n",
              " ('\"text\": \"seine kurzgeschichten erschienen in einem guten dutzend sammlungen\"',\n",
              "  ['common_voice_de_28908292']),\n",
              " ('\"text\": \"es folgte eine arbeit mit bezug zu deutschsprachigen ländern\"',\n",
              "  ['common_voice_de_32225017'])]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### find the Audio file name\n",
        "trans_dict['\"text\": \"diese mod fügt hauptsächlich hochauflösende texturen hinzu\"']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDXdL3UCrFF9",
        "outputId": "507df946-135f-4746-849e-c423d1f37a35"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['common_voice_de_17712128']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<font color=\"00ff00\">  **Set up dataframe for transcripts and files:**</font>\n",
        "<font color=white> **Set up dataframe for transcripts and files- Results.json file case:**</font> \n",
        " "
      ],
      "metadata": {
        "id": "U1npwGrFX-Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## store the contents of Accented files in an extra text files  ####################################################\n",
        "####################################################################################################################################################\n",
        "import json\n",
        "if __name__ == \"__main__\":\n",
        "  results = json.load(open('/content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/results.json'))\n",
        "  # Required_text=input(\"inser thte sentence: \")\n",
        "  for test_file in results:\n",
        "    list_values = [v[\"reference\"] for v in results[test_file].values()]\n",
        "    # string(ref_lens)\n",
        "    # print(f'{test_file} \\n',ref_lens)\n",
        "    str_values = ', '.join(str(x) for x in list_values)\n",
        "    \n",
        "    # print(df_trans.head(10))\n",
        "\n",
        "    # len(df_trans)\n",
        "\n",
        "    import os\n",
        "    # remove the script if exists \n",
        "    # os.remove(\"dataset_accent.py\")\n",
        "    # open script to write in the calculation of Mean of all accent\n",
        "    dataset_accent_write_file = open(f'dataset_{test_file}.py','w')\n",
        "    dataset_accent_write_file.write(str_values)\n",
        "    dataset_accent_write_file.close()\n",
        "    ! cp /content/*.txt.py /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_data/"
      ],
      "metadata": {
        "id": "MmAtoPKsX9X-"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_results_json = pd.read_json('results.json',encoding=\"utf-8\")\n",
        "dataset_results_json['test_at.txt'][3962]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8ee5e0-2c7e-43c5-be67-e92d93d8dbb2",
        "id": "hhQwK-TJX9X_"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'reference': 'chișinău ist die hauptstadt von moldau',\n",
              " 'hypothesis': 'kischinau ist die hauptstadt von moldau',\n",
              " 'wer': 0.16666666666666602}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tel_nr = {'Herold'      : 24862 ,\n",
        "          'Kisler'      : 22440 ,\n",
        "          'Schneehagen' : 23419 ,\n",
        "          'Stampka'     : 22991 ,\n",
        "          'Kujawski'    : 22440 ,\n",
        "          'Masovic'     : 22761 ,\n",
        "          'Jekosch'     : 25211 ,\n",
        "          'Paskiewicz'  : 29403 ,\n",
        "          'Radmann'     : 29403}\n",
        "\n",
        "tel_nr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylyF1MA0y78w",
        "outputId": "d652c5aa-4f2e-40e6-a22a-f64c20e08815"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Herold': 24862,\n",
              " 'Kisler': 22440,\n",
              " 'Schneehagen': 23419,\n",
              " 'Stampka': 22991,\n",
              " 'Kujawski': 22440,\n",
              " 'Masovic': 22761,\n",
              " 'Jekosch': 25211,\n",
              " 'Paskiewicz': 29403,\n",
              " 'Radmann': 29403}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################################################################################################################################\n",
        "############################# Creat Dictionary \"trans_dict_results\" of all Accent Audio files number and thier Values###################################\n",
        "########################################################################################################################################################\n",
        "import re\n",
        "import json\n",
        "\n",
        "trans_dict_results = {'Keys_MMM_2050': 1000}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  Dict_results = json.load(open('/content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/results.json'))\n",
        "  for test_file in Dict_results:\n",
        "    list_test_accent_txt_values = [v[\"reference\"] for v in Dict_results[test_file].values()]\n",
        "    # print('*********\\n',list_test_accent_txt_values)\n",
        "    # list_test_accent_txt_keys=[k.split('.',6)[2] for k in Dict_results[test_file].keys()]\n",
        "    list_test_accent_txt_keys=[re.split(r'[.|/]',k)[9] for k in Dict_results[test_file].keys()]\n",
        "\n",
        "    trans_dict_test_file_result = dict(zip(list_test_accent_txt_keys, list_test_accent_txt_values))\n",
        "    trans_dict_results.update(trans_dict_test_file_result)\n",
        "    \n",
        "######################################## write the dictionary \"trans_dict\" to a json file ###############################\n",
        "with open(\"trans_dict_results.json\",\"w\", encoding='utf-8') as jsonfile:  \n",
        "  json.dump(trans_dict_results,jsonfile,ensure_ascii=False)\n",
        "! cp /content/trans_dict_results.json /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/test_data/\n"
      ],
      "metadata": {
        "id": "4p_iVHNwavFz"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_dict_results\n",
        "len(trans_dict_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c78v0RAcuD4",
        "outputId": "c7582c63-1c28-42d3-e831-c9d403f25f5d"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47275"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Show the first 10 elements of the dict\n",
        "n_items = take(10, trans_dict_results.items())\n",
        "n_items "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnEJkuAIfpA5",
        "outputId": "3a4e97fc-7ec6-4006-8218-489806ff5bb8"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Keys_MMM_2050', 1000),\n",
              " ('common_voice_de_31449916',\n",
              "  'er ist nach eustache de saint pierre der bekannteste der gruppe'),\n",
              " ('common_voice_de_19730674',\n",
              "  'eine erste kleinformatige vorstudie in bleistift findet sich in einem von kellers skizzenbüchern'),\n",
              " ('common_voice_de_19703888',\n",
              "  'er blieb ohne erfolg und lag in ständigem streit mit den österreichischen generälen'),\n",
              " ('common_voice_de_18507972', 'mein garten ist teil eines kleingartenvereins'),\n",
              " ('common_voice_de_24131267', 'was habe ich damals für einen unsinn geträumt'),\n",
              " ('common_voice_de_21905399',\n",
              "  'der erste punkt bezieht sich auf die angst vor einwanderungswellen'),\n",
              " ('common_voice_de_20143462',\n",
              "  'anschließend war er bei der regierung in düsseldorf beschäftigt'),\n",
              " ('common_voice_de_21889444',\n",
              "  'jede quersubventionierung soll in zukunft unzulässig sein'),\n",
              " ('common_voice_de_18192538',\n",
              "  'silke und marco verkrümelten sich unauffällig um allein zu sein')]"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trans_dict_results"
      ],
      "metadata": {
        "id": "Zi7z_vW2ctW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "cCE6u3h9e_4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "f23c52e9-fcc8-41f3-c5d3-e5a96aa90987"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-190-3314789a9f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'common_voice_de_17712128'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_trans_ch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_trans_ch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'common_voice_de_17712128'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'common_voice_de_17712128'"
          ]
        }
      ],
      "source": [
        "file_meta['common_voice_de_17712128']['accent']\n",
        "print(df_trans_ch[df_trans_ch['file'] == 'common_voice_de_17712128'].transcript.values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVr1F1UYe_4d"
      },
      "source": [
        "###<font color=\"00ff00\">  **Normalize Attributions**</font>\n",
        "<font color=white> **Normalize Attributions**</font> "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " ! cp  /content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/audio_files/*.* /content/audio_files"
      ],
      "metadata": {
        "id": "3OO6wNRmna7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the contents of Accent tExt file\n",
        "# import json\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     results = json.load(open('/content/drive/MyDrive/QU-DFKI-Thesis-ASR/Experimentation/data/results.json'))\n",
        "#     # Required_text=input(\"inser thte sentence: \")\n",
        "#     for test_file in results:\n",
        "#       # if Required_text in \n",
        "#       #   # ref_lens = [len(v[\"reference\"].split(\" \")) for v in results[test_file].values()]\n",
        "#       #   # edits = [int(v[\"wer\"] * ref_lens[i]) for i, v in enumerate(results[test_file].values())]\n",
        "#       #   # print(test_file, round(sum(edits) / sum(ref_lens) * 100, 2))\n",
        "#       #   print(test_file)\n",
        "#       [print(v['reference']) for v in results['test_ca.txt'].values()]\n",
        "#       # available_text = [v.split(\", \") for v in results[test_file].values()]\n",
        "\n",
        "#       # if Required_text==available_text:\n",
        "#       #   print(f'the sentence exists in {test_file}')"
      ],
      "metadata": {
        "id": "n_kPgSWPVJcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<font color=\"00ff00\">  **Preparing the data Audio files:**</font>\n",
        "<font color=white> **Convert the data Audio MP3 files to WAV files :**</font> "
      ],
      "metadata": {
        "id": "XyKNI6NyLwqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################## Convert the data Audio MP3 files to WAV files  #####################\n",
        "\n",
        "! pip install pydub\n",
        "! apt-get install ffmpeg\n",
        "from os import path\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# files                                                                         \n",
        "src = \"/content/audio_files/common_voice_de_17712128.mp3\"\n",
        "dst = \"/content/audio_files/common_voice_de_17712128.wav\"\n",
        "\n",
        "# convert wav to mp3                                                            \n",
        "sound = AudioSegment.from_mp3(src)\n",
        "sound.export(dst, format=\"wav\")"
      ],
      "metadata": {
        "id": "grJmbv_xtjQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp2oktZOe_4d"
      },
      "outputs": [],
      "source": [
        "def get_norm_attr(index_dict):\n",
        "    norm_dict = {}\n",
        "    for key in index_dict.keys():\n",
        "        norm_dict[key] = index_dict[key]/np.sum(index_dict[key])\n",
        "        np.set_printoptions(precision=4)\n",
        "        #print(norm_dict[key])\n",
        "        #break\n",
        "    return norm_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2E7W5IYe_4e"
      },
      "source": [
        "###<font color=white> **Fetching frame allignmnets from the meta-data (using gentle)**</font> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baaKJUxee_4e"
      },
      "outputs": [],
      "source": [
        "def get_frame_allignment(file, input_size):\n",
        "    alligned = []\n",
        "    spec_stride = 0.01\n",
        "    window_size = 0.02\n",
        "    times = file_meta[file]['end_times']\n",
        "    last_idx = 0\n",
        "    for i in range(input_size):\n",
        "        frame_idx = i\n",
        "        window_start = frame_idx*spec_stride\n",
        "        window_mid = window_start + (window_size/2)\n",
        "        alligned_phone = 'na'\n",
        "        for j in range(len(times)):\n",
        "            if (window_mid < times[j]):\n",
        "                alligned_phone = file_meta[file]['phones'][j]\n",
        "                break\n",
        "        #assert alligned_phone != 'na', \"Failed to fetch allignment\"\n",
        "        if(alligned_phone != 'na'):\n",
        "            alligned.append(alligned_phone)\n",
        "            last_idx = i\n",
        "    pause_start = 0\n",
        "    pause_end = len(alligned)\n",
        "    for i in range(len(alligned)):\n",
        "        if(alligned[i] != 'pause'):\n",
        "            break\n",
        "        pause_start = i\n",
        "    \n",
        "    for i in range(len(alligned)-1,-1,-1):\n",
        "        if(alligned[i] != 'pause'):\n",
        "            break\n",
        "        pause_end = i\n",
        "        \n",
        "    #print(last_idx)\n",
        "    #print(pause_start, pause_end)\n",
        "    return alligned, pause_start +1, pause_end\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSsnMpzYe_4f"
      },
      "source": [
        "###<font color=white> **Visualizing (signed) attributions**</font>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBP-vl4pe_4g"
      },
      "outputs": [],
      "source": [
        "for file in trans_dict['\"text\": \"diese mod fügt hauptsächlich hochauflösende texturen hinzu\"']:\n",
        "    try:\n",
        "        #file = 'common_voice_de_17712128'\n",
        "        Fs, wav = wavfile.read('/content/audio_files/common_voice_de_17712128.wav'.format(file_meta[file]['accent'],file))\n",
        "        display(Audio(wav, rate=Fs))\n",
        "        with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "            file_attr = pickle.load(j)\n",
        "        print(file_attr['output'], file_meta[file]['accent'])\n",
        "        print(list(file_attr['attr dict'].keys()))\n",
        "        normalized_attr = get_norm_attr(file_attr['attr dict'])\n",
        "        keys = list(normalized_attr.keys())\n",
        "        input_size = len(file_attr['attr dict'][list(file_attr['attr dict'].keys())[0]]) # calculate properly once\n",
        "        buffer = 10\n",
        "    #print(get_frame_allignment(file,input_size))\n",
        "    #     with np.printoptions(precision=4, suppress=True):\n",
        "    #         print(normalized_attr[1][:291])\n",
        "        plot_vertical = False\n",
        "        idx = 0\n",
        "        if(plot_vertical):\n",
        "            fig = plt.figure(figsize = (1,35))\n",
        "            print(get_frame_allignment(file, input_size))\n",
        "            allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "            allignments = np.asarray(allignments)            \n",
        "            actual_size = len(allignments)\n",
        "            sns.heatmap(np.expand_dims(normalized_attr[1][:actual_size], axis = 1),annot = np.expand_dims(allignments, axis = 1), fmt=\"\", cmap='RdBu')\n",
        "            #plt.yticks(rotation=) \n",
        "            #plt.yticks(range(allignments.shape[0]),list(allignments))\n",
        "            plt.show()\n",
        "        else:\n",
        "            \n",
        "            fig = plt.figure(figsize = (37,1))\n",
        "            allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "            allignments = np.asarray(allignments)            \n",
        "            actual_size = len(allignments)\n",
        "            #actual_size = len(allignments)\n",
        "            \n",
        "            print('for',file_attr['output'][keys[idx]])\n",
        "            sns.heatmap(np.expand_dims(normalized_attr[keys[idx]][p_start - buffer:p_end + buffer], axis = 0), fmt=\"\", cmap='RdBu')\n",
        "            #plt.yticks(rotation=) \n",
        "            modified_allignments = allignments[p_start - buffer:p_end + buffer]\n",
        "            #phone_centres = []\n",
        "            phone_labels = [x[0] for x in groupby(modified_allignments)]\n",
        "            len_list = [len(list(x[1])) for x in groupby(modified_allignments)]\n",
        "            idx_list = [0]\n",
        "            ticks_list = []\n",
        "            for l in len_list:\n",
        "                ticks_list.append(idx_list[-1] + l//2)\n",
        "                idx_list.append(idx_list[-1] + l)\n",
        "                \n",
        "           \n",
        "            plt.xticks(ticks_list,phone_labels, rotation = 90)\n",
        "            \n",
        "            for j in idx_list:\n",
        "                plt.axvline(x=j, color='w', linestyle='-', linewidth=2.5)\n",
        "                plt.axvline(x=j, color='k', linestyle='--', linewidth=2.5)\n",
        "                \n",
        "                \n",
        "                \n",
        "            plt.show()\n",
        "    #break\n",
        "    except:\n",
        "        print('failed for file : {}'.format(file))\n",
        "        continue\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIt-eyf0e_4h"
      },
      "source": [
        "###<font color=\"00ff00\">  **Visualizing (modulus/magnitude) attributions**</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xBkG5K3e_4h"
      },
      "source": [
        "###<font color=white> Generate **|*Input.Gradient*|** Attributution at grapheme-level</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX87FXU3e_4i"
      },
      "outputs": [],
      "source": [
        "#for file in trans_dict[\"I'm going to them.\"]:\n",
        "def inp_grad_grapheme(file, idx):\n",
        "    try:\n",
        "        #file = 'common_voice_en_110121'\n",
        "#         Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "#         display(Audio(wav, rate=Fs))\n",
        "        with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "            file_attr = pickle.load(j)\n",
        "#         print(file_attr['output'], file_meta[file]['accent'])\n",
        "#         print(list(file_attr['attr dict'].keys()))\n",
        "        normalized_attr = get_norm_attr(file_attr['attr dict'])\n",
        "        keys = list(normalized_attr.keys())\n",
        "        input_size = len(file_attr['attr dict'][list(file_attr['attr dict'].keys())[0]]) # calculate properly once\n",
        "        buffer = 10\n",
        "    #print(get_frame_allignment(file,input_size))\n",
        "    #     with np.printoptions(precision=4, suppress=True):\n",
        "    #         print(normalized_attr[1][:291])\n",
        "        plot_vertical = False\n",
        "        #idx = 0\n",
        "        \n",
        "        if(plot_vertical):\n",
        "            fig = plt.figure(figsize = (1,35))\n",
        "            print(get_frame_allignment(file, input_size))\n",
        "            allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "            allignments = np.asarray(allignments)            \n",
        "            actual_size = len(allignments)\n",
        "            sns.heatmap(np.expand_dims(normalized_attr[1][:actual_size], axis = 1),annot = np.expand_dims(allignments, axis = 1), fmt=\"\", cmap='RdBu')\n",
        "            #plt.yticks(rotation=) \n",
        "            #plt.yticks(range(allignments.shape[0]),list(allignments))\n",
        "            plt.show()\n",
        "        else:\n",
        "            \n",
        "            fig = plt.figure(figsize = (37,1))\n",
        "            #print(allignments)\n",
        "            allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "            allignments = np.asarray(allignments)            \n",
        "            actual_size = len(allignments)\n",
        "            #actual_size = len(allignments)\n",
        "            \n",
        "            print('for',file_attr['output'][keys[idx]])\n",
        "            sns.heatmap(np.expand_dims(np.abs(normalized_attr[keys[idx]][p_start - buffer:p_end + buffer])/np.sum(np.abs(normalized_attr[keys[idx]][p_start - buffer:p_end + buffer])), axis = 0), fmt=\"\", cmap='Blues')\n",
        "            #plt.yticks(rotation=) \n",
        "            modified_allignments = allignments[p_start - buffer:p_end + buffer]\n",
        "            phone_labels = [x[0] for x in groupby(modified_allignments)]\n",
        "            len_list = [len(list(x[1])) for x in groupby(modified_allignments)]\n",
        "            idx_list = [0]\n",
        "            ticks_list = []\n",
        "            for l in len_list:\n",
        "                ticks_list.append(idx_list[-1] + l//2)\n",
        "                idx_list.append(idx_list[-1] + l)\n",
        "                \n",
        "            #ticks_list.append((idx_list[-1] + len(modified_allignments))//2)   \n",
        "            \n",
        "            plt.xticks(ticks_list,phone_labels, rotation = 90)\n",
        "            \n",
        "            for j in idx_list:\n",
        "                plt.axvline(x=j, color='w', linestyle='-', linewidth=2.5)\n",
        "                plt.axvline(x=j, color='k', linestyle='--', linewidth=2.5)\n",
        "            plt.show()\n",
        "                \n",
        "    #break\n",
        "    except:\n",
        "        print('failed for file : {}'.format(file))\n",
        "        #continue\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoYJlqjze_4i"
      },
      "source": [
        "##### Helper functions for word-level allignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zkY1FE2e_4j"
      },
      "outputs": [],
      "source": [
        "def get_space(inp):\n",
        "    spaces = []\n",
        "    for idx, val in enumerate(inp):\n",
        "        if(val == ' '): spaces.append(idx)\n",
        "    return spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOFagSc9e_4j"
      },
      "outputs": [],
      "source": [
        "def get_words(test_list, split_list,spaces):\n",
        "    temp = zip(chain([0], split_list), chain(split_list, [None])) \n",
        "    res = list(test_list[i : j] for i, j in temp) \n",
        "    #print(res)\n",
        "    final_res = []\n",
        "    for l in res:\n",
        "        if(l[0] in spaces):\n",
        "            final_res.append([l[0]])\n",
        "            final_res.append(l[1:])\n",
        "        else: \n",
        "            final_res.append(l)\n",
        "    #print(final_res)\n",
        "    return final_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PKfn-sCe_4k"
      },
      "outputs": [],
      "source": [
        "def get_words_wo_space(test_list, split_list,spaces):\n",
        "    temp = zip(chain([0], split_list), chain(split_list, [None])) \n",
        "    res = list(test_list[i : j] for i, j in temp) \n",
        "    #print(res)\n",
        "    final_res = []\n",
        "    for l in res:\n",
        "        if(l[0] in spaces):\n",
        "            final_res.append([l[0]])\n",
        "            final_res.append(l[1:])\n",
        "        else: \n",
        "            final_res.append(l)\n",
        "    #print(final_res)\n",
        "    return final_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzihbJkne_4k"
      },
      "source": [
        "##### Generate **|*Input.Gradient*|** Attributution at word-level "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjodU-CVe_4k"
      },
      "outputs": [],
      "source": [
        "#for file in trans_dict[\"I'm going to them.\"]:\n",
        "def inp_grad_word(file, word_idx):\n",
        "    try:\n",
        "    #file = 'common_voice_en_110121'\n",
        "    #print(file)\n",
        "#         Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "#         display(Audio(wav, rate=Fs))\n",
        "        with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "            file_attr = pickle.load(j)\n",
        "#         print(file_attr['output'], file_meta[file]['accent'])\n",
        "\n",
        "    #         print(file_attr['output'].split(' '))\n",
        "        normalized_attr = get_norm_attr(file_attr['attr dict'])\n",
        "        keys = list(normalized_attr.keys())\n",
        "#         print(keys)\n",
        "        spaces = get_space(file_attr['output'])\n",
        "        indices = [keys.index(x) for x in spaces]\n",
        "        words = get_words(keys,indices, spaces)\n",
        "        input_size = len(file_attr['attr dict'][list(file_attr['attr dict'].keys())[0]]) # calculate properly once\n",
        "        buffer = 10\n",
        "        plot_vertical = False\n",
        "        #word_idx = 0\n",
        "\n",
        "        word_activation = np.sum(np.asarray([np.abs(normalized_attr[idx]) for idx in words[word_idx]] ), axis = 0)\n",
        "\n",
        "        if(plot_vertical):\n",
        "            fig = plt.figure(figsize = (1,35))\n",
        "            print(get_frame_allignment(file, input_size))\n",
        "            allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "            allignments = np.asarray(allignments)            \n",
        "            actual_size = len(allignments)\n",
        "            sns.heatmap(np.expand_dims(normalized_attr[1][:actual_size], axis = 1),annot = np.expand_dims(allignments, axis = 1), fmt=\"\", cmap='RdBu')\n",
        "\n",
        "            plt.show()\n",
        "        else:\n",
        "            str_list = []\n",
        "            for x in words[word_idx]:\n",
        "                str_list.append(file_attr['output'][x])\n",
        "\n",
        "\n",
        "            print(''.join(str_list))\n",
        "            fig = plt.figure(figsize = (37,1))\n",
        "            allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "            allignments = np.asarray(allignments)            \n",
        "            actual_size = len(allignments)\n",
        "            #actual_size = len(allignments)\n",
        "\n",
        "            sns.heatmap(np.expand_dims(word_activation[p_start - buffer:p_end + buffer]/np.sum(word_activation[p_start - buffer:p_end + buffer]), axis = 0), fmt=\"\", cmap='Blues')\n",
        "            #plt.yticks(rotation=) [p_start - buffer:p_end + buffer]\n",
        "            modified_allignments = allignments[p_start - buffer:p_end + buffer]\n",
        "            phone_labels = [x[0] for x in groupby(modified_allignments)]\n",
        "            len_list = [len(list(x[1])) for x in groupby(modified_allignments)]\n",
        "            idx_list = [0]\n",
        "            ticks_list = []\n",
        "            for l in len_list:\n",
        "                ticks_list.append(idx_list[-1] + l//2)\n",
        "                idx_list.append(idx_list[-1] + l)\n",
        "\n",
        "\n",
        "            plt.xticks(ticks_list,phone_labels, rotation = 90)\n",
        "\n",
        "            for j in idx_list:\n",
        "                plt.axvline(x=j, color='w', linestyle='-', linewidth=2.5)\n",
        "                plt.axvline(x=j, color='k', linestyle='--', linewidth=2.5)\n",
        "\n",
        "\n",
        "            plt.show()\n",
        "    \n",
        "    except:\n",
        "        print('failed for file : {}'.format(file))\n",
        "#         continue\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1G0Z49pe_4l"
      },
      "source": [
        "##### Helper functions to compare WERs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVJP_DtJe_4l"
      },
      "outputs": [],
      "source": [
        "def wer(s1, s2):\n",
        "        \"\"\"\n",
        "        Computes the Word Error Rate, defined as the edit distance between the\n",
        "        two provided sentences after tokenizing to words.\n",
        "        Arguments:\n",
        "            s1 (string): space-separated sentence\n",
        "            s2 (string): space-separated sentence\n",
        "        \"\"\"\n",
        "\n",
        "        # build mapping of words to integers\n",
        "        b = set(s1.split() + s2.split())\n",
        "        word2char = dict(zip(b, range(len(b))))\n",
        "\n",
        "        # map the words to a char array (Levenshtein packages only accepts\n",
        "        # strings)\n",
        "        w1 = [chr(word2char[w]) for w in s1.split()]\n",
        "        w2 = [chr(word2char[w]) for w in s2.split()]\n",
        "\n",
        "        return Lev.distance(''.join(w1), ''.join(w2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFsZquNde_4l"
      },
      "outputs": [],
      "source": [
        "transcript_wer = {}\n",
        "#lm_wer = {}\n",
        "accent_lm_wer = {x: [] for x in ['us', 'indian','african','canada','australia','england','scotland']}\n",
        "accent_wer = {x: [] for x in ['us', 'indian','african','canada','australia','england','scotland']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRx4JrFNe_4m"
      },
      "outputs": [],
      "source": [
        "with open('wers.pickle', 'rb') as l:\n",
        "    lm_wer = pickle.load(l)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0OO4NZWe_4m"
      },
      "outputs": [],
      "source": [
        "for transcript in transcripts:\n",
        "    transcript_ = transcript.strip().upper()\n",
        "    wer_dict = {}\n",
        "\n",
        "    valid_punctuation = string.punctuation.replace(\"'\",\"\")\n",
        "    processed_transcript = transcript_.translate(str.maketrans({a:None for a in valid_punctuation }))\n",
        "    #print(processed_transcript)       \n",
        "    for file in trans_dict[transcript]:\n",
        "        try:\n",
        "            with open('attribution/{}.pickle'.format(file), 'rb') as j:\n",
        "                file_attr = pickle.load(j)\n",
        "            accent = file_meta[file]['accent']\n",
        "            \n",
        "            #print(accent)\n",
        "            output = file_attr['output'].replace('_', '')\n",
        "            #print(output)\n",
        "            num_tokens = len(processed_transcript.split())\n",
        "            wer_ = 100*wer(processed_transcript, output)/num_tokens\n",
        "            wer_dict[accent] = wer_\n",
        "            accent_wer[accent].append(wer_)\n",
        "            accent_lm_wer[accent].append(lm_wer[file]['wer'])\n",
        "            \n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "    \n",
        "    transcript_wer[transcript] = wer_dict\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhJyuCAOe_4m"
      },
      "outputs": [],
      "source": [
        "print('accent','|', 'greedy','|', 'lm rescored','|', 'num of files')\n",
        "for key in accent_wer.keys():\n",
        "    print(key,'|', np.asarray(accent_wer[key]).mean(),'|', np.asarray(accent_lm_wer[key]).mean(),'|', len(accent_wer[key]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2fWO9gKe_4m"
      },
      "outputs": [],
      "source": [
        "#get_colour = {'us':'k', 'indian':'g','african':'b','canada':'r','australia':'c','england':'m','scotland':'y'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR2odRIWe_4n"
      },
      "source": [
        "##### Generate **|*Gradient*|** Attribution at grapheme-level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4YKh9hfe_4n"
      },
      "outputs": [],
      "source": [
        "#for file in trans_dict[\"I'm going to them.\"]:\n",
        "def grad_grapheme(file, idx):\n",
        "    try:\n",
        "        #file = 'common_voice_en_110121'\n",
        "#         Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "#         display(Audio(wav, rate=Fs))\n",
        "        with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "            file_attr = pickle.load(j)\n",
        "#         print(file_attr['output'], file_meta[file]['accent'])\n",
        "#         print(list(file_attr['grad_dict'].keys()))\n",
        "        normalized_attr = get_norm_attr(file_attr['grad_dict'])\n",
        "        keys = list(normalized_attr.keys())\n",
        "        input_size = len(file_attr['grad_dict'][list(file_attr['grad_dict'].keys())[0]]) # calculate properly once\n",
        "        buffer = 10\n",
        "    #print(get_frame_allignment(file,input_size))\n",
        "    #     with np.printoptions(precision=4, suppress=True):\n",
        "    #         print(normalized_attr[1][:291])\n",
        "        plot_vertical = False\n",
        "        #idx = 0\n",
        "\n",
        "        if(plot_vertical):\n",
        "            fig = plt.figure(figsize = (1,35))\n",
        "            print(get_frame_allignment(file, input_size))\n",
        "            allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "            allignments = np.asarray(allignments)            \n",
        "            actual_size = len(allignments)\n",
        "            sns.heatmap(np.expand_dims(normalized_attr[1][:actual_size], axis = 1),annot = np.expand_dims(allignments, axis = 1), fmt=\"\", cmap='RdBu')\n",
        "            #plt.yticks(rotation=) \n",
        "            #plt.yticks(range(allignments.shape[0]),list(allignments))\n",
        "            plt.show()\n",
        "        else:\n",
        "\n",
        "            fig = plt.figure(figsize = (37,1))\n",
        "            #print(allignments)\n",
        "            allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "            allignments = np.asarray(allignments)            \n",
        "            actual_size = len(allignments)\n",
        "            #actual_size = len(allignments)\n",
        "\n",
        "            print('for',file_attr['output'][keys[idx]])\n",
        "            sns.heatmap(np.expand_dims(np.abs(normalized_attr[keys[idx]][p_start - buffer:p_end + buffer])/np.sum(np.abs(normalized_attr[keys[idx]][p_start - buffer:p_end + buffer])), axis = 0), fmt=\"\", cmap='Greens')\n",
        "            #plt.yticks(rotation=) \n",
        "            modified_allignments = allignments[p_start - buffer:p_end + buffer]\n",
        "            phone_labels = [x[0] for x in groupby(modified_allignments)]\n",
        "            len_list = [len(list(x[1])) for x in groupby(modified_allignments)]\n",
        "            idx_list = [0]\n",
        "            ticks_list = []\n",
        "            for l in len_list:\n",
        "                ticks_list.append(idx_list[-1] + l//2)\n",
        "                idx_list.append(idx_list[-1] + l)\n",
        "\n",
        "            #ticks_list.append((idx_list[-1] + len(modified_allignments))//2)   \n",
        "\n",
        "            plt.xticks(ticks_list,phone_labels, rotation = 90)\n",
        "\n",
        "            for j in idx_list:\n",
        "                plt.axvline(x=j, color='w', linestyle='-', linewidth=2.5)\n",
        "                plt.axvline(x=j, color='k', linestyle='--', linewidth=2.5)\n",
        "            plt.show()\n",
        "\n",
        "    #break\n",
        "    except:\n",
        "        print('failed for file : {}'.format(file))\n",
        "#         continue\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwvOJ44Me_4n"
      },
      "source": [
        "##### Generate **|*Gradient*|** Attribution at word-level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjFePI9-e_4n"
      },
      "outputs": [],
      "source": [
        "#for file in trans_dict[\"I'm going to them.\"]:\n",
        "def grad_word(file, word_idx):\n",
        "    #try:\n",
        "        #file = 'common_voice_en_110121'\n",
        "        #print(file)\n",
        "#         Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "#         display(Audio(wav, rate=Fs))\n",
        "    with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "        file_attr = pickle.load(j)\n",
        "#         print(file_attr['output'], file_meta[file]['accent'])\n",
        "\n",
        "#         print(file_attr['output'].split(' '))\n",
        "    normalized_attr = get_norm_attr(file_attr['grad_dict'])\n",
        "    keys = list(normalized_attr.keys())\n",
        "#         print(keys)\n",
        "    spaces = get_space(file_attr['output'])\n",
        "    indices = [keys.index(x) for x in spaces]\n",
        "    words = get_words(keys,indices, spaces)\n",
        "    input_size = len(file_attr['grad_dict'][list(file_attr['grad_dict'].keys())[0]]) # calculate properly once\n",
        "    buffer = 10\n",
        "    plot_vertical = False\n",
        "    #word_idx = 8\n",
        "#         print(words)\n",
        "#         print(keys)\n",
        "\n",
        "    word_activation = np.sum(np.asarray([np.abs(normalized_attr[idx]) for idx in words[word_idx]] ), axis = 0)\n",
        "\n",
        "    if(plot_vertical):\n",
        "        fig = plt.figure(figsize = (1,35))\n",
        "        print(get_frame_allignment(file, input_size))\n",
        "        allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "        allignments = np.asarray(allignments)            \n",
        "        actual_size = len(allignments)\n",
        "        sns.heatmap(np.expand_dims(normalized_attr[1][:actual_size], axis = 1),annot = np.expand_dims(allignments, axis = 1), fmt=\"\", cmap='RdBu')\n",
        "\n",
        "        plt.show()\n",
        "    else:\n",
        "        str_list = []\n",
        "        for x in words[word_idx]:\n",
        "            str_list.append(file_attr['output'][x])\n",
        "\n",
        "\n",
        "        print('Focus word:',''.join(str_list))\n",
        "        fig = plt.figure(figsize = (37,1))\n",
        "        allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "        allignments = np.asarray(allignments)            \n",
        "        actual_size = len(allignments)\n",
        "        #actual_size = len(allignments)\n",
        "\n",
        "        sns.heatmap(np.expand_dims(word_activation[p_start - buffer:p_end + buffer]/np.sum(word_activation[p_start - buffer:p_end + buffer]), axis = 0), fmt=\"\", cmap='Greens')\n",
        "        #plt.yticks(rotation=) [p_start - buffer:p_end + buffer]\n",
        "        modified_allignments = allignments[p_start - buffer:p_end + buffer]\n",
        "        phone_labels = [x[0] for x in groupby(modified_allignments)]\n",
        "        len_list = [len(list(x[1])) for x in groupby(modified_allignments)]\n",
        "        idx_list = [0]\n",
        "        ticks_list = []\n",
        "        for l in len_list:\n",
        "            ticks_list.append(idx_list[-1] + l//2)\n",
        "            idx_list.append(idx_list[-1] + l)\n",
        "\n",
        "\n",
        "        plt.xticks(ticks_list,phone_labels, rotation = 90, fontsize=16)\n",
        "\n",
        "        for j in idx_list:\n",
        "            plt.axvline(x=j, color='w', linestyle='-', linewidth=2.5)\n",
        "            plt.axvline(x=j, color='k', linestyle='--', linewidth=2.5)\n",
        "\n",
        "\n",
        "        plt.show()\n",
        "#         fig2 = plt.figure(figsize = (37,1))\n",
        "#         my_arr = word_activation[p_start - buffer:p_end + buffer]/np.sum(word_activation[p_start - buffer:p_end + buffer])\n",
        "#         plt.plot(word_activation[p_start - buffer:p_end + buffer]/np.sum(word_activation[p_start - buffer:p_end + buffer]))\n",
        "#         xk = np.arange(len(my_arr))\n",
        "#         #pk = (0.1, 0.2, 0.3, 0.1, 0.1, 0.0, 0.2)        \n",
        "#         custm = st.rv_discrete(name='custm', values=(xk, my_arr))\n",
        "#         print(custm.mean(), custm.std())\n",
        "#         #print(my_arr.mean(), my_arr.std())\n",
        "#         #plt.xticks(ticks_list,phone_labels, rotation = 90)\n",
        "#         plt.xlim(xmin = 0, xmax = len(word_activation[p_start - buffer:p_end + buffer]/np.sum(word_activation[p_start - buffer:p_end + buffer])))\n",
        "        \n",
        "        \n",
        "#         plt.show()\n",
        "\n",
        "#     except:\n",
        "#         print('failed for file : {}'.format(file))\n",
        "        #continue\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0V84OOee_4o"
      },
      "outputs": [],
      "source": [
        "target_transcript = \"I'm going to them.\" # indian gowmedo\n",
        "target_transcript = 'The burning fire had been extinguished.' #across accents\n",
        "#target_transcript = 'It was the first time she had done that.' #SHE across accents, sheld in usm hi in indian\n",
        "#target_transcript = 'I was scared, but wasted no time in going out and crossing the bridge to the sand pits.'\n",
        "grapheme_idx = 0\n",
        "word_idx = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0toiP95Le_4o"
      },
      "source": [
        "### Visualizing Attributions for all accents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2bi_HtKe_4o"
      },
      "source": [
        "#### (A) Grapheme-level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wf0Jsfhe_4p"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (37,20))\n",
        "for file in trans_dict[target_transcript]:\n",
        "    try:\n",
        "        Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "        display(Audio(wav, rate=Fs))\n",
        "        with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "            file_attr = pickle.load(j)\n",
        "        print(file_attr['output'], file_meta[file]['accent'])\n",
        "        #print(list(file_attr['attr dict'].keys()))\n",
        "        inp_grad_grapheme(file, grapheme_idx)\n",
        "        grad_grapheme(file,grapheme_idx)\n",
        "        \n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATA4BlU4e_4p"
      },
      "source": [
        "#### (B) Word Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZy436Zwe_4p"
      },
      "outputs": [],
      "source": [
        "word_idx = 4\n",
        "for file in trans_dict[target_transcript]:\n",
        "    try:\n",
        "        Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "        display(Audio(wav, rate=Fs))\n",
        "        with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "            file_attr = pickle.load(j)\n",
        "        print(file_attr['output'], file_meta[file]['accent'])\n",
        "        #print(list(file_attr['attr dict'].keys()))\n",
        "        inp_grad_word(file, word_idx)\n",
        "        #print(file_meta[file]['accent'])\n",
        "        grad_word(file,word_idx)\n",
        "#     break\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY14JQfee_4q"
      },
      "outputs": [],
      "source": [
        "syllables = pickle.load(open('syll.pickle','rb'))\n",
        "syllables['THE']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lYA8rq2e_4q"
      },
      "outputs": [],
      "source": [
        "syllables['PEOPLE']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJgGLpc6e_4q"
      },
      "outputs": [],
      "source": [
        "def is_correct(s1, s2, idx):\n",
        "        \"\"\"\n",
        "        Computes the Word Error Rate, defined as the edit distance between the\n",
        "        two provided sentences after tokenizing to words.\n",
        "        Arguments:\n",
        "            s1 (string): space-separated sentence\n",
        "            s2 (string): space-separated sentence\n",
        "        \"\"\"\n",
        "\n",
        "        # build mapping of words to integers\n",
        "        b = set(s1.split() + s2.split())\n",
        "        word2char = dict(zip(b, range(len(b))))\n",
        "\n",
        "        # map the words to a char array (Levenshtein packages only accepts\n",
        "        # strings)\n",
        "        w1 = [chr(word2char[w]) for w in s1.split()]\n",
        "        w2 = [chr(word2char[w]) for w in s2.split()]\n",
        "        ops = Lev.editops(''.join(w1), ''.join(w2))\n",
        "        #print(ops)\n",
        "        words_changed = [x[1] for x in ops]\n",
        "        \n",
        "        return not idx in words_changed\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOJRZHMZe_4q"
      },
      "outputs": [],
      "source": [
        "cond_transcripts = []\n",
        "for t in transcripts:\n",
        "    \n",
        "    order = sorted(transcript_wer[t], key=lambda k: transcript_wer[t][k])\n",
        "    #print(order)\n",
        "    if(order[0]== 'canada' or order[0] == 'us'):\n",
        "        cond_transcripts.append(t)\n",
        "    \n",
        " # do this after LM rescoring ?   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zz1lru7e_4r"
      },
      "outputs": [],
      "source": [
        "print(len(cond_transcripts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsGetOkre_4r"
      },
      "outputs": [],
      "source": [
        "print(cond_transcripts[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgmog01ce_4r"
      },
      "outputs": [],
      "source": [
        "all_words = []\n",
        "valid_punctuation = string.punctuation.replace(\"'\",\"\")\n",
        "for t in transcripts:\n",
        "    t_ = t.strip().upper()\n",
        "    t_ = t_.translate(str.maketrans({a:None for a in valid_punctuation }))\n",
        "    all_words.extend(t_.split())\n",
        "    #print(all_words)\n",
        "    #break\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kilNjBjke_4r"
      },
      "source": [
        "##### Calculate Most Frequent Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPSJ1dRDe_4r"
      },
      "outputs": [],
      "source": [
        "allWordDist = nltk.FreqDist(all_words)\n",
        "record_frequency = allWordDist.most_common(75)\n",
        "most_frequent = [ x[0] for x in record_frequency]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWECMNFye_4s"
      },
      "outputs": [],
      "source": [
        "print(most_frequent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb2W8LV5e_4s"
      },
      "outputs": [],
      "source": [
        "most_frequent = ['THE', 'TO', 'AND', 'A', 'OF', 'WAS', 'I', 'IT', 'HE', 'THAT', 'IN', 'YOU', 'HAD', 'HIS', 'AS', 'BUT', 'WITH', 'BOY', 'IS', 'THEY', 'WERE', 'FOR', 'AT', 'ABOUT', 'BE', 'ON', 'ME', 'THERE', 'FROM', 'MY', 'WE', 'HIM', 'HAVE', 'NOT', 'OUT', 'THIS', 'SOME', 'ALL', 'THOUGHT', 'AN', 'PEOPLE', 'BEEN', 'HER', 'INTO', 'TIME', 'YOUR', 'SO', 'ARE', 'HERE', 'CAN', 'GET', 'THEN', 'WAY', 'SHE', 'ONE', 'WHEN', 'ONLY', \"DON'T\", \"I'M\", 'OTHER', 'UP', 'WHAT', 'SEE', 'COULD', 'LITTLE', 'NO', 'GOING', 'DO', 'WILL', 'IF', 'ITS', 'MORE', 'BY', 'MAN', 'STILL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWuLKNZje_4s"
      },
      "outputs": [],
      "source": [
        "mf_us = {x:[] for x in most_frequent}\n",
        "mf_canada = {x:[] for x in most_frequent}\n",
        "mf_indian = {x:[] for x in most_frequent}\n",
        "mf_african = {x:[] for x in most_frequent}\n",
        "mf_england = {x:[] for x in most_frequent}\n",
        "mf_scotland = {x:[] for x in most_frequent}\n",
        "mf_australian = {x:[] for x in most_frequent}\n",
        "mf_accents = {'us':mf_us,'canada':mf_canada,'indian':mf_indian,'african':mf_african,'england':mf_england,'scotland':mf_scotland,'australia':mf_australian}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arUob689e_4s"
      },
      "outputs": [],
      "source": [
        "#transcript_freq = {}\n",
        "for t in transcripts:\n",
        "    \n",
        "    t_ = t.strip().upper()\n",
        "    t_ = t_.translate(str.maketrans({a:None for a in valid_punctuation }))\n",
        "    \n",
        "    t_list = t_.split()\n",
        "    true_nz_counts = {x:t_list.count(x) for x in most_frequent if t_list.count(x) > 0}\n",
        "    true_nz_indices = {x:[index for index, value in enumerate(t_list) if value == x] for x in true_nz_counts.keys()}\n",
        "    #print(true_nz_counts)\n",
        "    for file in trans_dict[t]:\n",
        "        try:\n",
        "            acc = {x:0 for x in true_nz_counts.keys()}\n",
        "            with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "                file_attr = pickle.load(j)\n",
        "            op = file_attr['output'].replace('_', '')\n",
        "            accent = file_meta[file]['accent']\n",
        "            \n",
        "            \n",
        "            for w in true_nz_counts.keys():\n",
        "                \n",
        "                for j in true_nz_indices[w]:\n",
        "                #mf_accents[accent][w][0].append(true_nz_counts[w])\n",
        "                    \n",
        "                    mf_accents[accent][w].append(is_correct(t_,op,j))\n",
        "                #print(cond)\n",
        "           \n",
        "                #mf_accents[accent][w][1].append(min(op.split().count(w),true_nz_counts[w]))\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "        #print(mf_accents[accent])\n",
        "    \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5eFu_-5e_4s"
      },
      "outputs": [],
      "source": [
        "#print(mf_us)\n",
        "mf_us = {x:[] for x in most_frequent}\n",
        "mf_canada = {x:[] for x in most_frequent}\n",
        "mf_indian = {x:[] for x in most_frequent}\n",
        "mf_african = {x:[] for x in most_frequent}\n",
        "mf_england = {x:[] for x in most_frequent}\n",
        "mf_scotland = {x:[] for x in most_frequent}\n",
        "mf_australian = {x:[] for x in most_frequent}\n",
        "avg_stats =  {'us':mf_us,'canada':mf_canada,'indian':mf_indian,'african':mf_african,'england':mf_england,'scotland':mf_scotland,'australia':mf_australian}\n",
        "#print(avg_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msnGZI0Ge_4s"
      },
      "outputs": [],
      "source": [
        "for a in mf_accents.keys():\n",
        "    print(a)\n",
        "    #a = 'indian'\n",
        "    for w in mf_accents[a].keys():\n",
        "        \n",
        "        avg_stats[a][w].append(sum(mf_accents[a][w])/len(mf_accents[a][w]))\n",
        "        \n",
        "        #print(w,np.asarray(mf_accents[a][w]).mean(),np.asarray(mf_accents[a][w]).std())\n",
        "    \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOjXy3E2e_4t"
      },
      "source": [
        "##### Accuracy of correctly predicting most frequent words across accents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6I_oxJ7e_4t"
      },
      "outputs": [],
      "source": [
        "for a in avg_stats.keys():\n",
        "    temp = []\n",
        "    for k in avg_stats[a].keys():\n",
        "        temp.append(avg_stats[a][k][0])\n",
        "    print(a, np.asarray(temp).mean(), np.asarray(temp).std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11RviiYXe_4t"
      },
      "outputs": [],
      "source": [
        "us = [0.7803950509449439, 0.121667382968697]\n",
        "canada =[0.8572825991328428, 0.10528083986888306]\n",
        "indian =[0.513035145387385, 0.18122269227720902]\n",
        "african =[0.7522708227207352, 0.1365991362807828]\n",
        "england =[0.6919775766172566, 0.16313311778908576]\n",
        "scotland =[0.6535836230311115, 0.1547649386757295]\n",
        "australia =[0.7459351700052215, 0.14893722495703424]\n",
        "acc = {'us':us,'indian':indian,'canada':canada,'african':african,'england':england,'scotland':scotland,'australia':australia}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUVXo4Kje_4t"
      },
      "outputs": [],
      "source": [
        "objects = ['us', 'indian', 'african', 'canada', 'england', 'australia', 'scotland']\n",
        "y_pos = np.arange(len(objects))\n",
        "#fig = plt.figure(figsize = (15,10))\n",
        "# def create_plots(layer, name):\n",
        "fig = plt.figure(figsize = (10,5))\n",
        "y_pos = np.arange(len(objects))\n",
        "y = [100*acc[x][0] for x in objects ]\n",
        "err = [100*acc[x][1]  for  x in objects]\n",
        "#plt.plot(y_pos, y,'-o', alpha=0.7,)\n",
        "#plt.figure()\n",
        "plt.bar(y_pos, y, yerr= err,align='center', capsize=7, edgecolor='k')\n",
        "plt.xticks(y_pos, objects, fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "# plt.errorbar(y_pos, y, yerr=err,fmt='-o',capsize=5 )\n",
        "# plt.xticks(y_pos, objects)\n",
        "#plt.ylim(ymin = 0)obj\n",
        "#plt.axhline(14.28, linewidth=1, color='k')\n",
        "plt.ylabel('Accuracy %',color='k',fontsize=18)\n",
        "plt.xlabel('Accents',color='k',fontsize=18)\n",
        "#plt.title('MF words classificant trends')\n",
        "#plt.legend(frameon=False)\n",
        "plt.savefig('MF-words.pdf',bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8mv2Ghue_4u"
      },
      "source": [
        "##### Helper Function for alligning words with frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D9IDCK0e_4u"
      },
      "outputs": [],
      "source": [
        "def get_word_allignment(file, input_size):\n",
        "    alligned = []\n",
        "    spec_stride = 0.01\n",
        "    window_size = 0.02\n",
        "    times = file_meta[file]['end_times']\n",
        "    json_path = 'my_data/align_common/{}.json'.format(file)\n",
        "    with open(json_path,'r') as j:\n",
        "        gentle = json.load(j)\n",
        "    word_ends = []\n",
        "    word_ends.append(('start',times[0]))\n",
        "    for g in range(len(gentle['words'])):\n",
        "        word_ends.append((gentle['words'][g]['word'], gentle['words'][g]['end']))\n",
        "    word_ends.append(('end',times[-1]))\n",
        "    \n",
        "    #last_idx = 0\n",
        "    for i in range(input_size):\n",
        "        frame_idx = i\n",
        "        window_start = frame_idx*spec_stride\n",
        "        window_mid = window_start + (window_size/2)\n",
        "        alligned_word = 'na'\n",
        "        for j in range(len(word_ends)):\n",
        "            if (window_mid < word_ends[j][1]):\n",
        "                alligned_word = word_ends[j][0]\n",
        "                break\n",
        "        #assert alligned_phone != 'na', \"Failed to fetch allignment\"\n",
        "        if(alligned_word != 'na'):\n",
        "            alligned.append(alligned_word)\n",
        "            #last_idx = i\n",
        "    pause_start = 0\n",
        "    pause_end = len(alligned)\n",
        "    for i in range(len(alligned)):\n",
        "        if(alligned[i] != 'start'):\n",
        "            break\n",
        "        pause_start = i\n",
        "    \n",
        "    for i in range(len(alligned)-1,-1,-1):\n",
        "        if(alligned[i] != 'end'):\n",
        "            break\n",
        "        pause_end = i\n",
        "        \n",
        "    #print(last_idx)\n",
        "    \n",
        "    return alligned, pause_start+1, pause_end,\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0pCo3I8e_4u"
      },
      "outputs": [],
      "source": [
        "wrd = get_word_allignment('common_voice_en_540956', 100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wJpOzZ4e_4u"
      },
      "source": [
        "### Visualizing attribution of a particular word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXL3yQu9e_4u"
      },
      "outputs": [],
      "source": [
        "def grad_clubbed_word(file, word_idx, disable_prints = False):\n",
        "    \n",
        "    #try:\n",
        "        #file = 'common_voice_en_110121'\n",
        "        #print(file)\n",
        "#         Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "#         display(Audio(wav, rate=Fs))\n",
        "    with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "        file_attr = pickle.load(j)\n",
        "#         print(file_attr['output'], file_meta[file]['accent'])\n",
        "\n",
        "#         print(file_attr['output'].split(' '))\n",
        "    normalized_attr = get_norm_attr(file_attr['grad_dict'])\n",
        "    keys = list(normalized_attr.keys())\n",
        "#         print(keys)\n",
        "    spaces = get_space(file_attr['output'])\n",
        "    indices = [keys.index(x) for x in spaces]\n",
        "    words = get_words(keys,indices, spaces)\n",
        "    input_size = len(file_attr['grad_dict'][list(file_attr['grad_dict'].keys())[0]]) # calculate properly once\n",
        "    buffer = 10\n",
        "    plot_vertical = False\n",
        "    #word_idx = 8\n",
        "#         print(words)\n",
        "#         print(keys)\n",
        "\n",
        "    word_activation = np.sum(np.asarray([np.abs(normalized_attr[idx]) for idx in words[word_idx]] ), axis = 0)\n",
        "    \n",
        "\n",
        "    if(plot_vertical):\n",
        "        fig = plt.figure(figsize = (1,35))\n",
        "        print(get_frame_allignment(file, input_size))\n",
        "        allignments, p_start, p_end = get_word_allignment(file, input_size)\n",
        "        allignments = np.asarray(allignments)            \n",
        "        actual_size = len(allignments)\n",
        "        sns.heatmap(np.expand_dims(normalized_attr[1][:actual_size], axis = 1),annot = np.expand_dims(allignments, axis = 1), fmt=\"\", cmap='RdBu')\n",
        "\n",
        "        plt.show()\n",
        "    else:\n",
        "        str_list = []\n",
        "        for x in words[word_idx]:\n",
        "            str_list.append(file_attr['output'][x])\n",
        "\n",
        "\n",
        "        \n",
        "        allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "        wrd_allignments,w_start,w_end = get_word_allignment(file, input_size)\n",
        "        wrd_allignments = np.asarray(wrd_allignments)\n",
        "        allignments = np.asarray(allignments)            \n",
        "        actual_size = len(allignments)\n",
        "        #actual_size = len(allignments)\n",
        "        \n",
        "        #plt.yticks(rotation=) [p_start - buffer:p_end + buffer]\n",
        "        modified_allignments = allignments[p_start - buffer:p_end + buffer]\n",
        "        assert len(wrd_allignments) == len(allignments), \"dimensions don't match\"\n",
        "        \n",
        "        modified_wrd_allignments = wrd_allignments[w_start - buffer:w_end + buffer]\n",
        "        #assert len(modified_wrd_allignments) == len(modified_allignments), \"dimensions don't match\"\n",
        "        #wrds = [list(x[0]) for x in groupby(modified_wrd_allignments)]\n",
        "        wrd_labels = [list(x[1]) for x in groupby(modified_wrd_allignments)]\n",
        "        #print(wrd_labels)\n",
        "        wrd_indices = [0]\n",
        "        for j in wrd_labels:\n",
        "            wrd_indices.append(wrd_indices[-1] + len(j))\n",
        "        #print(wrd_indices)\n",
        "        my_arr = word_activation[w_start - buffer:w_end + buffer]/np.sum(word_activation[w_start - buffer:w_end + buffer])\n",
        "        wrd_combined_wonorm = []\n",
        "        wrd_combined = []\n",
        "        for m in range(len(wrd_labels)):\n",
        "            wrd_combined_wonorm.append(np.sum(my_arr[wrd_indices[m]:wrd_indices[m+1]]))\n",
        "            wrd_combined.append(np.sum(my_arr[wrd_indices[m]:wrd_indices[m+1]])/len(wrd_labels[m]))\n",
        "        wrd_combined = np.asarray(wrd_combined)\n",
        "        \n",
        "        wrd_combined = wrd_combined/np.sum(wrd_combined)\n",
        "        \n",
        "        #print(np.asarray(wrd_combined).sum())\n",
        "        if(not disable_prints):\n",
        "            print(''.join(str_list))\n",
        "            fig = plt.figure(figsize = (37,1))\n",
        "            sns.heatmap(np.expand_dims(word_activation[p_start - buffer:p_end + buffer]/np.sum(word_activation[p_start - buffer:p_end + buffer]), axis = 0), fmt=\"\", cmap='Greens')\n",
        "            phone_labels = [x[0] for x in groupby(modified_allignments)]\n",
        "            len_list = [len(list(x[1])) for x in groupby(modified_allignments)]\n",
        "            idx_list = [0]\n",
        "            ticks_list = []\n",
        "            for l in len_list:\n",
        "                ticks_list.append(idx_list[-1] + l//2)\n",
        "                idx_list.append(idx_list[-1] + l)\n",
        "\n",
        "\n",
        "            plt.xticks(ticks_list,phone_labels, rotation = 90)\n",
        "\n",
        "            for j in idx_list:\n",
        "                plt.axvline(x=j, color='w', linestyle='-', linewidth=2.5)\n",
        "                plt.axvline(x=j, color='k', linestyle='--', linewidth=2.5)\n",
        "\n",
        "\n",
        "            plt.show()\n",
        "            print(wrd_combined_wonorm)\n",
        "            print(wrd_combined)  \n",
        "            fig2 = plt.figure(figsize = (37,1))\n",
        "            #my_arr = word_activation[w_start - buffer:w_end + buffer]/np.sum(word_activation[w_start - buffer:w_end + buffer])\n",
        "            plt.plot(word_activation[p_start - buffer:p_end + buffer]/np.sum(word_activation[p_start - buffer:p_end + buffer]))\n",
        "            xk = np.arange(len(my_arr))\n",
        "            #pk = (0.1, 0.2, 0.3, 0.1, 0.1, 0.0, 0.2)        \n",
        "            custm = st.rv_discrete(name='custm', values=(xk, my_arr))\n",
        "            print(custm.mean(), custm.std())\n",
        "              \n",
        "            #print(my_arr.mean(), my_arr.std())\n",
        "            #plt.xticks(ticks_list,phone_labels, rotation = 90)\n",
        "            plt.xlim(xmin = 0, xmax = len(word_activation[p_start - buffer:p_end + buffer]/np.sum(word_activation[p_start - buffer:p_end + buffer])))\n",
        "\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            fig3 = plt.figure(figsize = (37,1))\n",
        "            my_arr = wrd_combined\n",
        "            plt.plot(my_arr)\n",
        "            xk = np.arange(len(my_arr))\n",
        "            #pk = (0.1, 0.2, 0.3, 0.1, 0.1, 0.0, 0.2)        \n",
        "            custm = st.rv_discrete(name='custm', values=(xk, my_arr))\n",
        "            print(custm.mean(), custm.std())\n",
        "            #print(my_arr.mean(), my_arr.std())\n",
        "            #plt.xticks(ticks_list,phone_labels, rotation = 90)\n",
        "            plt.xlim(xmin = 0, xmax = len(my_arr))\n",
        "\n",
        "\n",
        "            plt.show()\n",
        "        #else:\n",
        "        return wrd_combined, wrd_combined_wonorm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3OyPMHEe_4v"
      },
      "source": [
        "##### Visualizing normalized attributions at granularity of frames and words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QLbVdlae_4v"
      },
      "outputs": [],
      "source": [
        "#target_transcript = 'I was scared, but wasted no time in going out and crossing the bridge to the sand pits.'\n",
        "\n",
        "for file in trans_dict[target_transcript]:\n",
        "    Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "    display(Audio(wav, rate=Fs))\n",
        "    with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "        file_attr = pickle.load(j)\n",
        "    print(file_attr['output'], file_meta[file]['accent'])\n",
        "    print(list(file_attr['attr dict'].keys()))\n",
        "    #inp_grad_grapheme(file, grapheme_idx)\n",
        "    print(file)\n",
        "    distr, distr_wo = grad_clubbed_word(file, 2*idx, disable_prints=False)\n",
        "                    #print(distr)\n",
        "    max_idx = np.argmax(np.asarray(distr))\n",
        "    max_idx_wo = np.argmax(np.asarray(distr_wo))\n",
        "    custm = st.rv_discrete(name='custm', values=(np.arange(len(distr)), distr))\n",
        "    spread = custm.expect(lambda x : (x- custm.mean())**2)\n",
        "    print(max_idx,spread**0.5)\n",
        "          \n",
        "    #break\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocx3rx6Ve_4v"
      },
      "source": [
        "#### Analysis of attributions summed at word level (from transcription)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi3sq2rZe_4v"
      },
      "outputs": [],
      "source": [
        "mf_us = {x:([],[],[]) for x in most_frequent}\n",
        "mf_canada = {x:([],[],[]) for x in most_frequent}\n",
        "mf_indian = {x:([],[],[]) for x in most_frequent}\n",
        "mf_african = {x:([],[],[]) for x in most_frequent}\n",
        "mf_england = {x:([],[],[]) for x in most_frequent}\n",
        "mf_scotland = {x:([],[],[]) for x in most_frequent}\n",
        "mf_australian = {x:([],[],[]) for x in most_frequent}\n",
        "max_accents = {'us':mf_us,'canada':mf_canada,'indian':mf_indian,'african':mf_african,'england':mf_england,'scotland':mf_scotland,'australia':mf_australian}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDCFysoYe_4w"
      },
      "outputs": [],
      "source": [
        "\n",
        "for t in transcripts:\n",
        "    \n",
        "    t_ = t.strip().upper()\n",
        "    t_ = t_.translate(str.maketrans({a:None for a in valid_punctuation }))\n",
        "    #print(t_)\n",
        "    t_list = t_.split()\n",
        "    true_nz_counts = {x:t_list.count(x) for x in most_frequent if t_list.count(x) > 0}\n",
        "    true_nz_indices = {x:[index for index, value in enumerate(t_list) if value == x] for x in true_nz_counts.keys()}\n",
        "    \n",
        "    for file in trans_dict[t]:\n",
        "        \n",
        "        try:   \n",
        "            with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "                file_attr = pickle.load(j)\n",
        "            op = file_attr['output'].replace('_', '')\n",
        "            op_list = op.split()\n",
        "            accent = file_meta[file]['accent']\n",
        "\n",
        "            a_indices = {x:[index for index, value in enumerate(op_list) if value == x] for x in true_nz_counts.keys()}\n",
        "        \n",
        "            for w in true_nz_counts.keys():\n",
        "                for idx in a_indices[w]:\n",
        "                    # compute condition for correctness\n",
        "                    distr, distr_wo = grad_clubbed_word(file, 2*idx, disable_prints=True)\n",
        "                    #print(distr)\n",
        "                    max_idx = np.argmax(np.asarray(distr))\n",
        "                    max_idx_wo = np.argmax(np.asarray(distr_wo))\n",
        "                    \n",
        "            #print(custm.mean(), custm.std())\n",
        "                    if(is_correct(op,t_,idx)):\n",
        "                        custm = st.rv_discrete(name='custm', values=(np.arange(len(distr)), distr))\n",
        "                        spread = custm.expect(lambda x : (x - max_idx)**2)\n",
        "                        #print(spread)\n",
        "                        max_accents[accent][w][2].append(spread**0.5)\n",
        "                        \n",
        "                        idx2 = true_nz_indices[w][a_indices[w].index(idx)]\n",
        "                        max_accents[accent][w][0].append(max_idx == idx2 + 1)\n",
        "                        max_accents[accent][w][1].append(max_idx_wo == idx2 + 1)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "            \n",
        "        #print(mf_accents[accent])\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "835lvQ6ze_4w"
      },
      "source": [
        "##### Accuracy of how often the word alligned from meta data has the highest cumulative attribtuion for words from transcription (given that word is transcribed correctly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMDrjJ9Se_4w"
      },
      "outputs": [],
      "source": [
        "for a in max_accents.keys():\n",
        "    print(a)\n",
        "    acc = ([],[],[])\n",
        "    #a = 'indian'\n",
        "    for w in mf_accents[a].keys():\n",
        "        acc[0].append(sum(max_accents[a][w][0])/len(max_accents[a][w][0]))\n",
        "        acc[1].append(sum(max_accents[a][w][1])/len(max_accents[a][w][1]))\n",
        "        #acc[2].append(np.asarray(max_accents[a][w][2]).mean())\n",
        "    print(np.asarray(acc[0]).mean(),np.asarray(acc[1]).mean(),np.asarray(acc[2]).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBlU0u-me_4w"
      },
      "outputs": [],
      "source": [
        "# us\n",
        "# 0.822280626362392 0.7107428299869577\n",
        "# canada\n",
        "# 0.8579726753622985 0.7690201257461524\n",
        "# indian\n",
        "# 0.7958703395891465 0.6934186392183957\n",
        "# african\n",
        "# 0.8308889379878893 0.7204420312446458\n",
        "# england\n",
        "# 0.7781214935118137 0.7200572594990948\n",
        "# scotland\n",
        "# 0.8124665953005906 0.7094160821071285\n",
        "# australia\n",
        "# 0.7892047076340786 0.7292303868344401"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys9H3i_2e_4x"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(predictions, targets,N):\n",
        "    \"\"\"\n",
        "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
        "    and predictions. \n",
        "    Input: predictions (N, k) ndarray\n",
        "           targets (N, k) ndarray        \n",
        "    Returns: scalar\n",
        "    \"\"\"\n",
        "    #predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    #N = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n",
        "    return ce\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3Jj7cGye_4x"
      },
      "source": [
        "### EMD Calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfqNSdt8e_4x"
      },
      "source": [
        "##### EMD & entropy wrt baselines at word level. Wd1 and e1 correspond to emd and entropy between the segment of at attribution of frame corresponding to the word and a uniformly distributed baseline for the duration of the word respectively. Wd2 and e2 correspond to emd and entropy between the attribution of the entire transcription and a uniformly distributed baseline for the duration of the word and zero every where else respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQUqp0Bee_4x"
      },
      "outputs": [],
      "source": [
        "def grad_word_dist(file, word_idx, actual_idx, taper = False):\n",
        "    \n",
        "    #try:\n",
        "        #file = 'common_voice_en_110121'\n",
        "        #print(file)\n",
        "#         Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "#         display(Audio(wav, rate=Fs))\n",
        "    with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "        file_attr = pickle.load(j)\n",
        "#         print(file_attr['output'], file_meta[file]['accent'])\n",
        "\n",
        "#         print(file_attr['output'].split(' '))\n",
        "    normalized_attr = get_norm_attr(file_attr['grad_dict'])\n",
        "    keys = list(normalized_attr.keys())\n",
        "#         print(keys)\n",
        "    spaces = get_space(file_attr['output'])\n",
        "    indices = [keys.index(x) for x in spaces]\n",
        "    words = get_words(keys,indices, spaces)\n",
        "    input_size = len(file_attr['grad_dict'][list(file_attr['grad_dict'].keys())[0]]) # calculate properly once\n",
        "    buffer = 10\n",
        "    plot_vertical = False\n",
        "    #word_idx = 8\n",
        "#         print(words)\n",
        "#         print(keys)\n",
        "\n",
        "    word_activation = np.sum(np.asarray([np.abs(normalized_attr[idx]) for idx in words[word_idx]] ), axis = 0)\n",
        "    \n",
        "\n",
        "\n",
        "    str_list = []\n",
        "    for x in words[word_idx]:\n",
        "        str_list.append(file_attr['output'][x])\n",
        "\n",
        "\n",
        "    #print(''.join(str_list))\n",
        "    allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "    wrd_allignments,w_start,w_end = get_word_allignment(file, input_size)\n",
        "    wrd_allignments = np.asarray(wrd_allignments)\n",
        "    allignments = np.asarray(allignments)            \n",
        "    actual_size = len(allignments)\n",
        "    \n",
        "    modified_allignments = allignments[p_start - buffer:p_end + buffer]\n",
        "    assert len(wrd_allignments) == len(allignments), \"dimensions don't match\"\n",
        "    modified_allignments = allignments[w_start - buffer:w_end + buffer]\n",
        "    modified_wrd_allignments = wrd_allignments[w_start - buffer:w_end + buffer]\n",
        "    \n",
        "    wrd_labels = [list(x[1]) for x in groupby(modified_wrd_allignments)]\n",
        "#     print(wrd_labels)\n",
        "    wrd_indices = [0]\n",
        "    for j in wrd_labels:\n",
        "        wrd_indices.append(wrd_indices[-1] + len(j))\n",
        "#     print(wrd_indices)\n",
        "    my_arr = word_activation[w_start - buffer:w_end + buffer]/np.sum(word_activation[w_start - buffer:w_end + buffer])\n",
        "    m = actual_idx + 1\n",
        "#     print(m)\n",
        "    \n",
        "    assert len(my_arr) == len(modified_wrd_allignments), 'assumption failed'\n",
        "    word_frame = my_arr[wrd_indices[m]:wrd_indices[m+1]]\n",
        "    word_frame_norm = word_frame/np.sum(word_frame)\n",
        "    baseline_frame_ = np.ones(len(word_frame))/len(word_frame)\n",
        "    baseline_frame = np.array(signal.tukey(int(2*len(word_frame))))\n",
        "    if(not taper):\n",
        "        baseline_frame = baseline_frame_\n",
        "#     print('here')\n",
        "#     print(len(baseline_frame))\n",
        "#     print(len(baseline_wind[wrd_indices[m]:wrd_indices[m+1]]))\n",
        "\n",
        "    baseline_frame = baseline_frame / sum(baseline_frame)\n",
        "#     print(len(baseline_frame))\n",
        "#     print(wrd_indices[m+1] - wrd_indices[m])\n",
        "    count = len(set(modified_allignments[wrd_indices[m]:wrd_indices[m+1]]))#huersitic\n",
        "    (wd1,e1) = 100*wd(word_frame_norm,baseline_frame_)/count, cross_entropy(word_frame_norm,baseline_frame_,count)\n",
        "    baseline_wind = np.zeros(len(my_arr))\n",
        "    if(not taper):\n",
        "        baseline_wind[wrd_indices[m] :wrd_indices[m+1] ] = baseline_frame\n",
        "    else : \n",
        "        if(wrd_indices[m] - len(word_frame)//2 >= 0 and wrd_indices[m+1] + len(word_frame) - len(word_frame)//2 < len(my_arr)):\n",
        "            baseline_wind[wrd_indices[m] - len(word_frame)//2 :wrd_indices[m+1] + len(word_frame) - len(word_frame)//2 ] = baseline_frame\n",
        "        elif(wrd_indices[m] - len(word_frame)//2 < 0):\n",
        "            baseline_wind[0:wrd_indices[m+1] + len(word_frame) - len(word_frame)//2 ] = baseline_frame[-wrd_indices[m] + len(word_frame)//2]\n",
        "        else:\n",
        "            dist = len(my_arr) - (wrd_indices[m+1] + len(word_frame) - len(word_frame)//2)\n",
        "            baseline_wind[wrd_indices[m] - len(word_frame)//2 : len(my_arr) ] = baseline_frame[:len(word_frame) - dist]\n",
        "            \n",
        "            \n",
        "    #my_arr = my_arr/sum(my_arr)\n",
        "#     plt.plot(my_arr)\n",
        "# #     print(round(sum(my_arr),2))\n",
        "#     plt.plot(baseline_wind)\n",
        "# #     print(round(sum(baseline_wind),2))\n",
        "#     plt.show()\n",
        "    #print(wrd_indices)\n",
        "    \n",
        "    #\n",
        "    \n",
        "#     (wd2, e2) = 100*wd(my_arr,baseline_wind)/len(my_arr), cross_entropy(my_arr,baseline_wind,len(my_arr))\n",
        "    bins = np.arange(len(my_arr))\n",
        "    euc_dist = ed(bins.reshape(-1,1), bins.reshape(-1,1))\n",
        "    (wd2, e2) = 100*wd(my_arr,baseline_wind), emd(my_arr.astype(np.float64),baseline_wind.astype(np.float64), euc_dist.astype(np.float64))\n",
        "    return wd1, e1, wd2,e2\n",
        "    \n",
        "    \n",
        "    \n",
        "#     wrd_combined_wonorm = []\n",
        "#     wrd_combined = []\n",
        "#     for m in range(len(wrd_labels)):\n",
        "#         wrd_combined_wonorm.append(np.sum(my_arr[wrd_indices[m]:wrd_indices[m+1]]))\n",
        "#         wrd_combined.append(np.sum(my_arr[wrd_indices[m]:wrd_indices[m+1]])/len(wrd_labels[m]))\n",
        "#     wrd_combined = np.asarray(wrd_combined)\n",
        "\n",
        "#     wrd_combined = wrd_combined/np.sum(wrd_combined)\n",
        "#     print(''.join(str_list))\n",
        "\n",
        "    #return wrd_combined, wrd_combined_wonorm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLZn50hse_4y"
      },
      "source": [
        "##### EMD & entropy wrt baselines at syllable level. (wd1, e1), (wd2, e2) represent the same things as above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeYN16nJe_4y"
      },
      "outputs": [],
      "source": [
        "def grad_syll_dist(file, word_idx, actual_idx, syll_num, taper = False):\n",
        "    \n",
        "    #try:\n",
        "        #file = 'common_voice_en_110121'\n",
        "        #print(file)\n",
        "#         Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "#         display(Audio(wav, rate=Fs))\n",
        "    with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "        file_attr = pickle.load(j)\n",
        "#         print(file_attr['output'], file_meta[file]['accent'])\n",
        "\n",
        "#     print(file_attr['output'])\n",
        "    normalized_attr = get_norm_attr(file_attr['grad_dict'])\n",
        "    keys = list(normalized_attr.keys())\n",
        "    #print(keys)\n",
        "    sent = file_attr['output'].replace('_','').lower()\n",
        "    chunks = {}\n",
        "    for chunk in sent.split():\n",
        "        chunks[chunk.upper()] = hyphenate_word(chunk)\n",
        "        #print(syllables[chunk.upper()])\n",
        "    #chunks = [x.upper() for x in chunks]\n",
        "    #print(chunks)\n",
        "    mod_sent = ' '.join(chunks)\n",
        "    spaces = get_space(file_attr['output']) \n",
        "    #print(spaces)\n",
        "    indices = [keys.index(x) for x in spaces]\n",
        "    #print(indices)\n",
        "    words = get_words(keys,indices, spaces)\n",
        "    #print(words)\n",
        "    w_new = []\n",
        "    mult_syll = False\n",
        "    for w in words:\n",
        "        if(len(w) ==1 and w[0] in spaces):\n",
        "            w_new.append(w)\n",
        "            continue\n",
        "        #print(w)\n",
        "        can = ''.join([file_attr['output'][i] for i in w])\n",
        "        if(len(chunks[can]) == 1): w_new.append(w)\n",
        "        else:\n",
        "#             mult_syll = True\n",
        "            Inputt = iter(w)\n",
        "            length_to_split = [len(i) for i in chunks[can]]\n",
        "            Output = [list(islice(Inputt, elem)) for elem in length_to_split]\n",
        "            w_new.extend(Output)\n",
        "    #print(w_new)\n",
        "#     print(mult_syll)   \n",
        "    input_size = len(file_attr['grad_dict'][list(file_attr['grad_dict'].keys())[0]]) # calculate properly once\n",
        "    buffer = 10\n",
        "    plot_vertical = False\n",
        "    #word_idx = 8\n",
        "#         print(words)\n",
        "#         print(keys)\n",
        "    syll_idx = word_idx + syll_num\n",
        "    #print(w_new[syll_idx])\n",
        "    word_activation = np.sum(np.asarray([np.abs(normalized_attr[idx]) for idx in w_new[syll_idx]] ), axis = 0)\n",
        "    \n",
        "\n",
        "\n",
        "    str_list = []\n",
        "\n",
        "    for x in w_new[syll_idx]:\n",
        "        str_list.append(file_attr['output'][x])\n",
        "\n",
        "\n",
        "#     print(''.join(str_list))\n",
        "    allignments, p_start, p_end = get_frame_allignment(file, input_size)\n",
        "    wrd_allignments,w_start,w_end = get_word_allignment(file, input_size)\n",
        "    #print('here',allignments)\n",
        "    wrd_allignments = np.asarray(wrd_allignments)\n",
        "    allignments = np.asarray(allignments)            \n",
        "    actual_size = len(allignments)\n",
        "    \n",
        "    modified_allignments = allignments[p_start - buffer:p_end + buffer]\n",
        "    assert len(wrd_allignments) == len(allignments), \"dimensions don't match\"\n",
        "    modified_allignments = allignments[w_start - buffer:w_end + buffer]\n",
        "    modified_wrd_allignments = wrd_allignments[w_start - buffer:w_end + buffer]\n",
        "#     print(modified_allignments)\n",
        "#     print(modified_wrd_allignments)\n",
        "    \n",
        "    \n",
        "    wrd_labels = [list(x[1]) for x in groupby(modified_wrd_allignments)]\n",
        "#     print(wrd_labels)\n",
        "    wrd_indices = [0]\n",
        "    for j in wrd_labels:\n",
        "        wrd_indices.append(wrd_indices[-1] + len(j))\n",
        "#     print(wrd_indices)\n",
        "\n",
        "    my_arr = word_activation[w_start - buffer:w_end + buffer]/np.sum(word_activation[w_start - buffer:w_end + buffer])\n",
        "    m = actual_idx + 1\n",
        "#     print(m)\n",
        "#     print(modified_allignments[wrd_indices[m] :wrd_indices[m+1]])\n",
        "    \n",
        "    #target_word = wrd_labels[a]\n",
        "    #print()\n",
        "    #print(chunks[])\n",
        "    items = modified_allignments[wrd_indices[m] :wrd_indices[m+1]]\n",
        "    my_pron = list(OrderedDict.fromkeys(items))\n",
        "    my_pron = ' '.join(my_pron).upper()\n",
        "#     print(my_pron)\n",
        "    my_syll = syllables[wrd_labels[m][0]]\n",
        "    if(len(my_syll) > 1): mult_syll = True\n",
        "    flag = True\n",
        "    item_labels = [list(x[1]) for x in groupby(items)]\n",
        "    item_lens = [len(x) for x in item_labels]\n",
        "    for i in range(len(syllables[wrd_labels[m][0]])):\n",
        "        if (my_syll[i]['pron'] == my_pron):\n",
        "            flag = False\n",
        "            assert len(my_syll[i]['syll']) == len(hyphenate_word(wrd_labels[m][0])), 'syll-hyph failed'\n",
        "            phn_splits = []\n",
        "            for j in my_syll[i]['syll']:\n",
        "                j = [k for k in j if len(k)!=0 ]\n",
        "#                 print(j)\n",
        "                phn_splits.append(len(j))\n",
        "    Inputt = iter(item_lens)\n",
        "    length_to_split = phn_splits\n",
        "    Output = [list(islice(Inputt, elem)) for elem in length_to_split]\n",
        "    syll_indices = [0]\n",
        "    syll_indices.extend([sum(j) for j in Output])\n",
        "#     print(syll_indices)\n",
        "#     print(wrd_indices[m], wrd_indices[m+1])\n",
        "    assert not flag, 'no syllables'\n",
        "    \n",
        "#     print(Output)\n",
        "#     print(item_lens)    \n",
        "    assert len(my_arr) == len(modified_wrd_allignments), 'assumption failed'\n",
        "    word_frame = my_arr[wrd_indices[m]:wrd_indices[m+1]]\n",
        "    word_frame_norm = word_frame/np.sum(word_frame)\n",
        "    baseline_frame_ = np.ones(len(word_frame))/len(word_frame)\n",
        "    baseline_frame = np.array(signal.tukey(int(2*len(word_frame))))\n",
        "    if(not taper):\n",
        "        baseline_frame = baseline_frame_\n",
        "#     print('here')\n",
        "#     print(len(baseline_frame))\n",
        "#     print(len(baseline_wind[wrd_indices[m]:wrd_indices[m+1]]))\n",
        "\n",
        "    baseline_frame = baseline_frame / sum(baseline_frame)\n",
        "#     print(len(baseline_frame))\n",
        "#     print(wrd_indices[m+1] - wrd_indices[m])\n",
        "    count = len(set(modified_allignments[wrd_indices[m]:wrd_indices[m+1]]))#huersitic\n",
        "    (wd1,e1) = 100*wd(word_frame_norm,baseline_frame_)/count, cross_entropy(word_frame_norm,baseline_frame_,count)\n",
        "    baseline_wind = np.zeros(len(my_arr))\n",
        "    if(not taper):\n",
        "        if( not mult_syll):\n",
        "            baseline_wind[wrd_indices[m] :wrd_indices[m+1] ] = baseline_frame\n",
        "        else:\n",
        "            #print()\n",
        "            baseline_syll = np.ones(syll_indices[syll_num +1 ] - syll_indices[syll_num])\n",
        "            baseline_syll = baseline_syll/len(baseline_syll)\n",
        "            baseline_wind[wrd_indices[m] + syll_indices[syll_num] : wrd_indices[m] + syll_indices[syll_num + 1] ] = baseline_syll\n",
        "            \n",
        "            \n",
        "    else : \n",
        "        if(wrd_indices[m] - len(word_frame)//2 >= 0 and wrd_indices[m+1] + len(word_frame) - len(word_frame)//2 < len(my_arr)):\n",
        "            baseline_wind[wrd_indices[m] - len(word_frame)//2 :wrd_indices[m+1] + len(word_frame) - len(word_frame)//2 ] = baseline_frame\n",
        "        elif(wrd_indices[m] - len(word_frame)//2 < 0):\n",
        "            baseline_wind[0:wrd_indices[m+1] + len(word_frame) - len(word_frame)//2 ] = baseline_frame[-wrd_indices[m] + len(word_frame)//2]\n",
        "        else:\n",
        "            dist = len(my_arr) - (wrd_indices[m+1] + len(word_frame) - len(word_frame)//2)\n",
        "            baseline_wind[wrd_indices[m] - len(word_frame)//2 : len(my_arr) ] = baseline_frame[:len(word_frame) - dist]\n",
        "            \n",
        "            \n",
        "    #my_arr = my_arr/sum(my_arr)\n",
        "#     plt.plot(my_arr)\n",
        "# #     print(round(sum(my_arr),2))\n",
        "#     plt.plot(baseline_wind)\n",
        "# #     print(round(sum(baseline_wind),2))\n",
        "#     plt.show()\n",
        "    #print(wrd_indices)\n",
        "    \n",
        "    #\n",
        "    \n",
        "#     (wd2, e2) = 100*wd(my_arr,baseline_wind)/len(my_arr), cross_entropy(my_arr,baseline_wind,len(my_arr))\n",
        "    bins = np.arange(len(my_arr))\n",
        "    euc_dist = ed(bins.reshape(-1,1), bins.reshape(-1,1))\n",
        "    (wd2, e2) = 100*wd(my_arr,baseline_wind), emd(my_arr.astype(np.float64),baseline_wind.astype(np.float64), euc_dist.astype(np.float64))\n",
        "    return wd1, e1, wd2,e2\n",
        "    \n",
        "    \n",
        "    \n",
        "#     wrd_combined_wonorm = []\n",
        "#     wrd_combined = []\n",
        "#     for m in range(len(wrd_labels)):\n",
        "#         wrd_combined_wonorm.append(np.sum(my_arr[wrd_indices[m]:wrd_indices[m+1]]))\n",
        "#         wrd_combined.append(np.sum(my_arr[wrd_indices[m]:wrd_indices[m+1]])/len(wrd_labels[m]))\n",
        "#     wrd_combined = np.asarray(wrd_combined)\n",
        "\n",
        "#     wrd_combined = wrd_combined/np.sum(wrd_combined)\n",
        "#     print(''.join(str_list))\n",
        "\n",
        "    #return wrd_combined, wrd_combined_wonorm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJzvu3d3e_4y"
      },
      "outputs": [],
      "source": [
        "grad_syll_dist('common_voice_en_179645',2,1,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA0TM8qfe_4z"
      },
      "outputs": [],
      "source": [
        "\n",
        "mf_us = {x:([],[],[],[]) for x in most_frequent}\n",
        "mf_canada = {x:([],[],[],[]) for x in most_frequent}\n",
        "mf_indian = {x:([],[],[],[]) for x in most_frequent}\n",
        "mf_african = {x:([],[],[],[]) for x in most_frequent}\n",
        "mf_england = {x:([],[],[],[]) for x in most_frequent}\n",
        "mf_scotland = {x:([],[],[],[]) for x in most_frequent}\n",
        "mf_australian = {x:([],[],[],[]) for x in most_frequent}\n",
        "dist_accents = {'us':mf_us,'canada':mf_canada,'indian':mf_indian,'african':mf_african,'england':mf_england,'scotland':mf_scotland,'australia':mf_australian}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBQLgnP0e_4z"
      },
      "outputs": [],
      "source": [
        "# print(target_transcript)\n",
        "# for file in trans_dict[target_transcript]:\n",
        "#     Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "#     display(Audio(wav, rate=Fs))\n",
        "#     with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "#         file_attr = pickle.load(j)\n",
        "#     print(file_attr['output'], file_meta[file]['accent'])\n",
        "#     print(list(file_attr['attr dict'].keys()))\n",
        "#     #inp_grad_grapheme(file, grapheme_idx)\n",
        "#     print(file)\n",
        "#     print(grad_word_dist(file,2, 1))\n",
        "#     break\n",
        "valid_punctuation = string.punctuation.replace(\"'\",\"\")\n",
        "for t in transcripts:\n",
        "#     print(t)\n",
        "    \n",
        "    t_ = t.strip().upper()\n",
        "    t_ = t_.translate(str.maketrans({a:None for a in valid_punctuation }))\n",
        "    #print(t_)\n",
        "    t_list = t_.split()\n",
        "    true_nz_counts = {x:t_list.count(x) for x in most_frequent if t_list.count(x) > 0}\n",
        "    true_nz_indices = {x:[index for index, value in enumerate(t_list) if value == x] for x in true_nz_counts.keys()}\n",
        "    \n",
        "    for file in trans_dict[t]:\n",
        "        \n",
        "        try:   \n",
        "            with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "                file_attr = pickle.load(j)\n",
        "            op = file_attr['output'].replace('_', '')\n",
        "    #             print(op)\n",
        "            op_list = op.split()\n",
        "            accent = file_meta[file]['accent']\n",
        "            #print(accent,'------------------')\n",
        "            a_indices = {x:[index for index, value in enumerate(op_list) if value == x] for x in true_nz_counts.keys()}\n",
        "\n",
        "            for w in true_nz_counts.keys():\n",
        "                for idx in a_indices[w]:\n",
        "    #                     print('here')\n",
        "    #                     print(w)\n",
        "                    # compute condition for correctness\n",
        "                    distr, distr_wo = grad_clubbed_word(file, 2*idx, disable_prints=True)\n",
        "                    #print(distr)\n",
        "                    max_idx = np.argmax(np.asarray(distr))\n",
        "                    max_idx_wo = np.argmax(np.asarray(distr_wo))\n",
        "                    if(is_correct(op,t_,idx)):\n",
        "    #                         print('lol')\n",
        "    #                         print(true_nz_indices[w])\n",
        "    #                         print(a_indices[w].index(idx))\n",
        "                        idx2 = true_nz_indices[w][a_indices[w].index(idx)]\n",
        "                        wd1,e1,wd2,e2 = grad_word_dist(file,2*idx,idx2, taper= False)\n",
        "                        dist_accents[accent][w][0].append(wd1)\n",
        "                        dist_accents[accent][w][1].append(e1)\n",
        "                        if(np.isnan(e2/len(w))):\n",
        "                           print('encountered nan', e2, len(w))\n",
        "                           continue\n",
        "                        dist_accents[accent][w][2].append(e2/len(w))\n",
        "                        #print(wd2, e2)\n",
        "                        dist_accents[accent][w][3].append(e2)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6znDjNwke_4z"
      },
      "outputs": [],
      "source": [
        "\n",
        "mf_us = {x:([],[],[],[]) for x in chosen}\n",
        "mf_canada = {x:([],[],[],[]) for x in chosen}\n",
        "mf_indian = {x:([],[],[],[]) for x in chosen}\n",
        "mf_african = {x:([],[],[],[]) for x in chosen}\n",
        "mf_england = {x:([],[],[],[]) for x in chosen}\n",
        "mf_scotland = {x:([],[],[],[]) for x in chosen}\n",
        "mf_australian = {x:([],[],[],[]) for x in chosen}\n",
        "dist_accents = {'us':mf_us,'canada':mf_canada,'indian':mf_indian,'african':mf_african,'england':mf_england,'scotland':mf_scotland,'australia':mf_australian}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTGJpyiNe_4z"
      },
      "outputs": [],
      "source": [
        "# print(target_transcript)\n",
        "# for file in trans_dict[target_transcript]:\n",
        "#     Fs, wav = wavfile.read('my_data/MCV_validated_{}/wav/{}.wav'.format(file_meta[file]['accent'],file))\n",
        "#     display(Audio(wav, rate=Fs))\n",
        "#     with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "#         file_attr = pickle.load(j)\n",
        "#     print(file_attr['output'], file_meta[file]['accent'])\n",
        "#     print(list(file_attr['attr dict'].keys()))\n",
        "#     #inp_grad_grapheme(file, grapheme_idx)\n",
        "#     print(file)\n",
        "#     print(grad_word_dist(file,2, 1))\n",
        "#     break\n",
        "valid_punctuation = string.punctuation.replace(\"'\",\"\")\n",
        "most_frequent = chosen_words\n",
        "for t in transcripts:\n",
        "    \n",
        "    t_ = t.strip().upper()\n",
        "    t_ = t_.translate(str.maketrans({a:None for a in valid_punctuation }))\n",
        "    #print(t_)\n",
        "    t_list = t_.split()\n",
        "    true_nz_counts = {x:t_list.count(x) for x in most_frequent if t_list.count(x) > 0}\n",
        "    true_nz_indices = {x:[index for index, value in enumerate(t_list) if value == x] for x in true_nz_counts.keys()}\n",
        "    \n",
        "    for file in trans_dict[t]:\n",
        "        \n",
        "        try:   \n",
        "            with open('attribution/grad/{}.pickle'.format(file), 'rb') as j:\n",
        "                file_attr = pickle.load(j)\n",
        "            op = file_attr['output'].replace('_', '')\n",
        "    #             print(op)\n",
        "            op_list = op.split()\n",
        "            accent = file_meta[file]['accent']\n",
        "            #print(accent,'------------------')\n",
        "            a_indices = {x:[index for index, value in enumerate(op_list) if value == x] for x in true_nz_counts.keys()}\n",
        "\n",
        "            for w in true_nz_counts.keys():\n",
        "                for idx in a_indices[w]:\n",
        "    #                     print('here')\n",
        "    #                     print(w)\n",
        "                    # compute condition for correctness\n",
        "                    distr, distr_wo = grad_clubbed_word(file, 2*idx, disable_prints=True)\n",
        "                    #print(distr)\n",
        "                    max_idx = np.argmax(np.asarray(distr))\n",
        "                    max_idx_wo = np.argmax(np.asarray(distr_wo))\n",
        "                    if(is_correct(op,t_,idx)):\n",
        "    #                         print('lol')\n",
        "    #                         print(true_nz_indices[w])\n",
        "    #                         print(a_indices[w].index(idx))\n",
        "                        idx2 = true_nz_indices[w][a_indices[w].index(idx)]\n",
        "                        for l in range(len(hyphenate_word(w))):\n",
        "                            wd1,e1,wd2,e2 = grad_syll_dist(file,2*idx,idx2,l, taper= False)\n",
        "#                             dist_accents[accent][w][0].append(wd1)\n",
        "#                         dist_accents[accent][w][1].append(e1)\n",
        "                            if(np.isnan(e2)):\n",
        "                               print('encountered nan', e2)\n",
        "                               continue\n",
        "                            #dist_accents[accent][w][2].append(e2/len(w))\n",
        "                            #print(wd2, e2)\n",
        "                            #print(dist_accents[accent])\n",
        "                            #print([hyphenate_word(w)[l]])\n",
        "                            dist_accents[accent][hyphenate_word(w)[l]][3].append(e2)\n",
        "                            #print(dist_accents[accent])\n",
        "                            #break\n",
        "                    \n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "DEvviqNUe_40"
      },
      "outputs": [],
      "source": [
        "acc_chosen = {k:{a:0 for a in dist_accents.keys()} for k in chosen}\n",
        "for a in dist_accents.keys():\n",
        "#     print(a)\n",
        "    #print(chosen)\n",
        "    acc = {k:[] for k in chosen}\n",
        "    #a = 'indian'\n",
        "    #print(mf_accents[a].keys())\n",
        "    for w in chosen:\n",
        "        #print(len(acc[2]))\n",
        "        #print(len(dist_accents[a][w][2]))\n",
        "#         acc[0].append(np.asarray(dist_accents[a][w][0]).mean())\n",
        "#         acc[1].append(np.asarray(dist_accents[a][w][1]).mean())\n",
        "        inter = np.asarray(dist_accents[a][w][3]).mean()\n",
        "        if(not np.isnan(inter)):\n",
        "            acc[w].append(inter)\n",
        "            acc_chosen[w][a] = inter\n",
        "        #acc[3].append(np.asarray(dist_accents[a][w][3]).mean())\n",
        "    \n",
        "#         acc[word_cluster[w]].extend(np.random.choice(np.asarray(dist_accents[a][w][2]),int(min_dist[most_frequent.index(w)]), replace = False))\n",
        "#         acc[word_cluster[w]].append(np.random.choice(np.asarray(dist_accents[a][w][2]),int(min_dist[most_frequent.index(w)]), replace = False).mean())\n",
        "#         acc[3].append(np.asarray(dist_accents[a][w][3]).mean())\n",
        "    #print(len(np.asarray(acc[2])))\n",
        "#     print(a, acc)\n",
        "#     print('------------------------')\n",
        "print('___________________________')\n",
        "for l in acc_chosen.keys():\n",
        "    print(l, acc_chosen[l])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhkM2nh6e_40"
      },
      "source": [
        "### Clustering words based on number of phones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MErjuypZe_40"
      },
      "outputs": [],
      "source": [
        "nltk.download('cmudict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdfnBFLNe_41"
      },
      "outputs": [],
      "source": [
        "def get_cluster(num):\n",
        "    if(num < 3):\n",
        "        return 0\n",
        "    elif(num < 4):\n",
        "        return 1\n",
        "    else: return 2\n",
        "# def get_cluster(num):\n",
        "#     if(num < 3):\n",
        "#         return 0\n",
        "#     elif(num < 5):\n",
        "#         return 1\n",
        "#     else: return 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "L-Nkx7hxe_41"
      },
      "outputs": [],
      "source": [
        "arpabet = nltk.corpus.cmudict.dict()\n",
        "word_phn = np.zeros(len(most_frequent))\n",
        "word_len = np.zeros(len(most_frequent))\n",
        "for w in most_frequent:\n",
        "    word_phn[most_frequent.index(w)] = len(arpabet[w.lower()][0])\n",
        "    #break\n",
        "print(word_phn)\n",
        "for w in most_frequent:\n",
        "    word_len[most_frequent.index(w)] = len(w)\n",
        "    #break\n",
        "# print(word_phn)\n",
        "print(word_len)\n",
        "print(most_frequent)\n",
        "word_cluster = {x: get_cluster(word_phn[most_frequent.index(x)]) for x in most_frequent}\n",
        "print(word_cluster)\n",
        "print(word_phn[most_frequent.index(w)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPky8pwoe_41"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYi0BQy7e_45"
      },
      "outputs": [],
      "source": [
        "min_dist = np.ones(len(most_frequent))*10000\n",
        "for w in most_frequent:\n",
        "    #print(w)\n",
        "    \n",
        "    for a in dist_accents.keys():\n",
        "       \n",
        "        min_dist[most_frequent.index(w)] = min(len(dist_accents[a][w][2]), min_dist[most_frequent.index(w)])\n",
        "        \n",
        "print(min_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gViOKve_45"
      },
      "source": [
        "##### Report EMD based on wd2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Achh6UU0e_46"
      },
      "outputs": [],
      "source": [
        "for a in dist_accents.keys():\n",
        "#     print(a)\n",
        "    acc = ([],[],[],[])\n",
        "    #a = 'indian'\n",
        "    for w in mf_accents[a].keys():\n",
        "        #print(len(acc[2]))\n",
        "        #print(len(dist_accents[a][w][2]))\n",
        "#         acc[0].append(np.asarray(dist_accents[a][w][0]).mean())\n",
        "#         acc[1].append(np.asarray(dist_accents[a][w][1]).mean())\n",
        "        acc[word_cluster[w]].append(np.asarray(dist_accents[a][w][3]).mean())\n",
        "        acc[3].append(np.asarray(dist_accents[a][w][3]).mean())\n",
        "    \n",
        "#         acc[word_cluster[w]].extend(np.random.choice(np.asarray(dist_accents[a][w][2]),int(min_dist[most_frequent.index(w)]), replace = False))\n",
        "#         acc[word_cluster[w]].append(np.random.choice(np.asarray(dist_accents[a][w][2]),int(min_dist[most_frequent.index(w)]), replace = False).mean())\n",
        "#         acc[3].append(np.asarray(dist_accents[a][w][3]).mean())\n",
        "    #print(len(np.asarray(acc[2])))\n",
        "    print(a, np.asarray(acc[0]).mean().round(2), np.asarray(acc[1]).mean().round(2), np.asarray(acc[2]).mean().round(2), np.asarray(acc[3]).mean().round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aIb1Cq8e_46"
      },
      "source": [
        "##### Computing Syllables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vb5Q5mme_46"
      },
      "outputs": [],
      "source": [
        "with open('syll.pickle','rb') as s:\n",
        "    syllables = pickle.load(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_b1fSlGe_46"
      },
      "outputs": [],
      "source": [
        "syll_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsWitl6ne_46"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "seg = 0\n",
        "for w in syllables.keys():\n",
        "    \n",
        "    \n",
        "    wl = w.lower()\n",
        "    hyp = hyphenate_word(wl)\n",
        "    for h in hyp:\n",
        "        if(h not in syll_dict.keys()):\n",
        "            syll_dict[h]={'count':my_freq[my_freq_words.index(w)][1],'words':[w]}\n",
        "        else:\n",
        "            syll_dict[h]['count'] += my_freq[my_freq_words.index(w)][1]\n",
        "            syll_dict[h]['words'].append(w)\n",
        "            \n",
        "    try:\n",
        "        if(len(syllables[w][0]['syll'] ) == len(hyp)):\n",
        "            seg += len(hyp)\n",
        "            syllables[w][0]['splt'] = hyp\n",
        "        else:\n",
        "            count+=1\n",
        "            print(w, count)\n",
        "    except:\n",
        "        count += 1\n",
        "        print(w)\n",
        "#print(seg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS0qdq-We_47"
      },
      "outputs": [],
      "source": [
        "print(syll_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oihkHHL9e_47"
      },
      "outputs": [],
      "source": [
        "chosen = [k.upper() for k, v in sorted(syll_dict.items(), key=lambda item: item[1]['count'])][::-1][:75]\n",
        "print(chosen)\n",
        "chosen_words = []\n",
        "for c in chosen:\n",
        "    chosen_words.extend(syll_dict[c.lower()]['words'] )\n",
        "chosen_words = list(set(chosen_words))\n",
        "print(chosen_words)\n",
        "print(len(chosen_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE8ShvVAe_47"
      },
      "outputs": [],
      "source": [
        "syllables['CREATIVE']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDWlGVUse_47"
      },
      "outputs": [],
      "source": [
        "syll_dict['minder']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqQPHe5fe_48"
      },
      "outputs": [],
      "source": [
        "len(list(syllables.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiUb3IMfe_48"
      },
      "outputs": [],
      "source": [
        "my_freq = allWordDist.most_common(1256)\n",
        "my_freq_words = [w[0] for w in my_freq]"
      ]
    }
  ]
}