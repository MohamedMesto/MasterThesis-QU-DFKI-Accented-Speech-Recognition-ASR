\subsection{Vectorizing the representations} \label{unsupervised_approach_vectorization}

Once all the appropriate texts have been retrieved to represent each document and subject, the next step is to vectorize the representations. Before replacing words by their corresponding vectors, we order the words of each document and subject by frequency, so the most frequent ones are at the front. The words of the venues, advisors and referees are added in a weighted manner to their associated documents before the sorting occurs.

We then present three different methods in which we compute cosine distances, which are based on concatenation, averaging and addition. We will offer some examples afterwards, which show that summing the vectors yields the most interpretable results. The proper experiments with them are presented in chapter \ref{eval}.

\subsubsection{Item representation} \label{unsupervised_approach_our_representation}

Before replacing the words of a text with vectors, we count how often each word occurs and sort them by frequency. This is important because when computing cosine distances we may remove some vectors of a representation, so the representations of documents and subjects match in size.

Documents are represented both by their own words and those of its associated venues, but the latter are weighted down. Here we first join the counts of the document's representation and the weighted counts of the venue's representation before sorting them. Once the words of a representation are sorted, we replace them with vectors. The length of these representations was already presented in section \ref{unsupervised_approach_representations}.

\subsubsection{Computing cosine distances}

We have considered three ways of computing cosine distances. Given the word vectors of a document and a subject, we can:

\begin{enumerate}
    \item compute one cosine distance where the word vectors are concatenated,
    \item compute the cosine distance between each pair of words (one from the document and one from the subject), and then average over the resulting distances or
    \item compute one cosine distance where the word vectors are summed.
\end{enumerate}

Concatenating the vectors is the easiest way to proceed. All vectors representing each document and subject are gathered into one, making the computation of the cosine distance straight forward. This is the method used in the \acrshort{mag}. Averaging over the word vectors differs from the other two in that word vectors are compared independently. Here it is therefore important that words are ordered by frequency, so the most relevant words of a document are compared with the relevant ones of the subject.

The possibility of adding the word vectors together arises from the linear relationships between the skip-gram embeddings \cite{mikolov2013distributed}. The authors argue that this property can be explained with the training objective, where adding word vectors is equivalent to multiplying their context distributions. Thus, words that often appear together can be associated by addition.
