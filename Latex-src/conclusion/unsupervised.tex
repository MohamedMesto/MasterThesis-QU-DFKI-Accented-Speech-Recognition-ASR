\subsection{Comparison of the unsupervised approach and MAG}

The results of our approach, although promising, are nowhere near the accuracy achieved by \acrshort{mag}, which state to have achieved over 80 \% accuracy. However, their evaluation set consisted of only 500 subject-document pairs, which is very small compared to the one billion pairs present in \acrshort{mag}. Furthermore, their evaluation procedure doesn't assign the same subject-document pair to multiple judges, thus not accounting for the inter-indexer inconsistency often talked about in the literature (\cite{medelyan2008domain}, \cite{csomai2007investigations}). Still, their indexing accuracy is most likely far superior to ours. There are several differences between our use cases that may explain the difference in accuracy. We discuss them in the following paragraphs.

\subsubsection{Dataset size}

First and foremost is the size of the dataset. \acrshort{mag}'s approach relies on connecting documents through their venues, citations and references to enrich their representations. The number of connections depends on the size of the dataset, and ours is tiny in comparison with \acrshort{mag}'s. For example, we barely found any references to other documents in the repositories.

\subsubsection{Manual steps}

The second factor that explains the difference in performance is the number of manual steps taken by Microsoft. Some of them were unfeasible for our use case and were omitted. The step that we believe has hindered the performance of our approach the most is assigning subjects to venues, which would enrich the representations of the subjects significantly.

The poor performance of the approach on the handwritten evaluation set evidences this issue, where the model cannot differentiate well between the subjects of each field. On the other hand, it is able to identify fields of study much better, as their corresponding vectors are further apart in vector space. This indicates that the representation of the subjects was not precise enough.

\subsubsection{Data quality}

Data quality also plays an important role. As explained throughout chapter \ref{repo_analysis}, our dataset comprises many elements that cannot be used to relate documents, e.g. because they appear only once. The metadata that represent the documents is sparse. Another issue arises from the fact that the metadata is input manually into the repositories (i.e. fields are not picked from lists). This results in numerous names of venues, advisors and referees written in different ways, which hinders the task of relating documents to one another through them.

Furthermore, our data is very noisy. For example, there are German texts tagged as they were written in English (see section \ref{repo_analysis_data}). There are also 1,833 documents without an abstract, which accounts for 6 \% of the documents. All these documents could not be represented well and are thus much harder to index.