\subsection{Comparison of the approaches}

In this section, we compare both approaches in terms of their implementations and their potential. We have already done a similar assessment of each approach in their respective concluding sections. We now bring them together here. We also discuss the interpretability of each approach, which is a requirement for certain use cases, e.g. where the indexing is not reviewed by a human.

In section \ref{results_final}, we showed that the \acrfull{sm} outperforms the \acrshort{um} on all evaluation sets and for both metrics. The difference in performance is especially large for the handwritten dataset. This is because the representations of documents and subjects on the \acrshort{um} are not detailed enough to accurately map documents to their correct subjects. Performance is the most important comparison tool, as the goal of this thesis is to find out which learning paradigm, supervised or unsupervised, is better suited for performing subject indexing in small, technical and multidisciplinary repositories. However, there are other aspects of the approaches that are worth considering, such as their implementations, their interpretability and their potential.

The \acrshort{sm} is easy to implement, but the models are expensive to train. On the other hand, the \acrshort{um} is cheaper to compute, but the implementation is much more complex and requires more time. Furthermore, the \acrshort{um} is easier to interpret, as the vector representations of documents and subjects can be analyzed, as well as the relationships between documents. The \acrshort{sm} is harder to interpret, as it has thousands of parameters whose purpose is unknown. Regarding their potential, the performance of the \acrshort{sm} can be easily improved by gathering more training data. Improving the \acrshort{um} requires more complex changes, such as improving the vocabulary or manually assigning subjects to venues.

In summary, the \acrshort{sm} performs better, is easier to implement and has more room for improvement. On the other hand, the \acrshort{um} is cheaper to compute and is more interpretable. Given that performance is the goal of this thesis, we conclude that the \acrshort{sm} is better suited for our task.

\subsubsection{Results}

We have compared the best methods we could find for each of the approaches in section \ref{results_final}. Their performance on the evaluation sets show that the \acrshort{sm} is better at identifying fields of study, although the margin between both approaches is not as large. For instance, when only one candidate field is considered, the \acrshort{sm} outperforms the \acrshort{um} by 2 \% on the \acrshort{ddc} set, whereas the roles are reversed in the venue set, also with a difference of 2 \%.

Recall that the \acrshort{ddc} set is much larger than the venue set: it comprises 70 \% of the documents in the repositories, whereas the venue set comprises only 11 \% of the documents. Therefore, the performance of the methods in the \acrshort{ddc} set is more importance, which favors the \acrshort{sm}. Furthermore, the \acrshort{sm} is more accurate for all other numbers of candidates, reaching a difference of 11 \% when 5 candidate fields are considered. We therefore conclude that the \acrshort{sm} is better at identifying fields of study.

The difference in hit rate between the approaches is much larger when the assignment of all subjects is considered, i.e. their performance on the handwritten evaluation set. The \acrshort{sm} outperforms the \acrshort{um} by 8 \% when 5 candidates are considered, and reaches a maximum value of 36 \% for 50 candidates. This significant difference in performance is confirmed by the \acrfull{lcas}, which indicates that the guesses of the \acrshort{um} are close to the correct subjects.

\subsubsection{Implementation}

The implementation of both methods are best compared by the amount of work they require, both to develop and to compute. The \acrfull{sm} requires much less development time, but is computationally more expensive than the \acrfull{um}. We look at each of these two aspects separately.

\paragraph{Computational cost} \mbox{}

The computational cost of the \acrshort{sm} model arises from the difficulty of fine-tuning the parameters of the model and the training procedure. There are several guidelines for their values, but in the end it always comes to trial and error. This empirical approach requires training numerous models, each requiring a lot of computational power. In our case, each model took around 11 hours for every 20 epochs of training. If we had the right parameters from the beginning, the computational cost of the \acrshort{sm} would be comparable to that of the \acrshort{um}.

An important difference between the implementations is the word embeddings. The ones used for the \acrshort{um} were trained from scratch in our dataset, which takes around 20 hours. The embeddings used in the \acrshort{sm} were pre-trained, which reduces the computational cost. Processing the texts (i.e., tokenizing and lemmatizing them) is also an expensive task. Processing the documents of the repositories takes a couple of hours, whereas processing the training data takes ten times as much, proportional to how many texts there are.

\paragraph{Implementation time} \mbox{}

The \acrshort{um}, while cheaper to compute, takes much more time to implement. It requires extracting specific metadata from the repositories, such as venues, advisors and referees. Then, documents are related through the extracted metadata. Furthermore, the subjects are represented by their Wikipedia articles, which have to be retrieved from the Internet and processed. On the other hand, the \acrshort{sm} does not require any special representation of the documents and subjects. The model receives as input the processed titles and abstracts of the documents, without any further metadata, and subjects are not represented at all. This makes it much faster to implement.

\subsubsection{Interpretability}

Although interpretability is not directly related to the performance of the model, it constrains the use cases of each approach. A model is considered interpretable if it indicates what aspects of the document led it to assign the document certain subjects. Consider an administrator that wants to review the subject assignments of a document. If the indexing program showed which aspect of the document are responsible for each subject assignment, that would help him assess if the subject assignment is appropriate or not. Otherwise, reviewing assignments would have to be done by experts in the respective fields.

None of our approaches can offer such insights out of the box. However, the operations performed in the \acrshort{um} are easier to follow. One can ensure that the related documents are indeed topically similar to a certain document, or look at the closest documents in vector space. On the other hand, the \acrshort{sm} comprises thousands of parameters and performs large matrix multiplications that are impossible to understand. Neural networks are often called black boxes exactly for this reason.

There is a lot of research going on in this area, under the name of \textit{explainable artificial intelligence (XAI)}. For text classification, \cite{liu2019towards} proposes a generative framework that learns to classify and explain its decisions simultaneously. Thus, introducing interpretability in the \acrshort{sm} would require to modify the architecture. As it is, its output is not interpretable. On the other hand, the assignments made in the \acrshort{um} can be explored either in vector space, and the representation of documents can be verified.

\subsubsection{Potential}

We have already discussed the possible improvements for each approach in their respective chapters. Here we compare them, evaluating which of them we believe is easier to improve. We also mention other areas that can be improved, which we have discovered through the experiments with the evaluation sets.

Both models would benefit from cleaner data. As we discussed in section \ref{results_length}, the hit rate of the \acrshort{sm} for documents represented with less than 50 tokens is 20 \% worse than for larger documents. The data can be improved by adding the abstracts of the 6 \% of documents that are currently missing one, and fixing those that are currently incorrect. We estimate that at least 300 abstracts are not written in English (see section \ref{repo_analysis_data}).

The performance of the \acrshort{sm} can be improved either by gathering more training data or by modifying the architecture. Gathering more training data for the \acrshort{sm} is the easiest to implement out of all possible improvements. Modifying the architecture, either by increasing its complexity or adding further regularization, are in general also simple changes.

Another potential improvement that arises from our experiments is the importance of using appropriate embeddings. The embeddings trained on our dataset performed significantly better than pre-trained ones in the \acrshort{um}, with the difference between 20 and 30 \% for the \acrshort{ddc} and venue sets. Replacing the fastText pre-trained vectors with others that have been trained on more technical corpora could yield similar improvements for the \acrshort{sm}.

Improving the performance of the \acrshort{um} is more complicated: extending the vocabulary increases the complexity and cost of the indexing task, whereas improving the representation of subjects by assigning them to venues requires a lot of manual work. We therefore conclude that the \acrshort{sm} has more potential, as the modifications that improve its performance are easier to implement and don't affect the pipeline.