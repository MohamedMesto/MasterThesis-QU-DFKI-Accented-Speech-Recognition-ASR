\subsection{Pre-trained word embeddings} \label{supervised_approach_embeddings}

Word embedding is a technique that maps words to vectors, where the vectors represent the meaning of the words. They are usually learned in an unsupervised fashion from a given corpus, which are analyzed statistically \cite{mikolov2017advances}. The quality of the embeddings thus largely depends on the size of the corpus. Therefore, applications with small datasets use word embeddings that were trained in larger corpora, called pre-trained embeddings.

Using pre-trained word embeddings improves the generalization ability of downstream tasks, as the vector representations of the words arise from a wide variety of texts, which better capture the meaning and usage of words. Furthermore, they reduce the complexity of our task, as training them and fine-tuning them takes time and resources. We did this in the unsupervised approach, which revolved exclusively around our dataset. This approach aims to be more general, leveraging previous work and large datasets. In this section, we present the most popular options of pre-trained embeddings: transformers, fastText and GloVE. Then, we argue why we believe the fastText embeddings are the better suited for our use case.

\subsubsection{Transformers}

Transformers are encoder-decoder networks that feature attention \cite{vaswani2017attention}, which is able to remember long sequences better than its predecessors (recurrent networks) by considering all words at once instead of sequentially. This change also results in an increase of performance, as it allows parallelization. With attention, the encoded representation of the input, called the \textit{context vector}, is built with information from the encoder, the decoder and the alignment weights between them. Previous \acrshort{sota} models used the output of the encoder directly as the context vector.

In this section, we present two applications of transformers to language modeling, which we could use to retrieve vector representations from words. We also consider the costs involved in training such models, and the constraints that arise from their sizes.

\paragraph{BERT} \mbox{}

BERT \cite{devlin2018bert} uses transformers to model language. It is mainly characterized by its size, with hundreds of millions of parameters, and achieved \acrshort{sota} results in eleven \acrfull{nlp} tasks, often by a large margin. It features two training procedures that differ from the sequential learning approaches that were popular at the time. In the first procedure, the model is expected to predict a random number of words of a sentence, instead of the next word. This allows the model to learn from words both to the left and to the right of the center word, improving the accuracy of the context vectors. Their second training procedure consists of predicting the next sentence, which is relevant for \acrshort{nlp} tasks like question answering, where the relationships between sentences matter.

\paragraph{SciBERT} \mbox{}

SciBERT \cite{beltagy2019scibert} is a pre-trained language model that uses the same architecture and training procedure as BERT, but a different corpus. It is trained on a subset of Semantic Scholar that consists of Computer Science and Biology publications, whereas BERT is trained on a collection of novels and English Wikipedia. Thus, SciBERT is ideal for performing various \acrshort{nlp} tasks on scientific corpora.

\paragraph{Training cost} \mbox{}

The power of the transformer architectures presented above relies heavily on their size \cite{devlin2018bert}. Training these large models is very costly. For instance, training BERT requires as much energy as a trans-American flight \cite{bender2021dangers}. Training such large models have thus considerable environmental consequences. Furthermore, their research is only possible by those who can afford such costs, which usually means large corporations or the most powerful countries.

\subsubsection{GloVE}

GloVE word embeddings \cite{pennington2014glove} are trained on co-occurrence statistics of words. Pre-trained embeddings on various datasets, such as English Wikipedia and Common Crawl, are available online\footnote{\url{https://nlp.stanford.edu/projects/glove/}}. Their largest collection of embeddings includes over two million tokens.

Their approach combines local context methods, such as skip-gram, with global methods. Each of these two methods addresses the drawbacks of the other. Local methods optimize the vector space structure, but don't use the statistics of the corpus, such as co-occurrence counts, which is inefficient. Global methods introduce such statistics, but are poor at refining vector structures. Vector structures, which skip-gram and \acrshort{cbow} specialize on, are responsible for the performance of the model in analogy tasks, such as ``king is to queen as man is to woman'' \cite{pennington2014glove}.

GloVE embeddings are trained with a weighted least squares model that includes co-occurrence counts of pairs of words, which increases its efficiency. They achieved \acrshort{sota} performance on several tasks by the time of publication (2014). They were outperformed by fastText three years later, which we discuss in the following section.

\subsubsection{FastText}

The fastText word embeddings \cite{mikolov2017advances} are trained using the \acrfull{cbow} model, which learns word representations by predicting word vectors given the vectors of their surrounding words in the corpus. We first look at the training procedure and the features of the \acrshort{cbow} model used by the authors, and then present the training data they used and compare their results with those of GloVE.

\paragraph{Embedding model} \mbox{}

Their training procedure features several modifications that improve the accuracy of the resulting representation. It includes subsampling of frequent words and negative sampling, two techniques we included our skip-gram implementation (see section \ref{mag_skipgram}). Another improvement is considering the positions of words when computing the context vector, which is otherwise computed by averaging its constituent word vectors. The authors do so by learning representations for each position in the context window and multiplying them with their respective words.

The authors also include phrases in their vocabulary by iteratively merging n-grams according to their mutual information. However, the phrases are not included in the pre-trained vectors available online. One final feature of their new model is the introduction of subword information, which is often meaningful and can also be used e.g. for representing misspelled words. They learn n-grams of characters from the corpus and then add the resulting subword vectors to the word vectors when appropriate.

\paragraph{Training and results} \mbox{}

The authors have trained word vectors with several corpora, including Wikipedia, which includes over nine billion tokens, and Common Crawl, with 630 billion tokens. Their only processing step was tokenization. They did not normalize the vectors, which often improves the performance of downstream tasks, as doing so removes important information which may be useful for certain tasks.

They have compared their model with GloVE, both trained on similar corpora. FastText outperformed GloVE for all three evaluation sets used (analogy tasks and the Rare Words and SQuAD datasets). Furthermore, they reported \acrshort{sota} results on the word analogy tasks.

\subsubsection{Our choice}

We discard using transformers because they use their own vocabulary, and thus their own text pre-processing pipelines. We would have to again pre-process our repository dataset, which is very costly. The other options offer the embeddings of full words directly as files, which is easier to use and more flexible. In the case of fastText, the words included in the file are not processed in any way, so they are useful for a wide variety of use cases. GloVE vectors are only uncased.

fastText outperforms GloVE in all direct comparisons \cite{mikolov2017advances}. Furthermore, \cite{levy2015improving} shows that the \acrshort{cbow} model from fastText produces more accurate embeddings than GloVE, as well as being faster to train and more memory efficient. We therefore use the fastText pre-trained vectors. Specifically, we use those trained on a Wikipedia dump of 2017, a news dataset and a web corpus, with 16 billion tokens in total. They comprise 300-dimensional vectors for one million words. The words are neither lower-cased nor lemmatized. For example, the file includes vectors for the words \textit{learn} and \textit{learned}, both lower-cased and capitalized.
