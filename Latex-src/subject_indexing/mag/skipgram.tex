\subsubsection{Skip-gram} \label{mag_skipgram}

The key component of Microsoft's approach is Skip-gram \cite{mikolov2013distributed}, a model for representing words as vectors, called embeddings. The embeddings are improved by training the model. The model receives the center word, and it should predict the context words. How many context words the model should be able to predict depends on the window size. We have implemented skip-gram in PyTorch\footnote{\url{https://github.com/carlosfranzreb/skipgram}}. Details about the training procedure and examples of embeddings can be found in section \ref{implementation_skipgram}.

Skip-gram arises as a counterpart to the \acrfull{cbow} model. This model is trained by giving it the words surrounding a certain word as input, called context words. It then has to predict the center word, i.e. the word that is surrounded by the context words in the data. Skip-gram was an improvement in both training time and semantic quality of the representation, according to the experiments of the authors \cite{mikolov2013efficient}.

The initial model was improved by adding two features. The first is subsampling of frequent words. The representation of words that appear often doesn't change much after a while, as the model has already tuned them. On the other hand, words that seldom appear on the data need more iterations for their representations to achieve a comparable accuracy. Therefore, frequent words are subsampled according to the formula below, which aggressively discards words whose frequency exceeds the threshold $t$. This increases accuracy, as the model doesn't overfit frequent words.

$$ Discard(word) = 1 - \sqrt{\frac{t}{freq(word)}} $$

The second improvement to the model was the introduction of negative sampling. It alleviates the computational cost of the problem, which, without negative sampling, requires computing the softmax of all data points. With negative sampling, we only consider $k$ words other than the input words, called negative samples. We assume that their vector representations should be considerably different from the representation of our input word. Thus, the model should output zero when fed the input word and one of the negative samples.