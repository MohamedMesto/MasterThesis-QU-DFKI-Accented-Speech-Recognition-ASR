\section{Evaluation} \label{eval}

In this chapter, we present our evaluation procedure, which we use to assess the performance of the approaches. We have created three evaluation sets with the metadata from the repositories. Two of them consider only fields, whereas the third one considers all subjects. We evaluate the approaches on these sets with two different metrics, one that measures the hit rate of the assignments and one that uses the hierarchy to evaluate if the guesses are close to the correct subject. 

We discard performing a survey because every sample would require cross-checking, as human judges are inconsistent \cite{csomai2007investigations}. Furthermore, given how technical our documents are, assessing the appropriateness of a subject assignment might prove difficult for someone who is not well-read in the topics of the document. Another disadvantage is that numerous assignments have to be evaluated for the survey to gain statistical significance. Thus, we would need to gather experts in various fields (multiple per field) and have them assess many assignments. We therefore leave the survey for future work.

As we discussed when analyzing the repositories (section \ref{repo_analysis_subjects}), most of the popular subjects in the repositories are \acrfull{ddc} subjects. These are usually broader in their meaning, but are still useful descriptors of the topics handled by a document. We present how we have incorporated them into our evaluation procedure in section \ref{eval_ddc}. Essentially, given that we have to manually assign them to \acrshort{mag} subjects, we have mapped them to the fields. They will therefore be useful to evaluate if the indexing methods have grasped the broad meaning of the documents.

Venues, advisors and referees also describe the content of the documents they are assigned to. We therefore create another evaluation set with them. As for the \acrshort{ddc} evaluation set, in section \ref{eval_venues} we map popular venues, advisors and referees manually to the fields of study, and then retrieve their associated documents.

We create our final evaluation set by automatically mapping subjects present in the repositories to the ones in our subset of \acrshort{mag} subjects. This evaluation set is the only one that considers subjects that are not fields, and is thus the one that determines is the approach is able to correctly model the 2,157 labels we consider in this thesis.

We measure the performance of the models on these evaluation sets with two different metrics. The first outputs one if the correct subject is included in the top $k$ subjects output by a model, and otherwise zero. The second one, given a subject and a candidate output by a model, outputs a value between zero and one that describes how close the candidate is to the subject in the hierarchy. We present both metrics in section \ref{eval_metrics}, as well as other metrics we have found in the literature.

After presenting the evaluation sets and metrics, we use them to compare the candidates of each approach and then compare the resulting models to one another. Interestingly, our evaluation procedure indicates that our own word embeddings yield better results in the unsupervised approach than pre-trained embeddings. Regarding the supervised approach, we find out that extending the model with asymmetry or coherence does not improve the performance of the model, which may be because some parameters were optimized for the initial model, without considering the two extensions. Still, the supervised approach yields the best results in all evaluation sets and for both metrics.

We end this chapter by analyzing the impact of representation length on the hit rate of the supervised model. We find out that the hit rate of the model for documents that are represented by less than 50 tokens is over 20 \% worse than for larger representations. This indicates that our results could be improved by adding missing abstracts, and fixing those that are in other languages.

\input{evaluation/datasets}
\input{evaluation/metrics}
\input{evaluation/unsupervised_results}
\input{evaluation/supervised_results}
\input{evaluation/final_results}
\input{evaluation/results_length}
