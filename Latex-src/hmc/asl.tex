\subsection{Asymmetric loss function} \label{hmc_asl}

\cite{ben2020asymmetric} addresses the imbalance between positive and negative labels in multi-label settings with an \acrfull{asl} function that dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. Training the state-of-the-art models in computer vision with this loss function significantly outperforms previous methods. On ResNet101, the common architecture used for multi-label classification, the asymmetric loss improves previous results by more than 1 \%. The \acrshort{asl} is also popular for its robustness against noise, as it is better suited for modeling noise in \acrshort{mlc} settings \cite{zhao2021evaluating}. The choice of the loss function plays a key role on how the model handles noise \cite{karimi2020deep}.

\subsubsection{BCE and focal loss}

\acrfull{bce}, the most popular loss function in multi-label classification settings, treats every label the same. For example, in a one-label setting where the model outputs $p=0.7$, if the label is positive ($y=1$), the \acrshort{bce} loss would be $-log(0.7) = 0.4$. If negative ($y=0$), the \acrshort{bce} loss would be $-log(0.3) = 1.2$, i.e. larger than in the other case, as the predicted probability is further away from the true label. The problem with using \acrshort{bce} in a multi-label setting is that most of the true labels for any given input are usually negative. In our case, any document is assigned only a few of the available subjects.

$$ BCE = \frac{1}{K} \sum_{k=1}^K -y_k \cdot \log (p_k) - (1-y_k) \cdot \log (1-p_k) $$

\acrfull{fl} addresses this issue by weighting each label score in a way that model outputs that are further away from their true labels gain more importance, whereas the model outputs that are already close to their targets are weighted down. Returning to the previous example and setting $\gamma=1$, a model output of $0.7$ with a positive true label ($y=1$) would receive a \acrshort{fl} of $-0.3 \cdot log(0.7) = 0.1$, and with a negative label ($y=0$), the \acrshort{fl} would be $-0.7 \cdot log(0.3) = 0.8$.

If we compare the results of both losses, we see that the difference between the results is much larger when using the \acrshort{fl}: they differ by a factor of 8 instead of by a factor of 3. The \acrshort{fl} gives more importance to the model output that was far away from its true label. Thus, the higher the \acrshort{fl} parameter $\gamma$ is set, the larger the difference will be between outputs that are close to their labels and those that are further away. However, setting $\gamma$ too high could remove the gradients from the rare positive samples \cite{ben2020asymmetric}.

$$ FL_\gamma = \frac{1}{K} \sum_{k=1}^K -y_k \cdot (1-p_k)^\gamma \cdot \log (p_k) - (1-y_k)\cdot p_k^\gamma \cdot \log (1-p_k) $$

\subsubsection{Asymmetric loss}

\acrfull{asl} overcomes this issue by decoupling the $\gamma$ parameter for negative and positive true labels, i.e. into $\gamma_-$ and $\gamma_+$, respectively. Setting $\gamma_- > \gamma_+$ emphasizes the contribution of positive samples, while giving less importance to negative samples that are close to their true label. Each of these three loss functions is an extension of the previous one. \acrshort{fl} equals \acrshort{bce} for $\gamma=0$, and \acrshort{asl} equals \acrshort{fl} for $\gamma_- = \gamma_+$.

$$ ASL_{\gamma_-,\gamma_+} = \frac{1}{K} \sum_{k=1}^K -y_k \cdot (1-p_k)^{\gamma_+} \cdot \log (p_k) - (1-y_k)\cdot p_k^{\gamma_-} \cdot \log (1-p_k) $$

The authors also propose using a hard threshold to discard negative samples below it, to further direct the training towards the positive samples. Although improving the performance of the models, this addition increases the complexity of the model, adding another parameter that requires tuning. They also experimented with dynamically tuning the $\gamma$ values, but concluded that doing so hindered the performance of the models. Still, they state this scheme could be useful for novice users.