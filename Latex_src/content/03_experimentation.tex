\chapter{Experimentation}
\label{cha:comparison}
\section{Dataset}


pip install seaborn

Python for Data Science and Machine Learning Bootcamp : Distribution Plots

https://www.youtube.com/watch?v=lTc7NU9XpWE

https://seaborn.pydata.org/installing.html

% https://www.w3schools.com/python/numpy/numpy_random_seaborn.asp

https://seaborn.pydata.org/tutorial/distributions.html



\section{Improved/ significantly improved}



Data Augmentation (noise, time) --> librosa, specAug






**************
That is what I would do as you could still introduce RTF, WER etc. in later sections and describe the results in better detail. 

Will refine again

*********

The results of the tests carried out on the public LibriSpeech showed a resounding success. The Emformer achieved a WER of 2.50\text{\%}in the clean-up test and 5.62\text{\%} in the other-test for an average latency of 960ms \cite{shi2021emformer}. Besides, at a low latency of 80 ms, Emformer gains WER 3.01\text{\%} on test-clean and 7.09\text{\%} on test-other. Nevertheless, One of the promising technologies in this field is the Transformer Transducer. It exceeded the neural transducer with LSTM/BLSTM networks and achieved a WER of 6.37\text{\%} on the test-clean set and 15.30\text{\%} on the test-other set\cite{yeh2019transformer}. Moreover, One of the new technologies that grabbed the spotlight was the wav2vec2 pretrained model. It supported the ASR Systems in the Dysarthric speech recognition field\cite{karthick2022speaker}. 
